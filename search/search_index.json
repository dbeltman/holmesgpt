{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Installation","text":"<p>Choose your installation method</p>"},{"location":"#options","title":"Options","text":"<ul> <li> <p> Install CLI</p> <p>Run HolmesGPT from your terminal</p> <p> Install</p> </li> <li> <p> Install UI/TUI</p> <p>Use through a web interface or K9s plugin</p> <p> Install</p> </li> </ul>"},{"location":"#more-options","title":"More Options","text":"<p>Using HolmesGPT via a HTTP API or Python SDK</p> <ul> <li>Deploy as an HTTP server using(API only) as a Helm Chart</li> <li>Embed HolmesGPT in your applications with a Python SDK</li> </ul>"},{"location":"#need-help","title":"Need Help?","text":"<p>If you encounter issues during setup:</p> <ul> <li>Check our troubleshooting guide</li> <li>Visit our FAQ</li> <li>Join our Slack community</li> </ul> <p>Ready to get started? Pick your installation method above! \ud83d\ude80</p>"},{"location":"api-keys/","title":"Getting an LLM API Key for HolmesGPT","text":"<p>If you use HolmesGPT with Robusta SaaS, you can start using HolmesGPT right away, without an API Key like OpenAI.</p> <p>If you're running HolmesGPT standalone, you'll need to bring your own API Key for an AI model of your choice.</p> <p>The most popular LLM provider is OpenAI, but you can use most LiteLLM-compatible AI models with HolmesGPT. To use an LLM, set <code>--model</code> (e.g. <code>gpt-4o</code> or <code>bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0</code>) and <code>--api-key</code> (if necessary). Depending on the provider, you may need to set environment variables too.</p> <p>Instructions for popular LLMs:</p>"},{"location":"api-keys/#openai","title":"OpenAI","text":"<p>To work with OpenAI's GPT 3.5 or GPT-4 models you need a paid OpenAI API key.</p> <p>Note: This is different from being a \"ChatGPT Plus\" subscriber.</p> <p>Pass your API key to holmes with the <code>--api-key</code> cli argument. Because OpenAI is the default LLM, the <code>--model</code> flag is optional for OpenAI (gpt-4o is the default).</p> <pre><code>holmes ask --api-key=\"...\" \"what pods are crashing in my cluster and why?\"\n</code></pre> <p>If you prefer not to pass secrets on the cli, set the OPENAI_API_KEY environment variable or save the API key in a HolmesGPT config file.</p>"},{"location":"api-keys/#anthropic","title":"Anthropic","text":"<p>To use Anthropic's Claude models, you need an Anthropic API key.</p> <p>Set the <code>ANTHROPIC_API_KEY</code> environment variable and specify the model:</p> <pre><code>export ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\nholmes ask \"what pods are unhealthy and why?\" --model=\"anthropic/claude-3-opus-20240229\"\n</code></pre> <p>You can also pass the API key directly via the CLI:</p> <pre><code>holmes ask \"what pods are unhealthy and why?\" --model=\"anthropic/claude-3-opus-20240229\" --api-key=\"your-anthropic-api-key\"\n</code></pre> <p>Available models include <code>claude-3-opus-20240229</code>, <code>claude-3-sonnet-20240229</code>, and <code>claude-3-haiku-20240307</code>.</p>"},{"location":"api-keys/#azure-openai","title":"Azure OpenAI","text":"<p>To work with Azure AI, you need an Azure OpenAI resource and to set the following environment variables:</p> <ul> <li>AZURE_API_VERSION - e.g. 2024-02-15-preview</li> <li>AZURE_API_BASE - e.g. https://my-org.openai.azure.com/</li> <li>AZURE_API_KEY (optional) - equivalent to the <code>--api-key</code> cli argument</li> </ul> <p>Set those environment variables and run:</p> <pre><code>holmes ask \"what pods are unhealthy and why?\" --model=azure/&lt;DEPLOYMENT_NAME&gt; --api-key=&lt;API_KEY&gt;\n</code></pre> <p>Refer LiteLLM Azure docs \u2197 for more details.</p>"},{"location":"api-keys/#aws-bedrock","title":"AWS Bedrock","text":"<p>Before running the below command you must run <code>pip install boto3&gt;=1.28.57</code> and set the following environment variables:</p> <ul> <li><code>AWS_REGION_NAME</code></li> <li><code>AWS_ACCESS_KEY_ID</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code></li> </ul> <p>If the AWS cli is already configured on your machine, you may be able to find those parameters with:</p> <pre><code>cat ~/.aws/credentials ~/.aws/config\n</code></pre> <p>Once everything is configured, run: <pre><code>holmes ask \"what pods are unhealthy and why?\" --model=bedrock/&lt;MODEL_NAME&gt;\n</code></pre></p> <p>Be sure to replace <code>MODEL_NAME</code> with a model you have access to - e.g. <code>anthropic.claude-3-5-sonnet-20240620-v1:0</code>. To list models your account can access:</p> <pre><code>aws bedrock list-foundation-models --region=us-east-1\n</code></pre> <p>Note that different models are available in different regions. For example, Claude Opus is only available in us-west-2.</p> <p>Refer to LiteLLM Bedrock docs \u2197 for more details.</p>"},{"location":"api-keys/#ollama","title":"Ollama","text":"<p>Ollama is supported, but buggy. We recommend using other models if you can, until Ollama tool-calling capabilities improve. Specifically, Ollama often calls tools with non-existent or missing parameters.</p> <p>If you'd like to try using Ollama anyway, see below: <pre><code>export OLLAMA_API_BASE=\"http://localhost:11434\"\nholmes ask \"what pods are unhealthy in my cluster?\" --model=\"ollama_chat/llama3.1\"\n</code></pre></p> <p>You can also connect to Ollama in the standard OpenAI format (this should be equivalent to the above):</p> <pre><code># note the v1 at the end\nexport OPENAI_API_BASE=\"http://localhost:11434/v1\"\n# holmes requires OPENAPI_API_KEY to be set but value does not matter\nexport OPENAI_API_KEY=123\nholmes ask \"what pods are unhealthy in my cluster?\" --model=\"openai/llama3.1\"\n</code></pre>"},{"location":"api-keys/#gemini","title":"Gemini","text":"<p>To use Gemini, set the <code>GEMINI_API_KEY</code> environment variable as follows:</p> <pre><code>export GEMINI_API_KEY=\"your-gemini-api-key\"\n</code></pre> <p>Once the environment variable is set, you can run the following command to interact with Gemini:</p> <pre><code>holmes ask \"what pods are unhealthy and why?\" --model=gemini/&lt;MODEL_NAME&gt;\n</code></pre> <p>Be sure to replace <code>MODEL_NAME</code> with a model you have access to - e.g., <code>gemini-pro</code>,<code>gemini/gemini-1.5-flash</code>, etc.</p>"},{"location":"api-keys/#google-vertex-ai","title":"Google Vertex AI","text":"<p>To use Vertex AI with Gemini models, set the following environment variables:</p> <pre><code>export VERTEXAI_PROJECT=\"your-project-id\"\nexport VERTEXAI_LOCATION=\"us-central1\"\nexport GOOGLE_APPLICATION_CREDENTIALS=\"path/to/your/service_account_key.json\"\n</code></pre> <p>Once the environment variables are set, you can run the following command to interact with Vertex AI Gemini models:</p> <pre><code>poetry run python holmes.py ask \"what pods are unhealthy and why?\" --model \"vertex_ai/&lt;MODEL_NAME&gt;\"\n</code></pre> <p>Be sure to replace <code>MODEL_NAME</code> with a model you have access to - e.g., <code>gemini-pro</code>,<code>gemini-2.0-flash-exp</code>, etc. Ensure you have the correct project, location, and credentials for accessing the desired Vertex AI model.</p>"},{"location":"api-keys/#other-openai-compatible-models","title":"Other OpenAI-compatible models","text":"<p>You will need an LLM with support for function-calling (tool-calling).</p> <ul> <li>Set the environment variable for your URL with <code>OPENAI_API_BASE</code></li> <li>Set the model as <code>openai/&lt;your-model-name&gt;</code> (e.g., <code>llama3.1:latest</code>)</li> <li>Set your API key (if your URL doesn't require a key, then add a random value for <code>--api-key</code>)</li> </ul> <pre><code>export OPENAI_API_BASE=&lt;URL_HERE&gt;\nholmes ask \"what pods are unhealthy and why?\" --model=openai/&lt;MODEL_NAME&gt; --api-key=&lt;API_KEY_HERE&gt;\n</code></pre> <p>Important: Please verify that your model and inference server support function calling! HolmesGPT is currently unable to check if the LLM it was given supports function-calling or not. Some models that lack function-calling capabilities will  hallucinate answers instead of reporting that they are unable to call functions. This behaviour depends on the model.</p> <p>In particular, note that vLLM does not yet support function calling, whereas llama-cpp does support it.</p>"},{"location":"api-keys/#additional-llm-configuration","title":"Additional LLM Configuration","text":""},{"location":"api-keys/#trusting-custom-certificate-authority-ca-certificate","title":"Trusting custom Certificate Authority (CA) certificate","text":"<p>If your llm provider url uses a certificate from a custom CA, in order to trust it, base-64 encode the certificate, and store it in an environment variable named CERTIFICATE</p>"},{"location":"evals-introduction/","title":"HolmesGPT Evaluations - Introduction","text":"<ul> <li>Docs on writing your own evals.</li> <li>Use Braintrust to view analyze results (optional).</li> </ul>"},{"location":"evals-introduction/#overview","title":"Overview","text":"<p>HolmesGPT uses automated evaluations (evals) to ensure consistent performance across different LLM models and to catch regressions during development. These evaluations test the system's ability to correctly diagnose Kubernetes issues.</p> <p>The eval system comprises two main test suites:</p> <ul> <li>Ask Holmes: Tests single-question interactions with HolmesGPT</li> <li>Investigate: Tests HolmesGPT's ability to investigate specific issues reported by AlertManager</li> </ul> <p>Evals use fixtures that simulate real Kubernetes environments and tool outputs, allowing comprehensive testing without requiring live clusters.</p> <p>While results are tracked and analyzed using Braintrust, Braintrust is not necessary to writing, running and debugging evals.</p>"},{"location":"evals-introduction/#example","title":"Example","text":"<p>Below is an example of a report added to pull requests to catch regressions:</p> Test suite Test case Status ask_holmes 01_how_many_pods ask_holmes 02_what_is_wrong_with_pod ask_holmes 02_what_is_wrong_with_pod_LOKI ask_holmes 03_what_is_the_command_to_port_forward ask_holmes 04_related_k8s_events ask_holmes 05_image_version ask_holmes 06_explain_issue ask_holmes 07_high_latency ask_holmes 07_high_latency_LOKI ask_holmes 08_sock_shop_frontend ask_holmes 09_crashpod ask_holmes 10_image_pull_backoff ask_holmes 11_init_containers ask_holmes 12_job_crashing ask_holmes 12_job_crashing_CORALOGIX ask_holmes 12_job_crashing_LOKI ask_holmes 13_pending_node_selector ask_holmes 14_pending_resources ask_holmes 15_failed_readiness_probe ask_holmes 16_failed_no_toolset_found ask_holmes 17_oom_kill ask_holmes 18_crash_looping_v2 ask_holmes 19_detect_missing_app_details ask_holmes 20_long_log_file_search_LOKI ask_holmes 21_job_fail_curl_no_svc_account ask_holmes 22_high_latency_dbi_down ask_holmes 23_app_error_in_current_logs ask_holmes 23_app_error_in_current_logs_LOKI ask_holmes 24_misconfigured_pvc ask_holmes 25_misconfigured_ingress_class ask_holmes 26_multi_container_logs ask_holmes 27_permissions_error_no_helm_tools ask_holmes 28_permissions_error_helm_tools_enabled ask_holmes 29_events_from_alert_manager ask_holmes 30_basic_promql_graph_cluster_memory ask_holmes 31_basic_promql_graph_pod_memory ask_holmes 32_basic_promql_graph_pod_cpu ask_holmes 33_http_latency_graph ask_holmes 34_memory_graph ask_holmes 35_tempo ask_holmes 36_argocd_find_resource ask_holmes 37_argocd_wrong_namespace ask_holmes 38_rabbitmq_split_head ask_holmes 39_failed_toolset ask_holmes 40_disabled_toolset ask_holmes 41_setup_argo investigate 01_oom_kill investigate 02_crashloop_backoff investigate 03_cpu_throttling investigate 04_image_pull_backoff investigate 05_crashpod investigate 05_crashpod_LOKI investigate 06_job_failure investigate 07_job_syntax_error investigate 08_memory_pressure investigate 09_high_latency investigate 10_KubeDeploymentReplicasMismatch investigate 11_KubePodCrashLooping investigate 12_KubePodNotReady investigate 13_Watchdog investigate 14_tempo <p>Legend</p> <ul> <li> the test was successful</li> <li> the test failed but is known to be flakky or known to fail</li> <li> the test failed and should be fixed before merging the PR</li> </ul>"},{"location":"evals-introduction/#why-evaluations-matter","title":"Why Evaluations Matter","text":"<p>Evaluations serve several critical purposes:</p> <ol> <li>Quality Assurance: Ensure HolmesGPT provides accurate diagnostics and recommendations</li> <li>Model Comparison: Compare performance across different LLM models (GPT-4, Claude, Gemini, etc.)</li> <li>Regression Testing: Catch performance degradations when updating code or dependencies</li> <li>Toolset Validation: Verify that new toolsets and integrations work correctly</li> <li>Continuous Improvement: Identify areas where HolmesGPT needs enhancement</li> </ol>"},{"location":"evals-introduction/#how-to-run-evaluations","title":"How to Run Evaluations","text":""},{"location":"evals-introduction/#basic-usage","title":"Basic Usage","text":"<p>Run all evaluations: <pre><code>pytest ./tests/llm/test_*.py\n</code></pre></p> <p>By default the tests load and present mock files to the LLM whenever it asks for them. If a mock file is not present for a tool call, the tool call is passed through to the live tool itself. In a lot of cases this can cause the eval to fail unless the live environment (k8s cluster) matches what the LLM expects.</p> <p>Run specific test suite: <pre><code>pytest ./tests/llm/test_ask_holmes.py\npytest ./tests/llm/test_investigate.py\n</code></pre></p> <p>Run a specific test case: <pre><code>pytest ./tests/llm/test_ask_holmes.py -k \"01_how_many_pods\"\n</code></pre></p> <p>It is possible to investigate and debug why an eval fails by the output provided in the console. The output includes the correctness score, the reasoning for the score, information about what tools were called, the expected answer, as well as the LLM's answer.</p>"},{"location":"evals-introduction/#environment-variables","title":"Environment Variables","text":"<p>Configure evaluations using these environment variables:</p> Variable Example Description <code>MODEL</code> <code>MODEL=anthropic/claude-3.5</code> Specify which LLM model to use <code>CLASSIFIER_MODEL</code> <code>CLASSIFIER_MODEL=gpt-4o</code> The LLM model to use for scoring the answer (LLM as judge). Supported LLM providers are OpenAI and Azure OpenAI. Defaults to <code>MODEL</code> <code>ITERATIONS</code> <code>ITERATIONS=3</code> Run each test multiple times for consistency checking <code>RUN_LIVE</code> <code>RUN_LIVE=true</code> Execute <code>before-test</code> and <code>after-test</code> commands, ignore mock files <code>BRAINTRUST_API_KEY</code> <code>BRAINTRUST_API_KEY=sk-1dh1...swdO02</code> API key for Braintrust integration <code>UPLOAD_DATASET</code> <code>UPLOAD_DATASET=true</code> Sync dataset to Braintrust (safe, separated by branch) <code>PUSH_EVALS_TO_BRAINTRUST</code> <code>PUSH_EVALS_TO_BRAINTRUST=true</code> Upload evaluation results to Braintrust <code>EXPERIMENT_ID</code> <code>EXPERIMENT_ID=my_baseline</code> Custom experiment name for result tracking"},{"location":"evals-introduction/#simple-example","title":"Simple Example","text":"<p>Run a comprehensive evaluation: <pre><code>export MODEL=gpt-4o\n\n# Run with parallel execution for speed\npytest -n 10 ./tests/llm/test_*.py\n</code></pre></p>"},{"location":"evals-introduction/#live-testing","title":"Live Testing","text":"<p>For tests that require actual Kubernetes resources: <pre><code>export RUN_LIVE=true\n\npytest ./tests/llm/test_ask_holmes.py -k \"specific_test\"\n</code></pre></p> <p>Live testing requires a Kubernetes cluster and will execute <code>before-test</code> and <code>after-test</code> commands to set up/tear down resources. Not all tests support live testing. Some tests require manual setup.</p>"},{"location":"evals-introduction/#model-comparison-workflow","title":"Model Comparison Workflow","text":"<ol> <li> <p>Create Baseline: Run evaluations with a reference model    <pre><code>EXPERIMENT_ID=baseline_gpt4o MODEL=gpt-4o pytest -n 10 ./tests/llm/test_*\n</code></pre></p> </li> <li> <p>Test New Model: Run evaluations with the model you want to compare    <pre><code>EXPERIMENT_ID=test_claude35 MODEL=anthropic/claude-3.5 pytest -n 10 ./tests/llm/test_*\n</code></pre></p> </li> <li> <p>Compare Results: Use Braintrust dashboard to analyze performance differences</p> </li> </ol>"},{"location":"evals-introduction/#writing-evaluations","title":"Writing Evaluations","text":"<p>For detailed information on creating new evaluations, see the Writing Evaluations Guide.</p>"},{"location":"evals-introduction/#reporting-and-analysis","title":"Reporting and Analysis","text":"<p>Learn how to analyze evaluation results using Braintrust in the Reporting Guide.</p>"},{"location":"evals-introduction/#troubleshooting","title":"Troubleshooting","text":""},{"location":"evals-introduction/#common-issues","title":"Common Issues","text":"<ol> <li>Missing BRAINTRUST_API_KEY: Some tests are skipped without this key</li> <li>Live test failures: Ensure Kubernetes cluster access and proper permissions</li> <li>Mock file mismatches: Regenerate mocks with <code>generate_mocks: true</code></li> <li>Timeout errors: Increase test timeout or check network connectivity</li> </ol>"},{"location":"evals-introduction/#debug-mode","title":"Debug Mode","text":"<p>Enable verbose output: <pre><code>pytest -v -s ./tests/llm/test_ask_holmes.py -k \"specific_test\"\n</code></pre></p> <p>This shows detailed output including: - Expected vs actual results - Tool calls made by the LLM - Evaluation scores and rationales - Debugging information</p>"},{"location":"evals-reporting/","title":"HolmesGPT Evaluations - Reporting with Braintrust","text":"<p>This guide explains how to use Braintrust to analyze evaluation results, debug failures, and compare model performance.</p> <ul> <li>Introduction to HolmesGPT's evals.</li> <li>Docs on writing your own evals.</li> </ul>"},{"location":"evals-reporting/#overview","title":"Overview","text":"<p>Braintrust is a platform for tracking and analyzing LLM evaluations. HolmesGPT evals can be used without Braintrust but using Braintrust has a few advantages:</p> <ul> <li>We can track how Holmes perform over time</li> <li>It's easier to run and debug many evals with Braintrust over simpler pytests because Braintrust organises the different components of a HolmesGPT investigation like the input, tool calls, reasoning for scoring, etc.</li> </ul>"},{"location":"evals-reporting/#setting-up-braintrust","title":"Setting Up Braintrust","text":""},{"location":"evals-reporting/#1-create-account","title":"1. Create Account","text":"<ol> <li>Visit braintrust.dev</li> <li>Sign up for an account</li> <li>Create a new project (e.g., \"HolmesGPT\")</li> </ol>"},{"location":"evals-reporting/#2-get-api-key","title":"2. Get API Key","text":"<ol> <li>Click your profile icon (top right)</li> <li>Go to Settings \u2192 API Keys</li> <li>Generate a new API key</li> <li>Copy the key (starts with <code>sk-</code>)</li> </ol>"},{"location":"evals-reporting/#3-configure-environment","title":"3. Configure Environment","text":"<pre><code>export BRAINTRUST_API_KEY=sk-your-api-key-here\n</code></pre>"},{"location":"evals-reporting/#running-evaluations-with-braintrust","title":"Running Evaluations with Braintrust","text":""},{"location":"evals-reporting/#basic-evaluation-run","title":"Basic Evaluation Run","text":"<pre><code>export BRAINTRUST_API_KEY=sk-your-key\nexport UPLOAD_DATASET=true\nexport PUSH_EVALS_TO_BRAINTRUST=true\n\npytest ./tests/llm/test_ask_holmes.py\n</code></pre>"},{"location":"evals-reporting/#named-experiment","title":"Named Experiment","text":"<pre><code>export EXPERIMENT_ID=baseline_gpt4o\nexport MODEL=gpt-4o\npytest -n 10 ./tests/llm/test_*.py\n</code></pre>"},{"location":"evals-reporting/#key-environment-variables","title":"Key Environment Variables","text":"Variable Purpose <code>UPLOAD_DATASET</code> Sync test cases to Braintrust <code>PUSH_EVALS_TO_BRAINTRUST</code> Upload evaluation results <code>EXPERIMENT_ID</code> Name your experiment run. This makes it easier to find and track in Braintrust's UI <code>MODEL</code> The LLM model for Holmes to use <code>CLASSIFIER_MODEL</code> The LLM model to use for scoring the answer (LLM as judge)"},{"location":"evals-reporting/#analyzing-evaluation-results","title":"Analyzing Evaluation Results","text":""},{"location":"evals-reporting/#output","title":"Output","text":"<p>The main Span of an evaluation will present the input (either the AlertManager issue or the user's question for Ask Holmes) as well as HolmesGPT's answer.</p> <p></p>"},{"location":"evals-reporting/#score-types","title":"Score Types","text":"<p>Correctness Score: - Measures accuracy of LLM responses - Values: 0 or 1 - Shows how well output matches expectations</p> <p></p>"},{"location":"evals-reporting/#debugging-failed-evaluations","title":"Debugging Failed Evaluations","text":""},{"location":"evals-reporting/#1-identify-failing-tests","title":"1. Identify Failing Tests","text":"<p>In the experiment view: - Sort by score (ascending) to see worst performers - Filter by specific score types - Look for patterns in failures</p>"},{"location":"evals-reporting/#2-examine-tool-call-traces","title":"2. Examine Tool Call Traces","text":"<p>Click on a failing test to see: - Input: The original prompt/question - Tool Calls: Which tools the LLM invoked - Tool Results: What data each tool returned - Output: The LLM's final response - Expected: What the test expected</p> <p></p>"},{"location":"evals-reporting/#3-common-failure-patterns","title":"3. Common Failure Patterns","text":"<p>Low Correctness Score: - LLM missed key information in tool outputs - Response doesn't address the core question - Factual errors in the analysis</p> <p>Low Context Score: - LLM didn't reference important context items - May indicate prompt engineering issues - Could suggest irrelevant context was provided</p> <p>Missing Tool Calls: - LLM didn't invoke necessary tools - Check if tool descriptions are clear - Verify mock data is realistic</p>"},{"location":"evals-reporting/#4-debug-example","title":"4. Debug Example","text":"<p>Failing Test: \"02_what_is_wrong_with_pod\" - Score: Correctness 0.2, Context 0.1 - Issue: LLM said \"pod is healthy\" but expected \"CrashLoopBackOff detected\"</p> <p>Investigation: 1. Check <code>kubectl_describe.txt</code> mock - contains correct CrashLoopBackOff status 2. Verify <code>kubectl_logs.txt</code> shows crash errors 3. Review LLM's tool calls - did it call <code>kubectl_describe</code>? 4. Examine LLM output - did it misinterpret the kubectl output?</p> <p>Solution: Update mock files to be more explicit about the crash status</p>"},{"location":"evals-writing/","title":"Writing HolmesGPT Evaluations","text":"<p>This guide explains how to create new evaluations for HolmesGPT. Evaluations test the system's ability to correctly diagnose issues and provide accurate recommendations.</p> <ul> <li>Introduction to HolmesGPT's evals.</li> <li>Use Braintrust to view analyze results (optional).</li> </ul>"},{"location":"evals-writing/#overview","title":"Overview","text":"<p>HolmesGPT supports two types of evaluations:</p> <ol> <li>Ask Holmes Tests: Chat-like interactions (<code>tests/llm/test_ask_holmes.py</code>)</li> <li>Investigation Tests: Issue analysis for events triggered by AlertManager (<code>tests/llm/test_investigate.py</code>)</li> </ol> <p>Each test consists of: - A test case definition (<code>test_case.yaml</code>) - Mock tool outputs (e.g., <code>kubectl_describe.txt</code>) - Optional Kubernetes manifests for live testing - Optional custom toolset configurations</p>"},{"location":"evals-writing/#high-level-steps","title":"High-Level Steps","text":"<ol> <li>Choose test type: Ask Holmes vs Investigation. Choose Ask Holmes for most use cases. Choose Investigations for issues triggered by AlertManager</li> <li>Create a test folder: Use numbered naming convention</li> <li>Define your test case:</li> <li>Create <code>test_case.yaml</code> with prompt and expectations</li> <li>Define kubectl or helm setup and teardown manifests</li> <li>Generate mock data: Using a live system</li> <li>Set evaluation criteria: Define minimum scores for test success</li> <li>Test and iterate: Run the test and refine as needed</li> </ol>"},{"location":"evals-writing/#step-by-step-example-creating-an-ask-holmes-test","title":"Step-by-Step Example: Creating an Ask Holmes Test","text":"<p>Let's create a simple test that asks about pod health status.</p>"},{"location":"evals-writing/#step-1-create-test-folder","title":"Step 1: Create Test Folder","text":"<pre><code>mkdir tests/llm/fixtures/test_ask_holmes/99_pod_health_check\ncd tests/llm/fixtures/test_ask_holmes/99_pod_health_check\n</code></pre>"},{"location":"evals-writing/#step-2-create-test_caseyaml","title":"Step 2: Create test_case.yaml","text":"<pre><code>user_prompt: 'Is the nginx pod healthy?'\nexpected_output:\n  - nginx pod is healthy\nevaluation:\n  correctness: 1\nbefore_test: kubectl apply -f ./manifest.yaml\nafter_test: kubectl delete -f ./manifest.yaml\n</code></pre> <ul> <li><code>user_prompt</code>: This is the question that will trigger Holmes' investigation</li> <li><code>expected_output</code>: This is a list of expected elements that MUST be found in Holmes' answer. The combination of these elements lead to a <code>correctness</code> score based on HolmesGPT's output. This <code>expected_output</code> will be compared against HolmesGPT's answer and evaluated by a LLM ('LLM as judge'). The resulting score is called <code>correctness</code> and is a binary score with a value of either <code>0</code> or <code>1</code>. HolmesGPT's answer is score <code>0</code> is any of the expected element is not present in the answer, <code>1</code> if all expected elements are preent in the answer.</li> <li><code>evaluation.correctness</code>: This is the expected correctness score and is used for pytest to fail the test. This expected <code>correctness</code> score should be <code>0</code> unless you expect HolmesGPT to systematically succeed the evaluation. Because of this, it is important for <code>expected_output</code> to be reduced to the minimally accepted output from HolmesGPT.</li> <li><code>before_test</code> and <code>after_test</code>: These are setup and teardown steps to reproduce the test on a fresh environment. It is important for these to be present because as HolmesGPT's code, prompt and toolset evolve the mocks become insufficient or inaccurate. These scripts are run automatically when the env var <code>RUN_LIVE=true</code> is set</li> </ul>"},{"location":"evals-writing/#step-3-generate-mock-tool-outputs","title":"Step 3: Generate Mock Tool Outputs","text":"<p>Create mock files that simulate kubectl command outputs.</p> <p>The best way to do this is to:</p> <ol> <li>Deploy the test case you want to build an eval for in a kubernetes cluster (run the <code>before_test</code> script manually)</li> <li>Configure HolmesGPT to connect to the cluster (via kubectl and any other relevant toolsets)</li> <li>Enable the auto generation of mock files by setting <code>generate_mocks: True</code> in your <code>test_case.yaml</code></li> <li>Repeatedly run the eval with <code>ITERATIONS=100 pytest tests/llm/test_ask_holmes.py -k 99_pod_health_check</code></li> <li>Removing the prefix <code>.AUTOGENERATED</code> from all autogenerated files</li> </ol>"},{"location":"evals-writing/#step-4-run-the-test","title":"Step 4: Run the Test","text":"<pre><code>pytest ./tests/llm/test_ask_holmes.py -k \"99_pod_health_check\" -v\n</code></pre>"},{"location":"evals-writing/#test-case-configuration-reference","title":"Test Case Configuration Reference","text":""},{"location":"evals-writing/#common-fields","title":"Common Fields","text":"Field Type Required Description <code>user_prompt</code> string Yes The question or prompt for HolmesGPT <code>expected_output</code> string or list Yes Expected elements in the response <code>evaluation</code> dict No Minimum scores for test to pass"},{"location":"evals-writing/#ask-holmes-specific-fields","title":"Ask Holmes Specific Fields","text":"Field Type Description <code>before_test</code> string Command to run before test (requires <code>RUN_LIVE=true</code>) <code>after_test</code> string Command to run after test cleanup <code>generate_mocks</code> boolean Whether to generate new mock files"},{"location":"evals-writing/#investigation-specific-fields","title":"Investigation Specific Fields","text":"Field Type Description <code>expected_sections</code> dict Required/prohibited sections in output"},{"location":"evals-writing/#example-complex-investigation-test","title":"Example: Complex Investigation Test","text":"<pre><code>user_prompt: \"Investigate this CrashLoopBackOff issue\"\nexpected_output:\n  - Pod is experiencing CrashLoopBackOff\n  - Container exits with code 1 due to configuration error\n  - Missing environment variable DATABASE_URL\nexpected_sections:\n  \"Root Cause Analysis\":\n    - CrashLoopBackOff\n    - configuration error\n  \"Recommended Actions\": true\n  \"External Links\": false\nevaluation:\n  correctness: 0\nbefore_test: kubectl apply -f ./manifest.yaml\nafter_test: kubectl delete -f ./manifest.yaml\n</code></pre>"},{"location":"evals-writing/#mock-file-generation","title":"Mock File Generation","text":""},{"location":"evals-writing/#automatic-generation","title":"Automatic Generation","text":"<p>Set <code>generate_mocks: true</code> in <code>test_case.yaml</code> and run with a live cluster:</p> <pre><code>ITERATIONS=100 pytest ./tests/llm/test_ask_holmes.py -k \"your_test\"\n</code></pre> <p>This captures real tool outputs and saves them as mock files.</p>"},{"location":"evals-writing/#manual-creation","title":"Manual Creation","text":"<p>Create files matching the tool names used by HolmesGPT:</p> <ul> <li><code>kubectl_describe.txt</code> - Pod/resource descriptions</li> <li><code>kubectl_logs.txt</code> - Container logs</li> <li><code>kubectl_events.txt</code> - Kubernetes events</li> <li><code>prometheus_query.txt</code> - Metrics data</li> <li><code>fetch_loki_logs.txt</code> - Log aggregation results</li> </ul>"},{"location":"evals-writing/#naming-convention","title":"Naming Convention","text":"<p>Mock files follow the pattern: <code>{tool_name}_{additional_context}.txt</code></p> <p>Examples: - <code>kubectl_describe_pod_nginx_default.txt</code> - <code>kubectl_logs_all_containers_nginx.txt</code> - <code>execute_prometheus_range_query.txt</code></p>"},{"location":"evals-writing/#toolset-configuration","title":"Toolset Configuration","text":"<p>Some tests require specific toolsets. Create a <code>toolsets.yaml</code> file:</p> <pre><code>toolsets:\n  - name: kubernetes\n    enabled: true\n  - name: prometheus\n    enabled: true\n    config:\n      prometheus_url: http://localhost:9090 # requires port-forward\n  - name: grafana_loki\n    enabled: true\n    config:\n      base_url: http://localhost:3000 # requires port-forward\n      api_key: \"{{env.GRAFANA_API_KEY}}\"\n      grafana_datasource_uid: \"{{env.GRAFANA_DATASOURCE_UID}}\"\n</code></pre>"},{"location":"evals-writing/#live-testing-with-real-resources","title":"Live Testing with Real Resources","text":"<p>For tests that need actual Kubernetes resources:</p>"},{"location":"evals-writing/#step-1-create-manifest","title":"Step 1: Create Manifest","text":"<p>manifest.yaml: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: test-nginx\n  template:\n    metadata:\n      labels:\n        app: test-nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.20\n        ports:\n        - containerPort: 80\n</code></pre></p>"},{"location":"evals-writing/#step-2-configure-setupteardown","title":"Step 2: Configure Setup/Teardown","text":"<pre><code>user_prompt: 'How is the test-nginx deployment performing?'\nbefore-test: kubectl apply -f manifest.yaml\nafter-test: kubectl delete -f manifest.yaml\n# ... rest of configuration\n</code></pre>"},{"location":"evals-writing/#step-3-run-live-test","title":"Step 3: Run Live Test","text":"<pre><code>RUN_LIVE=true pytest ./tests/llm/test_ask_holmes.py -k \"your_test\"\n</code></pre> <p><code>RUN_LIVE</code> is currently incompatible with <code>ITERATIONS</code> &gt; 1.</p>"},{"location":"evals-writing/#evaluation-scoring","title":"Evaluation Scoring","text":""},{"location":"evals-writing/#correctness-score","title":"Correctness Score","text":"<p>Measures how well the output matches expected elements: - 1: Match - 0: Mismatch</p>"},{"location":"evals-writing/#setting-minimum-scores","title":"Setting Minimum Scores","text":"<pre><code>evaluation:\n  correctness: 1\n</code></pre>"},{"location":"evals-writing/#best-practices","title":"Best Practices","text":""},{"location":"evals-writing/#test-design","title":"Test Design","text":"<ol> <li>Start simple: Begin with basic scenarios before complex edge cases</li> <li>Clear expectations: Write specific, measurable expected outputs</li> <li>Realistic scenarios: Base tests on actual user problems</li> <li>Incremental complexity: Build from simple to complex test cases</li> </ol>"},{"location":"evals-writing/#mock-data-quality","title":"Mock Data Quality","text":"<ol> <li>Representative data: Use realistic kubectl outputs and logs</li> <li>Error scenarios: Include failure modes and edge cases</li> <li>Consistent formatting: Match actual tool output formats</li> <li>Sufficient detail: Include enough information for proper diagnosis</li> <li>Run repeatedly: Run mock generation many times to ensure all investigative paths are covered by mock files</li> </ol>"},{"location":"evals-writing/#troubleshooting-test-creation","title":"Troubleshooting Test Creation","text":""},{"location":"evals-writing/#common-issues","title":"Common Issues","text":"<p>Test always fails with low correctness score: - Check if expected_output matches actual LLM capabilities - Verify mock data provides sufficient information - Consider lowering score threshold temporarily</p> <p>Missing tool outputs: - Ensure mock files are named correctly - Check that required toolsets are enabled - Verify mock file content is properly formatted</p> <p>Inconsistent results: - Run multiple iterations: <code>ITERATIONS=5</code> - Check for non-deterministic elements in prompts - Consider using temperature=0 for more consistent outputs</p>"},{"location":"evals-writing/#debugging-commands","title":"Debugging Commands","text":"<pre><code># Verbose output showing all details\npytest -v -s ./tests/llm/test_ask_holmes.py -k \"your_test\"\n\n# Generate fresh mocks from live system\n# set `generate_mocks: True` in test_case.yaml` and then:\npytest ./tests/llm/test_ask_holmes.py -k \"your_test\"\n</code></pre> <p>This completes the evaluation writing guide. The next step is setting up reporting and analysis using Braintrust.</p>"},{"location":"installation/","title":"Installing HolmesGPT","text":"<p>You can install HolmesGPT in one of the follow three methods:</p> <ol> <li>Standalone: Run HolmesGPT from your terminal as a CLI tool. Typically installed with Homebrew or Pip/Pipx. Ideal for local use, embedding into shell scripts, or CI/CD pipelines. (E.g. to analyze why a pipeline deploying to Kubernetes failed.)</li> <li>Web UIs and TUIs: HolmesGPT is embedded in several third-party tools, like Robusta SaaS and K9s (as a plugin).</li> <li>API: Embed HolmesGPT in your own app to quickly add root-cause-analysis functionality and data correlations across multiple sources like logs, metrics, and events. HolmesGPT exposes an HTTP API and Python SDK, as well as Helm chart to deploy the HTTP server on Kubernetes.</li> </ol>"},{"location":"installation/#standalone","title":"Standalone","text":""},{"location":"installation/#brew-maclinux","title":"Brew (Mac/Linux)","text":"<ol> <li>Add our tap:</li> </ol> <pre><code>brew tap robusta-dev/homebrew-holmesgpt\n</code></pre> <ol> <li>Install holmesgpt:</li> </ol> <pre><code>brew install holmesgpt\n</code></pre> <ol> <li>Check that installation was successful. This will take a few seconds on the first run - wait patiently.:</li> </ol> <pre><code>holmes --help\n</code></pre> <ol> <li>Apply an example Pod to Kubernetes with an error that Holmes can investigate:</li> </ol> <pre><code>kubectl apply -f https://raw.githubusercontent.com/robusta-dev/kubernetes-demos/main/pending_pods/pending_pod_node_selector.yaml\n</code></pre> <ol> <li> <p>Setup an API key</p> </li> <li> <p>Run holmesgpt:</p> </li> </ol> <pre><code>holmes ask \"what is wrong with the user-profile-import pod?\"\n</code></pre>"},{"location":"installation/#docker-container","title":"Docker Container","text":"<p>You can run HolmesGPT via a prebuilt Docker container:</p> <pre><code>docker.pkg.dev/genuine-flight-317411/devel/holmes\n</code></pre> <p>Here is an example, that mounts relevant config files so that HolmesGPT can use kubectl and other tools:</p> <pre><code>docker run -it --net=host -v ~/.holmes:/root/.holmes -v ~/.aws:/root/.aws -v ~/.config/gcloud:/root/.config/gcloud -v $HOME/.kube/config:/root/.kube/config us-central1-docker.pkg.dev/genuine-flight-317411/devel/holmes ask \"what pods are unhealthy and why?\"\n</code></pre> <p>Don't forget to Setup an API key first.</p>"},{"location":"installation/#pip-and-pipx","title":"Pip and Pipx","text":"<p>You can install HolmesGPT from source with pip or pipx. Pipx is recommended, as it prevents dependency conflicts.</p> <p>First Pipx</p> <p>Then install HolmesGPT from git:</p> <pre><code>pipx install \"https://github.com/robusta-dev/holmesgpt/archive/refs/heads/master.zip\"\n</code></pre> <p>Verify that HolmesGPT was installed by checking the version:</p> <pre><code>holmes version\n</code></pre> <p>Setup an API key and start testing HolmesGPT:</p> <pre><code>holmes ask \"what pods are unhealthy and why?\"\n</code></pre> <p>When new versions of HolmesGPT are released, you can upgrade HolmesGPT with pipx:</p> <pre><code>pipx upgrade holmesgpt\n</code></pre>"},{"location":"installation/#from-source-python-poetry","title":"From Source (Python Poetry)","text":"<p>First install poetry (the python package manager)</p> <pre><code>git clone https://github.com/robusta-dev/holmesgpt.git\ncd holmesgpt\npoetry install --no-root\n</code></pre> <p>Setup an API key and run HolmesGPT:</p> <pre><code>poetry run python3 holmes.py ask \"what pods are unhealthy and why?\"\n</code></pre>"},{"location":"installation/#from-source-docker","title":"From Source (Docker)","text":"<p>Clone the project from github, setup an API key, and then run:</p> <pre><code>cd holmesgpt\ndocker build -t holmes . -f Dockerfile.dev\ndocker run -it --net=host -v -v ~/.holmes:/root/.holmes -v ~/.aws:/root/.aws -v ~/.config/gcloud:/root/.config/gcloud -v $HOME/.kube/config:/root/.kube/config holmes ask \"what pods are unhealthy and why?\"\n</code></pre>"},{"location":"installation/#web-uis-and-tuis","title":"Web UIs and TUIs","text":"<ul> <li>Robusta SaaS - Managed service with web UI</li> <li>K9s Plugin - Terminal UI for Kubernetes</li> </ul>"},{"location":"installation/#api","title":"API","text":"<ul> <li>Helm Chart - Deploy on Kubernetes</li> <li>Python API - Use as a Python library</li> </ul>"},{"location":"k9s/","title":"Using HolmesGPT in K9s","text":"<p>Add the following contents to the K9s plugin file, typically <code>~/.config/k9s/plugins.yaml</code> on Linux and <code>~/Library/Application Support/k9s/plugins.yaml</code> on Mac. Read more about K9s plugins here and check your plugin path here.</p> <p>Note: HolmesGPT must be installed and configured for the K9s plugin to work.</p> <p>Basic plugin to run an investigation on any Kubernetes object, using the shortcut <code>Shift + H</code>:</p> <pre><code>plugins:\n  holmesgpt:\n    shortCut: Shift-H\n    description: Ask HolmesGPT\n    scopes:\n      - all\n    command: bash\n    background: false\n    confirm: false\n    args:\n      - -c\n      - |\n        holmes ask \"why is $NAME of $RESOURCE_NAME in -n $NAMESPACE not working as expected\"\n        echo \"Press 'q' to exit\"\n        while : ; do\n        read -n 1 k &lt;&amp;1\n        if [[ $k = q ]] ; then\n        break\n        fi\n        done\n</code></pre> <p>Advanced plugin that lets you modify the questions HolmesGPT asks about the LLM, using the shortcut <code>Shift + O</code>. (E.g. you can change the question to \"generate an HPA for this deployment\" and the AI will follow those instructions and output an HPA configuration.) <pre><code>plugins:\n  custom-holmesgpt:\n    shortCut: Shift-Q\n    description: Custom HolmesGPT Ask\n    scopes:\n      - all\n    command: bash\n\n      - |\n        INSTRUCTIONS=\"# Edit the line below. Lines starting with '#' will be ignored.\"\n        DEFAULT_ASK_COMMAND=\"why is $NAME of $RESOURCE_NAME in -n $NAMESPACE not working as expected\"\n        QUESTION_FILE=$(mktemp)\n\n        echo \"$INSTRUCTIONS\" &gt; \"$QUESTION_FILE\"\n        echo \"$DEFAULT_ASK_COMMAND\" &gt;&gt; \"$QUESTION_FILE\"\n\n        # Open the line in the default text editor\n        ${EDITOR:-nano} \"$QUESTION_FILE\"\n\n        # Read the modified line, ignoring lines starting with '#'\n        user_input=$(grep -v '^#' \"$QUESTION_FILE\")\n        echo running: holmes ask \"\\\"$user_input\\\"\"\n\n        holmes ask \"$user_input\"\n        echo \"Press 'q' to exit\"\n        while : ; do\n        read -n 1 k &lt;&amp;1\n        if [[ $k = q ]] ; then\n        break\n        fi\n        done\n</code></pre></p>"},{"location":"python/","title":"Using HolmesGPT as a Python Library","text":"<p>You can use HolmesGPT as a library, if you want to:</p> <ul> <li>Use some custom LLM implementation not supported by HolmesGPT or LiteLLM</li> <li>Build a complex workflow not supported by HolmesGPT itself, while re-using all of Holmes's integrations and investigation capabilities</li> </ul> <p>First install the library using pip, pipx, or poetry. Then see an example implementation here.</p>"},{"location":"ai-providers/","title":"AI Providers","text":"<p>HolmesGPT supports multiple AI providers, giving you flexibility in choosing the best model for your needs and budget.</p> <ul> <li> Anthropic</li> <li> AWS Bedrock</li> <li> Azure OpenAI</li> <li> Gemini</li> <li> Google Vertex AI</li> <li> Ollama</li> <li> OpenAI</li> <li> OpenAI-Compatible</li> </ul>"},{"location":"ai-providers/#quick-start","title":"Quick Start","text":"<p>Recommended for New Users</p> <p>OpenAI GPT-4o provides the best balance of accuracy and reliability. Get started with:</p> <ol> <li>Get an OpenAI API key</li> <li>Set <code>export OPENAI_API_KEY=\"your-api-key\"</code></li> <li>Run <code>holmes ask \"what pods are failing?\"</code></li> </ol> <p>Choose your provider above to see detailed configuration instructions.</p>"},{"location":"ai-providers/anthropic/","title":"Anthropic","text":"<p>Configure HolmesGPT to use Anthropic's Claude models.</p>"},{"location":"ai-providers/anthropic/#setup","title":"Setup","text":"<p>Get an Anthropic API key.</p>"},{"location":"ai-providers/anthropic/#configuration","title":"Configuration","text":""},{"location":"ai-providers/anthropic/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code>export ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\nholmes ask \"what pods are unhealthy and why?\" --model=\"anthropic/claude-3-opus-20240229\"\n</code></pre>"},{"location":"ai-providers/anthropic/#using-cli-arguments","title":"Using CLI Arguments","text":"<pre><code>holmes ask \"what pods are unhealthy and why?\" --model=\"anthropic/claude-3-opus-20240229\" --api-key=\"your-anthropic-api-key\"\n</code></pre>"},{"location":"ai-providers/anthropic/#available-models","title":"Available Models","text":"<pre><code># Claude 3 Opus\nholmes ask \"analyze this complex deployment failure\" --model=\"anthropic/claude-3-opus-20240229\"\n\n# Claude 3 Sonnet\nholmes ask \"what pods are failing?\" --model=\"anthropic/claude-3-sonnet-20240229\"\n\n# Claude 3 Haiku\nholmes ask \"cluster status summary\" --model=\"anthropic/claude-3-haiku-20240307\"\n</code></pre>"},{"location":"ai-providers/anthropic/#troubleshooting","title":"Troubleshooting","text":"<p>Authentication Error <pre><code>Error: Invalid Anthropic API key\n</code></pre> - Verify your API key is correct - Check that your account has sufficient credits</p> <p>Model Not Found <pre><code>Error: Model not available\n</code></pre> - Verify the model name is spelled correctly - Check that you have access to the requested model</p> <p>Rate Limits <pre><code>Error: Rate limit exceeded\n</code></pre> - Wait for the rate limit to reset or upgrade your account</p>"},{"location":"ai-providers/aws-bedrock/","title":"AWS Bedrock","text":"<p>Configure HolmesGPT to use AWS Bedrock foundation models.</p>"},{"location":"ai-providers/aws-bedrock/#configuration","title":"Configuration","text":""},{"location":"ai-providers/aws-bedrock/#required-environment-variables","title":"Required Environment Variables","text":"<pre><code>export AWS_REGION_NAME=\"us-east-1\"\nexport AWS_ACCESS_KEY_ID=\"your-access-key\"\nexport AWS_SECRET_ACCESS_KEY=\"your-secret-key\"\n</code></pre>"},{"location":"ai-providers/aws-bedrock/#using-cli-arguments","title":"Using CLI Arguments","text":"<pre><code>holmes ask \"what pods are unhealthy and why?\" --model=bedrock/&lt;MODEL_NAME&gt;\n</code></pre>"},{"location":"ai-providers/aws-bedrock/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code>export MODEL=\"bedrock/&lt;MODEL_NAME&gt;\"\nholmes ask \"analyze deployment failure\"\n</code></pre>"},{"location":"ai-providers/aws-bedrock/#examples","title":"Examples","text":"<pre><code># Claude 3.5 Sonnet\nholmes ask \"analyze deployment failure\" # with MODEL already set\n\n# Claude 3 Opus\nholmes ask \"complex multi-service issue\" --model=bedrock/anthropic.claude-3-opus-20240229-v1:0\n\n# Claude 3 Haiku\nholmes ask \"quick status check\" --model=bedrock/anthropic.claude-3-haiku-20240307-v1:0\n</code></pre>"},{"location":"ai-providers/aws-bedrock/#troubleshooting","title":"Troubleshooting","text":"<p>Model Access Denied <pre><code>Error: Model access denied\n</code></pre> - Request access to the model in the Bedrock console - Wait for approval - Verify you're using the correct model identifier</p> <p>Region Errors <pre><code>Error: Model not available in region\n</code></pre> - Check if the model is available in your selected region - Switch to a supported region - Use <code>aws bedrock list-foundation-models</code> to check availability</p> <p>Authentication Issues <pre><code>Error: AWS credentials not found\n</code></pre> - Verify your AWS credentials are configured - Check environment variables are set correctly - Ensure IAM permissions include <code>bedrock:InvokeModel</code></p> <p>Throttling <pre><code>Error: Rate limit exceeded\n</code></pre> - Bedrock has built-in rate limiting - Wait and retry - Consider provisioned throughput for high-volume use cases</p>"},{"location":"ai-providers/azure-openai/","title":"Azure OpenAI","text":"<p>Configure HolmesGPT to use Azure OpenAI Service.</p>"},{"location":"ai-providers/azure-openai/#setup","title":"Setup","text":"<p>Create an Azure OpenAI resource.</p>"},{"location":"ai-providers/azure-openai/#configuration","title":"Configuration","text":""},{"location":"ai-providers/azure-openai/#required-environment-variables","title":"Required Environment Variables","text":"<pre><code>export AZURE_API_VERSION=\"2024-02-15-preview\"\nexport AZURE_API_BASE=\"https://my-org.openai.azure.com/\"\nexport AZURE_API_KEY=\"your-azure-api-key\"  # Optional, can use --api-key instead\n</code></pre>"},{"location":"ai-providers/azure-openai/#usage","title":"Usage","text":"<pre><code>holmes ask \"what pods are unhealthy and why?\" --model=azure/&lt;DEPLOYMENT_MODEL_NAME&gt; --api-key=&lt;API_KEY&gt;\n</code></pre> <p>Replace <code>&lt;DEPLOYMENT_NAME&gt;</code> with your actual Azure OpenAI deployment name.</p>"},{"location":"ai-providers/azure-openai/#available-models","title":"Available Models","text":"<p>Azure OpenAI uses deployment names instead of model names:</p> <pre><code># Using gpt-4 deployment\nholmes ask \"analyze cluster issues\" --model=azure/gpt-4-deployment\n\n# Using gpt-35-turbo deployment\nholmes ask \"quick cluster check\" --model=azure/gpt-35-turbo-deployment\n</code></pre>"},{"location":"ai-providers/azure-openai/#troubleshooting","title":"Troubleshooting","text":"<p>Authentication Errors <pre><code>Error: Azure authentication failed\n</code></pre> - Verify environment variables are set correctly - Check that your Azure OpenAI resource is deployed - Ensure your API key has the correct permissions</p> <p>Deployment Not Found <pre><code>Error: Deployment not found\n</code></pre> - Verify your deployment name is correct - Check that the deployment is in the same region as your API base - Ensure the deployment is fully deployed and running</p> <p>Regional Issues <pre><code>Error: Model not available in region\n</code></pre> - Some models are only available in specific regions - Consider deploying in a different region</p>"},{"location":"ai-providers/gemini/","title":"Gemini","text":"<p>Configure HolmesGPT to use Google's Gemini models via Google AI Studio.</p>"},{"location":"ai-providers/gemini/#setup","title":"Setup","text":"<p>Get your API key from Google AI Studio.</p>"},{"location":"ai-providers/gemini/#configuration","title":"Configuration","text":""},{"location":"ai-providers/gemini/#environment-variables","title":"Environment Variables","text":"<pre><code>export GEMINI_API_KEY=\"your-gemini-api-key\"\n</code></pre>"},{"location":"ai-providers/gemini/#usage","title":"Usage","text":"<pre><code>holmes ask \"what pods are unhealthy and why?\" --model=gemini/&lt;MODEL_NAME&gt;\n</code></pre>"},{"location":"ai-providers/gemini/#available-models","title":"Available Models","text":"<pre><code># Gemini Pro (standard model)\nholmes ask \"analyze cluster issues\" --model=gemini/gemini-pro\n\n# Gemini 1.5 Flash (fast and efficient)\nholmes ask \"quick diagnostics\" --model=gemini/gemini-1.5-flash\n\n# Gemini 1.5 Pro (most capable)\nholmes ask \"complex analysis\" --model=gemini/gemini-1.5-pro\n</code></pre>"},{"location":"ai-providers/gemini/#troubleshooting","title":"Troubleshooting","text":"<p>API Key Issues <pre><code>Error: Invalid API key\n</code></pre> - Verify your API key is correct and active - Check that you've enabled the Gemini API - Ensure the key hasn't been revoked or expired</p> <p>Rate Limiting <pre><code>Error: Rate limit exceeded\n</code></pre> - Wait for the rate limit to reset - Consider upgrading to a paid tier for higher limits</p> <p>Model Not Found <pre><code>Error: Model not available\n</code></pre> - Verify the model name is spelled correctly - Check that you have access to the requested model - Some models may be in preview and require special access</p> <p>Quota Exceeded <pre><code>Error: Quota exceeded\n</code></pre> - Check your daily/monthly quota limits - Wait for the quota to reset - Upgrade your plan for higher limits</p>"},{"location":"ai-providers/google-vertex-ai/","title":"Google Vertex AI","text":"<p>Configure HolmesGPT to use Google Vertex AI with Gemini models.</p>"},{"location":"ai-providers/google-vertex-ai/#setup","title":"Setup","text":"<ol> <li>Create a Google Cloud project with Vertex AI API enabled</li> <li>Create a service account with <code>Vertex AI User</code> role</li> <li>Download the JSON key file</li> </ol>"},{"location":"ai-providers/google-vertex-ai/#configuration","title":"Configuration","text":""},{"location":"ai-providers/google-vertex-ai/#required-environment-variables","title":"Required Environment Variables","text":"<pre><code>export VERTEXAI_PROJECT=\"your-project-id\"\nexport VERTEXAI_LOCATION=\"us-central1\"\nexport GOOGLE_APPLICATION_CREDENTIALS=\"path/to/your/service_account_key.json\"\n</code></pre>"},{"location":"ai-providers/google-vertex-ai/#usage","title":"Usage","text":"<pre><code>holmes ask \"what pods are unhealthy and why?\" --model=\"vertex_ai/&lt;MODEL_NAME&gt;\"\n</code></pre>"},{"location":"ai-providers/google-vertex-ai/#available-models","title":"Available Models","text":"<pre><code># Gemini Pro\nholmes ask \"analyze deployment issues\" --model=\"vertex_ai/gemini-pro\"\n\n# Gemini 2.0 Flash Experimental\nholmes ask \"complex troubleshooting\" --model=\"vertex_ai/gemini-2.0-flash-exp\"\n\n# Gemini 1.5 Flash\nholmes ask \"quick cluster check\" --model=\"vertex_ai/gemini-1.5-flash\"\n</code></pre>"},{"location":"ai-providers/google-vertex-ai/#authentication-methods","title":"Authentication Methods","text":""},{"location":"ai-providers/google-vertex-ai/#service-account-recommended","title":"Service Account (Recommended)","text":"<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/service-account-key.json\"\n</code></pre>"},{"location":"ai-providers/google-vertex-ai/#application-default-credentials","title":"Application Default Credentials","text":"<p>For Google Cloud environments:</p> <pre><code>gcloud auth application-default login\n</code></pre>"},{"location":"ai-providers/google-vertex-ai/#workload-identity-gke","title":"Workload Identity (GKE)","text":"<p>Configure workload identity in your deployment:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  annotations:\n    iam.gke.io/gcp-service-account: holmes@project.iam.gserviceaccount.com\n</code></pre> <p>Regional Availability</p> <p>Vertex AI Gemini models are available in specific regions:</p> <ul> <li>us-central1 - Primary region with all models</li> <li>us-east1 - Secondary region</li> <li>europe-west1 - European region</li> <li>asia-southeast1 - Asia-Pacific region</li> </ul>"},{"location":"ai-providers/google-vertex-ai/#troubleshooting","title":"Troubleshooting","text":"<p>Authentication Errors <pre><code>Error: Could not automatically determine credentials\n</code></pre> - Verify <code>GOOGLE_APPLICATION_CREDENTIALS</code> points to valid JSON key - Check that the service account has necessary permissions - Ensure the Vertex AI API is enabled in your project</p> <p>Project Configuration <pre><code>Error: Project not found or access denied\n</code></pre> - Verify <code>VERTEXAI_PROJECT</code> is set to the correct project ID - Check that Vertex AI API is enabled - Ensure billing is enabled for the project</p> <p>Regional Issues <pre><code>Error: Model not available in region\n</code></pre> - Some models are only available in specific regions - Try switching to <code>us-central1</code> or another supported region</p> <p>Permission Errors <pre><code>Error: Permission denied\n</code></pre> - Ensure service account has <code>Vertex AI User</code> role - Check that the account has access to the specific model - Verify project permissions and billing status</p> <p>Quota Exceeded <pre><code>Error: Quota exceeded\n</code></pre> - Check your quota limits in Google Cloud Console - Request quota increases if needed</p>"},{"location":"ai-providers/ollama/","title":"Ollama","text":"<p>Configure HolmesGPT to use local models with Ollama.</p> <p>Warning</p> <p>Ollama is supported, but buggy. We recommend using other models if you can, until Ollama tool-calling capabilities improve. Specifically, Ollama often calls tools with non-existent or missing parameters.</p>"},{"location":"ai-providers/ollama/#setup","title":"Setup","text":"<ol> <li>Download Ollama from ollama.com</li> <li>Install following the instructions for your operating system</li> <li>Start Ollama service:    <pre><code>ollama serve\n</code></pre></li> </ol>"},{"location":"ai-providers/ollama/#download-models","title":"Download Models","text":"<pre><code># Popular models for troubleshooting\nollama pull llama3.1\nollama pull codellama\nollama pull mistral\n</code></pre>"},{"location":"ai-providers/ollama/#configuration","title":"Configuration","text":""},{"location":"ai-providers/ollama/#method-1-ollama-native-format","title":"Method 1: Ollama Native Format","text":"<pre><code>export OLLAMA_API_BASE=\"http://localhost:11434\"\nholmes ask \"what pods are unhealthy in my cluster?\" --model=\"ollama_chat/llama3.1\"\n</code></pre>"},{"location":"ai-providers/ollama/#method-2-openai-compatible-format","title":"Method 2: OpenAI-Compatible Format","text":"<pre><code># Note the v1 at the end\nexport OPENAI_API_BASE=\"http://localhost:11434/v1\"\n# Holmes requires OPENAI_API_KEY to be set but value does not matter\nexport OPENAI_API_KEY=123\nholmes ask \"what pods are unhealthy in my cluster?\" --model=\"openai/llama3.1\"\n</code></pre>"},{"location":"ai-providers/ollama/#model-usage","title":"Model Usage","text":"<pre><code># Using different models\nholmes ask \"pod analysis\" --model=\"ollama_chat/llama3.1:8b\"\nholmes ask \"yaml debugging\" --model=\"ollama_chat/codellama:7b\"\nholmes ask \"quick check\" --model=\"ollama_chat/mistral:7b\"\n</code></pre>"},{"location":"ai-providers/ollama/#known-limitations","title":"Known Limitations","text":"<p>Current problems with Ollama:</p> <ol> <li>Missing parameters - Tools called without required arguments</li> <li>Invalid parameters - Non-existent parameter names</li> <li>Inconsistent behavior - Results may vary between runs</li> <li>Limited function following - May not follow tool schemas correctly</li> </ol>"},{"location":"ai-providers/ollama/#troubleshooting","title":"Troubleshooting","text":"<p>Ollama Not Running <pre><code>Error: Connection refused\n</code></pre> - Start Ollama service: <code>ollama serve</code> - Check if port 11434 is available - Verify firewall settings</p> <p>Model Not Found <pre><code>Error: Model not found\n</code></pre> - Pull the model: <code>ollama pull model-name</code> - List available models: <code>ollama list</code> - Check model name spelling</p> <p>Memory Issues <pre><code>Error: Out of memory\n</code></pre> - Choose a smaller model - Close other applications - Add more RAM or use GPU acceleration</p> <p>Slow Performance <pre><code>Taking too long to respond\n</code></pre> - Use smaller models (7B instead of 13B/70B) - Enable GPU acceleration - Increase CPU allocation: <code>export OLLAMA_NUM_THREAD=8</code></p>"},{"location":"ai-providers/openai-compatible/","title":"OpenAI-Compatible Models","text":"<p>Configure HolmesGPT to use any OpenAI-compatible API.</p> <p>Function Calling Required</p> <p>Your model and inference server must support function calling (tool calling). HolmesGPT cannot check if the LLM supports function-calling. Models that lack this capability may hallucinate answers instead of properly using tools.</p>"},{"location":"ai-providers/openai-compatible/#requirements","title":"Requirements","text":"<ol> <li>Function calling support - The model must support OpenAI-style tool calling</li> <li>OpenAI-compatible API - Standard endpoints and request/response format</li> </ol>"},{"location":"ai-providers/openai-compatible/#compatible-inference-servers","title":"Compatible Inference Servers","text":"<p>\u2705 Supported: - llama-cpp-python - Supports function calling - Text Generation WebUI - With OpenAI extension - LocalAI - Full OpenAI compatibility - FastChat - OpenAI-compatible server</p> <p>\u274c Not Supported: - vLLM - No function calling support yet</p>"},{"location":"ai-providers/openai-compatible/#configuration","title":"Configuration","text":""},{"location":"ai-providers/openai-compatible/#basic-setup","title":"Basic Setup","text":"<pre><code>export OPENAI_API_BASE=\"http://localhost:8000/v1\"\nholmes ask \"what pods are unhealthy and why?\" --model=openai/&lt;MODEL_NAME&gt; --api-key=&lt;API_KEY&gt;\n</code></pre>"},{"location":"ai-providers/openai-compatible/#environment-variables","title":"Environment Variables","text":"<pre><code># Required: API base URL\nexport OPENAI_API_BASE=\"http://your-server:8000/v1\"\n\n# Optional: API key (use random value if not required)\nexport OPENAI_API_KEY=\"your-api-key-or-random-value\"\n</code></pre>"},{"location":"ai-providers/openai-compatible/#common-setups","title":"Common Setups","text":""},{"location":"ai-providers/openai-compatible/#llama-cpp-python-server","title":"llama-cpp-python Server","text":"<pre><code># Install with function calling support\npip install 'llama-cpp-python[server]'\n\n# Start server with function calling enabled\npython -m llama_cpp.server \\\n  --model model.gguf \\\n  --host 0.0.0.0 \\\n  --port 8000 \\\n  --chat_format chatml\n</code></pre> <p>Configure HolmesGPT:</p> <pre><code>export OPENAI_API_BASE=\"http://localhost:8000/v1\"\nexport OPENAI_API_KEY=\"not-needed\"\nholmes ask \"what's wrong with my pods?\" --model=openai/model\n</code></pre>"},{"location":"ai-providers/openai-compatible/#localai","title":"LocalAI","text":"<pre><code># Start LocalAI\ndocker run -p 8080:8080 --name local-ai -ti localai/localai:latest\n\n# Download a model\ncurl http://localhost:8080/models/apply -H \"Content-Type: application/json\" -d '{\n  \"id\": \"model-gallery@llama3-instruct\",\n  \"name\": \"llama3-instruct\"\n}'\n</code></pre> <p>Configure HolmesGPT:</p> <pre><code>export OPENAI_API_BASE=\"http://localhost:8080/v1\"\nexport OPENAI_API_KEY=\"not-needed\"\nholmes ask \"cluster health check\" --model=openai/llama3-instruct\n</code></pre>"},{"location":"ai-providers/openai-compatible/#text-generation-webui","title":"Text Generation WebUI","text":"<pre><code># Start with OpenAI extension\npython server.py --extensions openai --listen --api-port 5000\n</code></pre> <p>Configure HolmesGPT:</p> <pre><code>export OPENAI_API_BASE=\"http://localhost:5000/v1\"\nexport OPENAI_API_KEY=\"not-needed\"\nholmes ask \"analyze my deployment\" --model=openai/your-loaded-model\n</code></pre>"},{"location":"ai-providers/openai-compatible/#testing-function-calling","title":"Testing Function Calling","text":"<p>Verify your setup:</p> <pre><code># Simple test that should call kubectl tools\nholmes ask \"list all pods in default namespace\" --model=openai/your-model\n\n# Check if tools are being called properly\nholmes ask \"describe the first pod you find\" --model=openai/your-model\n</code></pre>"},{"location":"ai-providers/openai-compatible/#expected-behavior","title":"Expected Behavior","text":"<p>\u2705 Working correctly: - HolmesGPT calls kubectl commands - Returns actual cluster data - Provides specific pod names and details</p> <p>\u274c Not working: - Generic responses without real data - \"I cannot access your cluster\" messages - Hallucinated pod names or information</p>"},{"location":"ai-providers/openai-compatible/#troubleshooting","title":"Troubleshooting","text":"<p>Model Doesn't Call Tools <pre><code>Response: \"I cannot access your Kubernetes cluster\"\n</code></pre> - Verify the model supports function calling - Check if the inference server properly handles tool calls - Try a different model known to support function calling</p> <p>Invalid Tool Parameters <pre><code>Error: Tool called with missing required parameter\n</code></pre> - The model may not understand the tool schema properly - Try a larger or more capable model - Check inference server logs for schema issues</p> <p>Connection Refused <pre><code>Error: Connection refused to localhost:8000\n</code></pre> - Ensure the inference server is running - Check the port number and host - Verify firewall settings</p> <p>API Compatibility Issues <pre><code>Error: Unexpected response format\n</code></pre> - Verify the server implements OpenAI-compatible API - Check API version compatibility - Review server logs for errors</p>"},{"location":"ai-providers/openai/","title":"OpenAI","text":"<p>Configure HolmesGPT to use OpenAI's GPT models.</p>"},{"location":"ai-providers/openai/#setup","title":"Setup","text":"<p>Get a paid OpenAI API key.</p> <p>Note</p> <p>This requires a paid OpenAI API key, not a ChatGPT Plus subscription.</p>"},{"location":"ai-providers/openai/#configuration","title":"Configuration","text":""},{"location":"ai-providers/openai/#using-cli-arguments","title":"Using CLI Arguments","text":"<pre><code>holmes ask --api-key=\"your-openai-api-key\" \"what pods are crashing in my cluster and why?\"\n</code></pre>"},{"location":"ai-providers/openai/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code>export OPENAI_API_KEY=\"your-openai-api-key\"\nholmes ask \"what pods are crashing in my cluster and why?\"\n</code></pre>"},{"location":"ai-providers/openai/#available-models","title":"Available Models","text":"<p>OpenAI is the default provider. Specify models with the <code>--model</code> flag:</p> <pre><code># Default model (gpt-4o)\nholmes ask \"what pods are failing?\"\n\n# Specific models\nholmes ask \"what pods are failing?\" --model=\"gpt-4\"\nholmes ask \"what pods are failing?\" --model=\"gpt-3.5-turbo\"\n</code></pre>"},{"location":"ai-providers/openai/#troubleshooting","title":"Troubleshooting","text":"<p>Invalid API Key <pre><code>Error: Invalid OpenAI API key\n</code></pre> - Verify your API key is correct - Ensure your account has sufficient credits</p> <p>Rate Limits <pre><code>Error: Rate limit exceeded\n</code></pre> - Wait for the limit to reset or upgrade your OpenAI account</p> <p>Insufficient Credits <pre><code>Error: You exceeded your current quota\n</code></pre> - Add payment method to your OpenAI account</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Advanced configuration options for optimizing HolmesGPT performance, security, and integration with your environment.</p>"},{"location":"configuration/#configuration-areas","title":"Configuration Areas","text":""},{"location":"configuration/#core-configuration","title":"Core Configuration","text":"<ul> <li>Helm Configuration - Complete Helm values reference</li> <li>Security &amp; Permissions - RBAC, secrets, and access control</li> </ul>"},{"location":"configuration/#performance-monitoring","title":"Performance &amp; Monitoring","text":"<ul> <li>Performance Tuning - Optimize AI provider usage and response times</li> <li>Monitoring &amp; Observability - Monitor HolmesGPT itself</li> </ul>"},{"location":"configuration/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Troubleshooting - Common issues and solutions</li> </ul>"},{"location":"configuration/#configuration-priority","title":"Configuration Priority","text":"<p>HolmesGPT configuration follows this hierarchy:</p> <ol> <li>Helm values - Primary configuration method</li> <li>Environment variables - Runtime overrides</li> <li>ConfigMaps - Shared configuration</li> <li>Secrets - Sensitive data (API keys, tokens)</li> </ol>"},{"location":"configuration/#quick-configuration-checklist","title":"Quick Configuration Checklist","text":"<p>Essential settings to configure:</p> <ul> <li> AI Provider - Choose and configure your preferred AI service</li> <li> Data Sources - Enable relevant toolsets for your infrastructure</li> <li> Security - Set up proper RBAC and secret management</li> <li> Monitoring - Enable observability for HolmesGPT operations</li> </ul>"},{"location":"configuration/#advanced-topics","title":"Advanced Topics","text":"<p>Once you have basic configuration working:</p> <ul> <li>Resource limits - Tune CPU and memory allocation</li> <li>Scaling - Configure horizontal pod autoscaling</li> <li>Network policies - Restrict network access</li> <li>Backup &amp; recovery - Protect configuration and data</li> </ul>"},{"location":"configuration/#configuration-examples","title":"Configuration Examples","text":"<p>Common configuration patterns:</p> Production SetupDevelopment SetupMulti-Tenant Setup <ul> <li>Multiple AI providers for redundancy</li> <li>Comprehensive monitoring</li> <li>Strict security policies</li> <li>Performance optimization</li> </ul> <ul> <li>Single AI provider</li> <li>Relaxed security for testing</li> <li>Debug logging enabled</li> <li>Cost optimization</li> </ul> <ul> <li>Namespace isolation</li> <li>Per-tenant AI provider configuration</li> <li>Shared monitoring infrastructure</li> </ul> <p>Start with Helm Configuration for the complete configuration reference.</p>"},{"location":"data-sources/","title":"Data Sources","text":"<p>HolmesGPT can incorporate data sources from various tools to improve its root cause analysis.</p> <ul> <li> <p>Built-in Toolsets</p> </li> <li> <p>Custom Toolsets</p> </li> <li> <p>Remote MCP Servers</p> </li> </ul>"},{"location":"data-sources/custom-toolsets/","title":"Custom Toolsets","text":"<p>If the built-in toolsets don't meet your needs, you can extend HolmesGPT's investigation capabilities by creating custom toolsets. This is especially useful for unique use cases, proprietary tools, or specialized infrastructure setups. Examples include advanced log analysis tools, external monitoring integrations, or custom diagnostic scripts.</p> <p>By creating custom toolsets, you can ensure HolmesGPT has access to all the data sources and tools necessary for thorough investigations in your specific environment.</p>"},{"location":"data-sources/custom-toolsets/#examples","title":"Examples","text":"<p>Below are three examples of how to create custom toolsets for different scenarios.</p>"},{"location":"data-sources/custom-toolsets/#example-1-grafana-toolset","title":"Example 1: Grafana Toolset","text":"<p>This example creates a toolset that helps HolmesGPT view and suggest relevant Grafana dashboards.</p> Helm ChartCLI <p>Helm Values:</p> <pre><code>holmes:\n  customToolsets:\n    grafana:\n      description: \"View and suggest Grafana dashboards\"\n      prerequisites: \"Grafana instance accessible from HolmesGPT\"\n      tags: [monitoring, observability]\n      installation: |\n        1. Ensure Grafana is accessible from HolmesGPT\n        2. Configure Grafana API credentials if authentication is required\n      tools:\n        - name: view_dashboard\n          description: \"View a specific Grafana dashboard by ID or name\"\n          command: |\n            curl -s \"{{ grafana_url }}/api/dashboards/uid/{{ dashboard_uid }}\" \\\n              -H \"Authorization: Bearer {{ grafana_token }}\"\n          additionalInstructions: |\n            Parse the JSON response to extract dashboard information.\n            If dashboard is not found, suggest similar dashboards.\n\n        - name: search_dashboards\n          description: \"Search for dashboards related to specific keywords\"\n          command: |\n            curl -s \"{{ grafana_url }}/api/search?query={{ search_query }}\" \\\n              -H \"Authorization: Bearer {{ grafana_token }}\"\n          additionalInstructions: |\n            Return the most relevant dashboards based on the search query.\n            Include dashboard URLs for easy access.\n</code></pre> <p>Environment Variables:</p> <pre><code>export GRAFANA_URL=\"http://grafana.monitoring.svc.cluster.local:3000\"\nexport GRAFANA_TOKEN=\"your-grafana-api-token\"\n</code></pre> <p>Helm Upgrade:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre> <p>Configuration File (<code>toolsets.yaml</code>):</p> <pre><code>toolsets:\n  grafana:\n    description: \"View and suggest Grafana dashboards\"\n    prerequisites: \"Grafana instance accessible from HolmesGPT\"\n    tags: [monitoring, observability]\n    installation: |\n      1. Ensure Grafana is accessible from HolmesGPT\n      2. Configure Grafana API credentials if authentication is required\n    tools:\n      - name: view_dashboard\n        description: \"View a specific Grafana dashboard by ID or name\"\n        command: |\n          curl -s \"${GRAFANA_URL}/api/dashboards/uid/{{ dashboard_uid }}\" \\\n            -H \"Authorization: Bearer ${GRAFANA_TOKEN}\"\n        additionalInstructions: |\n          Parse the JSON response to extract dashboard information.\n          If dashboard is not found, suggest similar dashboards.\n\n      - name: search_dashboards\n        description: \"Search for dashboards related to specific keywords\"\n        command: |\n          curl -s \"${GRAFANA_URL}/api/search?query={{ search_query }}\" \\\n            -H \"Authorization: Bearer ${GRAFANA_TOKEN}\"\n        additionalInstructions: |\n          Return the most relevant dashboards based on the search query.\n          Include dashboard URLs for easy access.\n</code></pre> <p>Environment Variables:</p> <pre><code>export GRAFANA_URL=\"http://grafana.monitoring.svc.cluster.local:3000\"\nexport GRAFANA_TOKEN=\"your-grafana-api-token\"\n</code></pre> <p>Run HolmesGPT:</p> <pre><code>holmes ask \"show me dashboards related to CPU usage\" --toolsets=toolsets.yaml\n</code></pre>"},{"location":"data-sources/custom-toolsets/#example-2-kubernetes-diagnostics-toolset","title":"Example 2: Kubernetes Diagnostics Toolset","text":"<p>This example creates a toolset with advanced diagnostic tools for Kubernetes clusters.</p> Helm ChartCLI <p>Helm Values:</p> <pre><code>holmes:\n  customToolsets:\n    k8s-diagnostics:\n      description: \"Advanced Kubernetes diagnostic tools\"\n      prerequisites: \"kubectl access to the cluster\"\n      tags: [kubernetes, diagnostics]\n      installation: |\n        1. Ensure kubectl is configured with cluster access\n        2. Verify necessary RBAC permissions are in place\n      tools:\n        - name: check_node_pressure\n          description: \"Check for node pressure conditions and resource usage\"\n          command: |\n            kubectl get nodes -o json | jq -r '\n              .items[] |\n              select(.status.conditions[]? | select(.type == \"MemoryPressure\" or .type == \"DiskPressure\" or .type == \"PIDPressure\") | .status == \"True\") |\n              .metadata.name + \": \" + (.status.conditions[] | select(.type == \"MemoryPressure\" or .type == \"DiskPressure\" or .type == \"PIDPressure\") | .type + \" = \" + .status)\n            '\n          additionalInstructions: |\n            If any nodes show pressure conditions, investigate further and suggest remediation steps.\n\n        - name: analyze_pod_distribution\n          description: \"Analyze pod distribution across nodes in a namespace\"\n          command: |\n            kubectl get pods -n {{ namespace }} -o wide --no-headers |\n            awk '{print $7}' | sort | uniq -c | sort -nr\n          additionalInstructions: |\n            Check for uneven pod distribution that might indicate scheduling issues.\n            Suggest rebalancing if necessary.\n\n        - name: check_resource_quotas\n          description: \"Check resource quota usage in a namespace\"\n          command: |\n            kubectl describe resourcequota -n {{ namespace }}\n          additionalInstructions: |\n            Alert if resource quotas are close to limits. Suggest scaling or quota adjustments.\n</code></pre> <p>Helm Upgrade:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre> <p>Configuration File (<code>toolsets.yaml</code>):</p> <pre><code>toolsets:\n  k8s-diagnostics:\n    description: \"Advanced Kubernetes diagnostic tools\"\n    prerequisites: \"kubectl access to the cluster\"\n    tags: [kubernetes, diagnostics]\n    installation: |\n      1. Ensure kubectl is configured with cluster access\n      2. Verify necessary RBAC permissions are in place\n    tools:\n      - name: check_node_pressure\n        description: \"Check for node pressure conditions and resource usage\"\n        command: |\n          kubectl get nodes -o json | jq -r '\n            .items[] |\n            select(.status.conditions[]? | select(.type == \"MemoryPressure\" or .type == \"DiskPressure\" or .type == \"PIDPressure\") | .status == \"True\") |\n            .metadata.name + \": \" + (.status.conditions[] | select(.type == \"MemoryPressure\" or .type == \"DiskPressure\" or .type == \"PIDPressure\") | .type + \" = \" + .status)\n          '\n        additionalInstructions: |\n          If any nodes show pressure conditions, investigate further and suggest remediation steps.\n\n      - name: analyze_pod_distribution\n        description: \"Analyze pod distribution across nodes in a namespace\"\n        command: |\n          kubectl get pods -n {{ namespace }} -o wide --no-headers |\n          awk '{print $7}' | sort | uniq -c | sort -nr\n        additionalInstructions: |\n          Check for uneven pod distribution that might indicate scheduling issues.\n          Suggest rebalancing if necessary.\n\n      - name: check_resource_quotas\n        description: \"Check resource quota usage in a namespace\"\n        command: |\n          kubectl describe resourcequota -n {{ namespace }}\n        additionalInstructions: |\n          Alert if resource quotas are close to limits. Suggest scaling or quota adjustments.\n</code></pre> <p>Run HolmesGPT:</p> <pre><code>holmes ask \"check for any resource pressure in the cluster\" --toolsets=toolsets.yaml\n</code></pre>"},{"location":"data-sources/custom-toolsets/#example-3-github-toolset","title":"Example 3: GitHub Toolset","text":"<p>This example shows how to create a toolset for fetching information from GitHub repositories.</p> Helm ChartCLI <p>Helm Values:</p> <pre><code>holmes:\n  customToolsets:\n    github:\n      description: \"Fetch information from GitHub repositories\"\n      prerequisites: \"GitHub API token with repository access\"\n      tags: [source-control, github]\n      installation: |\n        1. Create a GitHub personal access token\n        2. Set the token as an environment variable\n        3. Ensure network access to GitHub API\n      tools:\n        - name: get_repository_info\n          description: \"Get information about a GitHub repository\"\n          command: |\n            curl -s -H \"Authorization: token {{ github_token }}\" \\\n              \"https://api.github.com/repos/{{ owner }}/{{ repo }}\"\n          additionalInstructions: |\n            Extract relevant repository information like description, language, last update.\n            Check for any security alerts or issues.\n\n        - name: get_recent_commits\n          description: \"Get recent commits from a repository\"\n          command: |\n            curl -s -H \"Authorization: token {{ github_token }}\" \\\n              \"https://api.github.com/repos/{{ owner }}/{{ repo }}/commits?per_page={{ limit | default(10) }}\"\n          additionalInstructions: |\n            Show commit messages, authors, and timestamps.\n            Look for patterns that might relate to the current issue.\n\n        - name: search_issues\n          description: \"Search for issues in a repository\"\n          command: |\n            curl -s -H \"Authorization: token {{ github_token }}\" \\\n              \"https://api.github.com/search/issues?q=repo:{{ owner }}/{{ repo }}+{{ search_query }}\"\n          additionalInstructions: |\n            Find relevant issues that might be related to the current problem.\n            Include issue titles, states, and URLs.\n</code></pre> <p>Environment Variables:</p> <pre><code>export GITHUB_TOKEN=\"your-github-personal-access-token\"\n</code></pre> <p>Helm Upgrade:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre> <p>Configuration File (<code>toolsets.yaml</code>):</p> <pre><code>toolsets:\n  github:\n    description: \"Fetch information from GitHub repositories\"\n    prerequisites: \"GitHub API token with repository access\"\n    tags: [source-control, github]\n    installation: |\n      1. Create a GitHub personal access token\n      2. Set the token as an environment variable\n      3. Ensure network access to GitHub API\n    tools:\n      - name: get_repository_info\n        description: \"Get information about a GitHub repository\"\n        command: |\n          curl -s -H \"Authorization: token ${GITHUB_TOKEN}\" \\\n            \"https://api.github.com/repos/{{ owner }}/{{ repo }}\"\n        additionalInstructions: |\n          Extract relevant repository information like description, language, last update.\n          Check for any security alerts or issues.\n\n      - name: get_recent_commits\n        description: \"Get recent commits from a repository\"\n        command: |\n          curl -s -H \"Authorization: token ${GITHUB_TOKEN}\" \\\n            \"https://api.github.com/repos/{{ owner }}/{{ repo }}/commits?per_page={{ limit | default(10) }}\"\n        additionalInstructions: |\n          Show commit messages, authors, and timestamps.\n          Look for patterns that might relate to the current issue.\n\n      - name: search_issues\n        description: \"Search for issues in a repository\"\n        command: |\n          curl -s -H \"Authorization: token ${GITHUB_TOKEN}\" \\\n            \"https://api.github.com/search/issues?q=repo:{{ owner }}/{{ repo }}+{{ search_query }}\"\n        additionalInstructions: |\n          Find relevant issues that might be related to the current problem.\n          Include issue titles, states, and URLs.\n</code></pre> <p>Environment Variables:</p> <pre><code>export GITHUB_TOKEN=\"your-github-personal-access-token\"\n</code></pre> <p>Run HolmesGPT:</p> <pre><code>holmes ask \"check recent commits in robusta-dev/robusta repository\" --toolsets=toolsets.yaml\n</code></pre>"},{"location":"data-sources/custom-toolsets/#reference","title":"Reference","text":""},{"location":"data-sources/custom-toolsets/#toolset-configuration","title":"Toolset Configuration","text":"<p>A custom toolset consists of the following components:</p> <pre><code>toolsets:\n  &lt;toolset-name&gt;:\n    description: \"Human-readable description\"\n    prerequisites: \"What needs to be installed/configured\"\n    tags: [tag1, tag2]  # Optional: for categorization\n    installation: |\n      Multi-line installation instructions\n    tools:\n      - name: tool_name\n        description: \"What this tool does\"\n        command: |\n          Command or script to execute\n        parameters:  # Optional: can be inferred by LLM\n          - name: param_name\n            description: \"Parameter description\"\n        additionalInstructions: |\n          Instructions for post-processing the command output\n</code></pre>"},{"location":"data-sources/custom-toolsets/#tool-configuration","title":"Tool Configuration","text":"<p>Each tool within a toolset can be configured with:</p> <ul> <li>name: Unique identifier for the tool</li> <li>description: What the tool does (visible to the AI)</li> <li>command: Shell command or script to execute</li> <li>parameters: Optional parameter definitions (usually inferred)</li> <li>additionalInstructions: How to interpret/process the output</li> </ul>"},{"location":"data-sources/custom-toolsets/#variable-syntax","title":"Variable Syntax","text":"<p>HolmesGPT supports two types of variables in commands:</p> <ul> <li><code>{{ variable }}</code>: Dynamic variables inferred by the LLM based on context</li> <li><code>${VARIABLE}</code>: Environment variables (not visible to the LLM)</li> </ul>"},{"location":"data-sources/custom-toolsets/#tags","title":"Tags","text":"<p>Optional tags help categorize toolsets:</p> <ul> <li>core: Essential system tools</li> <li>cluster: Cluster-specific tools</li> <li>monitoring: Observability tools</li> <li>networking: Network-related tools</li> <li>storage: Storage-related tools</li> </ul>"},{"location":"data-sources/custom-toolsets/#advanced-adding-custom-binaries","title":"Advanced: Adding Custom Binaries","text":"<p>If your custom toolset requires additional binaries not available in the base HolmesGPT image, you can extend the Docker image:</p>"},{"location":"data-sources/custom-toolsets/#create-a-custom-dockerfile","title":"Create a Custom Dockerfile","text":"<pre><code>FROM us-central1-docker.pkg.dev/genuine-flight-317411/devel/holmes:latest\n\n# Install additional tools\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    your-custom-tool \\\n    another-binary \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Copy custom scripts\nCOPY scripts/ /usr/local/bin/\n\n# Make scripts executable\nRUN chmod +x /usr/local/bin/*.sh\n</code></pre>"},{"location":"data-sources/custom-toolsets/#build-and-push-your-image","title":"Build and Push Your Image","text":"<pre><code>docker build -t your-registry/holmes-custom:latest .\ndocker push your-registry/holmes-custom:latest\n</code></pre>"},{"location":"data-sources/custom-toolsets/#use-custom-image-in-helm-values","title":"Use Custom Image in Helm Values","text":"<pre><code>holmes:\n  image:\n    repository: your-registry/holmes-custom\n    tag: latest\n  customToolsets:\n    # Your custom toolset configuration\n</code></pre> <p>This approach allows you to include any additional tools or dependencies your custom toolsets might need.</p>"},{"location":"data-sources/permissions/","title":"Adding Permissions for Additional Resources","text":"<p>HolmesGPT may require access to additional Kubernetes resources or CRDs for specific analyses. Permissions can be extended by modifying the ClusterRole rules. The default configuration has limited resource access.</p>"},{"location":"data-sources/permissions/#common-scenarios-for-adding-permissions","title":"Common Scenarios for Adding Permissions","text":"<ol> <li>External Integrations and CRDs - Access to custom resources from ArgoCD, Istio, etc.</li> <li>Additional Kubernetes resources - Resources not included in the default permissions</li> </ol>"},{"location":"data-sources/permissions/#example-scenario-adding-argo-cd-permissions","title":"Example Scenario: Adding Argo CD Permissions","text":"<p>To enable HolmesGPT to analyze ArgoCD applications and projects, you need to add permissions for ArgoCD custom resources.</p>"},{"location":"data-sources/permissions/#steps-to-add-permissions","title":"Steps to Add Permissions","text":"<ol> <li> <p>Update <code>generated_values.yaml</code> with custom cluster role rules:</p> <pre><code>enableHolmesGPT: true\nholmes:\n  customClusterRoleRules:\n    - apiGroups: [\"argoproj.io\"]\n      resources: [\"applications\", \"appprojects\"]\n      verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> </li> <li> <p>Apply configuration using Helm:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre> </li> </ol>"},{"location":"data-sources/permissions/#key-benefits","title":"Key Benefits","text":"<ul> <li>Enables HolmesGPT to analyze specific Kubernetes resources</li> <li>Allows interaction with custom resources and CRDs</li> <li>Provides more comprehensive troubleshooting capabilities</li> </ul> <p>The configuration provides flexibility to extend HolmesGPT's permissions to suit specific cluster and tooling requirements.</p>"},{"location":"data-sources/remote-mcp-servers/","title":"Remote MCP Servers","text":"<p>Warning</p> <p>Remote MCP servers are in Tech Preview stage.</p> <p>HolmesGPT can integrate with remote MCP servers using SSE mode. This capability enables HolmesGPT to access external data sources and tools in real time. This guide provides step-by-step instructions for configuring HolmesGPT to connect with remote MCP servers over SSE.</p>"},{"location":"data-sources/remote-mcp-servers/#example-mcp-server-configuration","title":"Example: MCP server configuration","text":"Helm Chart <p>Helm Values:</p> <pre><code>holmes:\n  mcp_servers:\n    mcp_server_1:\n      # human-readable description of the mcp server (this is not seen by the AI model - its just for users)\n      description: \"Remote mcp server\"\n      url: \"http://example.com:8000/sse\"\n\n    mcp_server_2:\n      description: \"MCP server that runs in my cluster\"\n      url: \"http://&lt;service-name&gt;.&lt;namespace&gt;.svc.cluster.local:&lt;service-port&gt;\"\n      config:\n        headers:\n          key: \"{{ env.my_mcp_server_key }}\" # You can use holmes environment variables as headers for the MCP server requests.\n</code></pre> <p>Update your Helm values with the provided YAML configuration, then apply the changes with Helm upgrade:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre>"},{"location":"data-sources/builtin-toolsets/","title":"Built-in Toolsets","text":"<p>Holmes allows defining integrations (toolsets) that fetch data from external sources. Some toolsets are enabled by default, while others require the user to add their own configuration/credentials.</p>"},{"location":"data-sources/builtin-toolsets/#available-toolsets","title":"Available Toolsets","text":"<ul> <li> ArgoCD</li> <li> AWS</li> <li> Confluence</li> <li> Coralogix logs</li> <li> DataDog</li> <li> Datetime</li> <li> Docker</li> <li> GitHub</li> <li> Grafana Loki</li> <li> Grafana Tempo</li> <li> Helm</li> <li> Internet</li> <li> Kafka</li> <li> Kubernetes</li> <li> Notion</li> <li> New Relic</li> <li> OpenSearch logs</li> <li> OpenSearch status</li> <li> Prometheus</li> <li> RabbitMQ</li> <li> Robusta</li> <li> Slab</li> <li> ServiceNow</li> <li> Azure Kubernetes Service</li> <li> AKS Node Health</li> <li> Model Context Protocol</li> </ul>"},{"location":"data-sources/builtin-toolsets/#getting-started","title":"Getting Started","text":"<ol> <li>Review the toolsets relevant to your infrastructure</li> <li>Configure authentication for external services</li> <li>Test investigations to see which data sources are accessed</li> </ol> <p>Some toolsets work automatically with Kubernetes, while external services require API keys or credentials to be configured.</p>"},{"location":"data-sources/builtin-toolsets/aks-node-health/","title":"AKS Node Health","text":"<p>By enabling this toolset, HolmesGPT will be able to perform specialized health checks and troubleshooting for Azure Kubernetes Service (AKS) nodes, including node-specific diagnostics and performance analysis.</p>"},{"location":"data-sources/builtin-toolsets/aks-node-health/#prerequisites","title":"Prerequisites","text":"<ol> <li>Azure CLI installed and configured</li> <li>Appropriate Azure RBAC permissions for AKS clusters</li> <li>Access to the target AKS cluster</li> <li>Node-level access permissions</li> </ol>"},{"location":"data-sources/builtin-toolsets/aks-node-health/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, ensure you're authenticated with Azure:</p> <pre><code>az login\naz account set --subscription \"&lt;your subscription id&gt;\"\n</code></pre> <p>Then add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n  aks/node-health:\n    enabled: true\n    config:\n      subscription_id: \"&lt;your Azure subscription ID&gt;\"\n      resource_group: \"&lt;your AKS resource group&gt;\"\n      cluster_name: \"&lt;your AKS cluster name&gt;\"\n</code></pre> <pre><code>holmes:\n  toolsets:\n    aks/node-health:\n      enabled: true\n      config:\n        subscription_id: \"&lt;your Azure subscription ID&gt;\"\n        resource_group: \"&lt;your AKS resource group&gt;\"\n        cluster_name: \"&lt;your AKS cluster name&gt;\"\n</code></pre>"},{"location":"data-sources/builtin-toolsets/aks-node-health/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can configure additional health check parameters:</p> <pre><code>toolsets:\n  aks/node-health:\n    enabled: true\n    config:\n      subscription_id: \"&lt;your Azure subscription ID&gt;\"\n      resource_group: \"&lt;your AKS resource group&gt;\"\n      cluster_name: \"&lt;your AKS cluster name&gt;\"\n      health_check_interval: 300  # Health check interval in seconds\n      max_unhealthy_nodes: 3  # Maximum number of unhealthy nodes to report\n</code></pre>"},{"location":"data-sources/builtin-toolsets/aks-node-health/#capabilities","title":"Capabilities","text":"Tool Name Description aks_check_node_health Perform comprehensive health checks on AKS nodes aks_get_node_metrics Get detailed metrics for AKS nodes aks_diagnose_node_issues Diagnose common node-level issues aks_check_node_readiness Check if nodes are ready and schedulable aks_get_node_events Get events related to specific nodes aks_check_node_resources Check resource utilization on nodes"},{"location":"data-sources/builtin-toolsets/aks/","title":"Azure Kubernetes Service (AKS)","text":"<p>By enabling this toolset, HolmesGPT will be able to interact with Azure Kubernetes Service clusters, providing Azure-specific troubleshooting capabilities and cluster management.</p>"},{"location":"data-sources/builtin-toolsets/aks/#prerequisites","title":"Prerequisites","text":"<ol> <li>Azure CLI installed and configured</li> <li>Appropriate Azure RBAC permissions for AKS clusters</li> <li>Access to the target AKS cluster</li> </ol>"},{"location":"data-sources/builtin-toolsets/aks/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, ensure you're authenticated with Azure:</p> <pre><code>az login\naz account set --subscription \"&lt;your subscription id&gt;\"\n</code></pre> <p>Then add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n  aks/core:\n    enabled: true\n    config:\n      subscription_id: \"&lt;your Azure subscription ID&gt;\"\n      resource_group: \"&lt;your AKS resource group&gt;\"\n      cluster_name: \"&lt;your AKS cluster name&gt;\"\n</code></pre> <pre><code>holmes:\n  toolsets:\n    aks/core:\n      enabled: true\n      config:\n        subscription_id: \"&lt;your Azure subscription ID&gt;\"\n        resource_group: \"&lt;your AKS resource group&gt;\"\n        cluster_name: \"&lt;your AKS cluster name&gt;\"\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/aks/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can configure additional Azure settings:</p> <pre><code>toolsets:\n  aks/core:\n    enabled: true\n    config:\n      subscription_id: \"&lt;your Azure subscription ID&gt;\"\n      resource_group: \"&lt;your AKS resource group&gt;\"\n      cluster_name: \"&lt;your AKS cluster name&gt;\"\n      location: \"eastus\"  # Azure region\n      timeout: 60  # Request timeout in seconds\n</code></pre>"},{"location":"data-sources/builtin-toolsets/aks/#capabilities","title":"Capabilities","text":"Tool Name Description aks_get_cluster_info Get detailed information about the AKS cluster aks_get_node_pools List and describe AKS node pools aks_get_cluster_credentials Get cluster credentials for kubectl access aks_scale_node_pool Scale a specific node pool aks_get_cluster_logs Fetch AKS cluster logs aks_get_addon_status Get status of AKS addons"},{"location":"data-sources/builtin-toolsets/argocd/","title":"ArgoCD","text":"<p>By enabling this toolset, HolmesGPT will be able to fetch the status, deployment history, and configuration of ArgoCD applications.</p>"},{"location":"data-sources/builtin-toolsets/argocd/#configuration","title":"Configuration","text":"<p>This toolset requires an <code>ARGOCD_AUTH_TOKEN</code> environment variable. Generate such auth token by following these steps.</p> <p>You can consult the available environment variables on ArgoCD's official documentation for the CLI.</p> <p>The permissions required are below (<code>kubectl edit configmap argocd-rbac-cm -n argocd</code>). You can consult ArgoCD's documentation on user creation and permissions.</p> <pre><code># Ensure this data block is present in your argocd-rbac-cm configmap.\n# It enables the permissions for holmes to fetch the data it needs to\n# investigate argocd issues.\n#\n# These permissions depend on a new user `holmesgpt` being created,\n# for example using the `argocd-cm` configmap\ndata:\n  policy.default: role:readonly\n  policy.csv: |\n    p, role:admin, *, *, *, allow\n    p, role:admin, accounts, apiKey, *, allow\n    p, holmesgpt, accounts, apiKey, holmesgpt, allow\n    p, holmesgpt, projects, get, *, allow\n    p, holmesgpt, applications, get, *, allow\n    p, holmesgpt, repositories, get, *, allow\n    p, holmesgpt, clusters, get, *, allow\n    p, holmesgpt, applications, manifests, */*, allow\n    p, holmesgpt, applications, resources, */*, allow\n    g, admin, role:admin\n</code></pre>"},{"location":"data-sources/builtin-toolsets/argocd/#capabilities","title":"Capabilities","text":"<p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p> Tool Name Description argocd_app_list List ArgoCD applications argocd_app_get Get details of a specific ArgoCD application argocd_app_diff Show differences between live and desired state argocd_app_manifests Get manifests for an ArgoCD application argocd_app_resources Get resources for an ArgoCD application <p>Need Custom Integration?</p> <p>By adding custom toolsets, users can extend HolmesGPT's investigation capabilities to address unique use cases, specific infrastructure setups, or organization-specific requirements. For example, custom toolsets might include specialized log analysis patterns or integration with external monitoring systems.</p> <p>Can't find the toolset you need? Create a custom toolset or request a new integration.</p>"},{"location":"data-sources/builtin-toolsets/aws/","title":"AWS","text":""},{"location":"data-sources/builtin-toolsets/aws/#security","title":"Security","text":"<p>Set of tools to audit AWS CloudTrail events and audit logs.</p>"},{"location":"data-sources/builtin-toolsets/aws/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, add the following environment variables:</p> <pre><code>export AWS_ACCESS_KEY_ID=\"&lt;your AWS access key ID&gt;\"\nexport AWS_SECRET_ACCESS_KEY=\"&lt;your AWS secret access key&gt;\"\nexport AWS_DEFAULT_REGION=\"us-west-2\"\n</code></pre> <p>Then, add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n    aws/security:\n        enabled: true\n</code></pre> <p>To test, run:</p> <pre><code>holmes ask \"Are there any security misconfigurations my signup application, particularly in the database?\"\n</code></pre> <p>This builtin toolset is currently only available in HolmesGPT CLI.</p>"},{"location":"data-sources/builtin-toolsets/aws/#capabilities","title":"Capabilities","text":"Tool Name Description aws_cloudtrail_event_lookup Fetches events of a specified type from AWS CloudTrail along with the users that called them aws_cloudtrail_event_details Fetches and returns full event details for an AWS CloudTrail event in JSON format given an event ID aws_user_audit_logs Fetches audit logs for a specified user from AWS CloudTrail in past 24 hours. Provide username as was output by aws_event_lookup or aws_event_details"},{"location":"data-sources/builtin-toolsets/aws/#rds","title":"RDS","text":"<p>Read access to Amazon RDS instances, events and logs.</p>"},{"location":"data-sources/builtin-toolsets/aws/#configuration_1","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>Configure RDS access with your AWS credentials and region settings.</p> <p>This builtin toolset is currently only available in HolmesGPT CLI.</p>"},{"location":"data-sources/builtin-toolsets/aws/#capabilities_1","title":"Capabilities","text":"Tool Name Description aws_rds_describe_instances Describe RDS instances aws_rds_events Get RDS events aws_rds_logs Retrieve RDS logs"},{"location":"data-sources/builtin-toolsets/confluence/","title":"Confluence","text":"<p>By enabling this toolset, HolmesGPT will be able to fetch confluence pages. This is particularly useful if you store runbooks in Confluence and want Holmes to run investigations using these runbooks.</p> <p>This toolset requires an Atlassian API Key.</p>"},{"location":"data-sources/builtin-toolsets/confluence/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>Set the following environment variables and the Confluence toolset will be automatically enabled:</p> <pre><code>export CONFLUENCE_USER=\"&lt;confluence username&gt;\"\nexport CONFLUENCE_API_KEY=\"&lt;confluence API key&gt;\"\nexport CONFLUENCE_BASE_URL=\"&lt;confluence's base URL&gt;\"\n</code></pre> <p>To test, run:</p> <pre><code>holmes ask \"why is my application failing? Get relevant runbooks from Confluence\"\n</code></pre> <p>Helm Values:</p> <pre><code>holmes:\n    additionalEnvVars:\n        - name: CONFLUENCE_USER\n          value: &lt;Confluence's username&gt;\n        - name: CONFLUENCE_API_KEY\n          value: &lt;Confluence's API key&gt;\n        - name: CONFLUENCE_BASE_URL\n          value: &lt;Confluence's base URL&gt;\n    toolsets:\n        confluence:\n            enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/confluence/#capabilities","title":"Capabilities","text":"Tool Name Description fetch_confluence_url Fetch a page in confluence. Use this to fetch confluence runbooks if they are present before starting your investigation."},{"location":"data-sources/builtin-toolsets/coralogix-logs/","title":"Coralogix logs","text":"<p>By enabling this toolset, HolmesGPT will fetch pod logs from Coralogix.</p> <p>You should enable this toolset to replace the default Kubernetes logs toolset if all your kubernetes pod logs are consolidated inside Coralogix. It will make it easier for HolmesGPT to fetch incident logs, including the ability to precisely consult past logs.</p> <p>Logging Toolsets</p> <p>Only one logging toolset should be enabled at a time. If you enable this toolset, disable the default <code>kubernetes/logs</code> toolset.</p> <p>HolmesGPT provides several out-of-the-box alternatives for log access. You can select from these options:</p> <ul> <li>kubernetes/logs: Access logs directly through Kubernetes. This is the default toolset.</li> <li>coralogix/logs: Access logs through Coralogix.</li> <li>grafana/loki: Access Loki logs by proxying through a Grafana instance.</li> <li>opensearch/logs: Access logs through OpenSearch.</li> <li>datadog: Access logs through DataDog.</li> </ul>"},{"location":"data-sources/builtin-toolsets/coralogix-logs/#prerequisites","title":"Prerequisites","text":"<ol> <li>A Coralogix API key which is assigned the <code>DataQuerying</code> permission preset</li> <li>A Coralogix domain. For example <code>eu2.coralogix.com</code></li> <li>Your team's name or hostname. For example <code>your-company-name</code></li> </ol> <p>You can deduct the <code>domain</code> and <code>team_hostname</code> configuration fields by looking at the URL you use to access the Coralogix UI.</p> <p>For example if you access coralogix at <code>https://my-team.app.eu2.coralogix.com/</code> then the <code>team_hostname</code> is <code>my-team</code> and the coralogix <code>domain</code> is <code>eu2.coralogix.com</code>.</p>"},{"location":"data-sources/builtin-toolsets/coralogix-logs/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n  coralogix/logs:\n    enabled: true\n    config:\n      api_key: \"&lt;your coralogix API key&gt;\"\n      domain: \"eu2.coralogix.com\"\n      team_hostname: \"your-company-name\"\n\n  kubernetes/logs:\n    enabled: false  # Disable default Kubernetes logging\n</code></pre> <pre><code>holmes:\n  toolsets:\n    coralogix/logs:\n      enabled: true\n      config:\n        api_key: \"&lt;your coralogix API key&gt;\"\n        domain: \"eu2.coralogix.com\"\n        team_hostname: \"your-company-name\"\n\n    kubernetes/logs:\n      enabled: false  # Disable default Kubernetes logging\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/coralogix-logs/#capabilities","title":"Capabilities","text":"Tool Name Description coralogix_fetch_logs Fetch logs from Coralogix for specified pods and time ranges"},{"location":"data-sources/builtin-toolsets/datadog/","title":"DataDog","text":"<p>By enabling this toolset, HolmesGPT will be able to fetch logs from DataDog. This allows Holmes to access your application logs stored in DataDog for investigation purposes.</p> <p>Logging Toolsets</p> <p>Only one logging toolset should be enabled at a time. If you enable this toolset, disable the default <code>kubernetes/logs</code> toolset.</p> <p>HolmesGPT provides several out-of-the-box alternatives for log access. You can select from these options:</p> <ul> <li>kubernetes/logs: Access logs directly through Kubernetes. This is the default toolset.</li> <li>coralogix/logs: Access logs through Coralogix.</li> <li>grafana/loki: Access Loki logs by proxying through a Grafana instance.</li> <li>opensearch/logs: Access logs through OpenSearch.</li> <li>datadog: Access logs through DataDog.</li> </ul>"},{"location":"data-sources/builtin-toolsets/datadog/#prerequisites","title":"Prerequisites","text":"<ol> <li>A DataDog API key with log access permissions</li> <li>A DataDog Application key</li> </ol> <p>You can find these in your DataDog account under Organization Settings &gt; API Keys and Application Keys.</p>"},{"location":"data-sources/builtin-toolsets/datadog/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, set the following environment variables:</p> <pre><code>export DD_API_KEY=\"&lt;your DataDog API key&gt;\"\nexport DD_APP_KEY=\"&lt;your DataDog application key&gt;\"\n</code></pre> <p>Then add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n  datadog/logs:\n    enabled: true\n    config:\n      site: \"datadoghq.com\"  # or datadoghq.eu for EU, etc.\n\n  kubernetes/logs:\n    enabled: false  # Disable default Kubernetes logging\n</code></pre> <pre><code>holmes:\n  additionalEnvVars:\n    - name: DD_API_KEY\n      value: \"&lt;your DataDog API key&gt;\"\n    - name: DD_APP_KEY\n      value: \"&lt;your DataDog application key&gt;\"\n  toolsets:\n    datadog/logs:\n      enabled: true\n      config:\n        site: \"datadoghq.com\"  # or datadoghq.eu for EU, etc.\n\n    kubernetes/logs:\n      enabled: false  # Disable default Kubernetes logging\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/datadog/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can customize the DataDog site and other parameters:</p> <pre><code>toolsets:\n  datadog/logs:\n    enabled: true\n    config:\n      site: \"datadoghq.com\"  # Options: datadoghq.com, datadoghq.eu, us3.datadoghq.com, etc.\n      timeout: 30  # Request timeout in seconds\n</code></pre>"},{"location":"data-sources/builtin-toolsets/datadog/#capabilities","title":"Capabilities","text":"Tool Name Description datadog_fetch_logs Fetch logs from DataDog for specified time ranges and filters datadog_search_logs Search logs in DataDog using query patterns"},{"location":"data-sources/builtin-toolsets/datetime/","title":"Datetime \u2713","text":"<p>Enabled by Default</p> <p>This toolset is enabled by default. You do not need to configure it. You can disable it if you want to but doing so may negatively impact HolmesGPT's ability to investigate issues.</p> <p>By enabling this toolset, HolmesGPT will be able to get the current UTC date and time. This feature should be kept enabled as it can be necessary for other toolsets that rely on dates and time.</p> <p>The following built-in toolsets depend on <code>datetime</code>:</p> <ul> <li>Grafana Loki</li> <li>Prometheus</li> <li>Coralogix logs</li> </ul>"},{"location":"data-sources/builtin-toolsets/datetime/#configuration","title":"Configuration","text":"<pre><code>holmes:\n    toolsets:\n        datetime:\n            enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/datetime/#capabilities","title":"Capabilities","text":"Tool Name Description <p>| get_current_time | Return current time information. Useful to build queries that require a time information |</p>"},{"location":"data-sources/builtin-toolsets/docker/","title":"Docker","text":"<p>Not Recommended for Kubernetes</p> <p>This integration is not recommended for monitoring a kubernetes cluster because it is neither necessary nor useful. It is documented here for users of HolmesGPT CLI.</p> <p>Read access to Docker resources.</p>"},{"location":"data-sources/builtin-toolsets/docker/#configuration","title":"Configuration","text":"Holmes CLI <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n    docker/core:\n        enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/docker/#capabilities","title":"Capabilities","text":"<p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p> Tool Name Description docker_images List all Docker images docker_ps List all running Docker containers docker_ps_all List all Docker containers, including stopped ones docker_inspect Inspect detailed information about a Docker container or image docker_logs Fetch the logs of a Docker container docker_top Display the running processes of a container docker_events Get real-time events from the Docker server docker_history Show the history of an image docker_diff Inspect changes to files or directories on a container's filesystem <p>Need Custom Integration?</p> <p>By adding custom toolsets, users can extend HolmesGPT's investigation capabilities to address unique use cases, specific infrastructure setups, or organization-specific requirements. For example, custom toolsets might include specialized log analysis patterns or integration with external monitoring systems.</p> <p>Can't find the toolset you need? Create a custom toolset or request a new integration.</p>"},{"location":"data-sources/builtin-toolsets/github/","title":"GitHub","text":"<p>By enabling this toolset, HolmesGPT will be able to interact with GitHub repositories, read files, create pull requests, and manage branches. This is useful for accessing runbooks, documentation, or configuration files stored in GitHub.</p>"},{"location":"data-sources/builtin-toolsets/github/#prerequisites","title":"Prerequisites","text":"<ol> <li>A GitHub Personal Access Token with appropriate permissions:</li> <li><code>repo</code> scope for private repositories</li> <li><code>public_repo</code> scope for public repositories</li> <li><code>pull_requests:write</code> for creating PRs</li> </ol> <p>You can create a token at GitHub Settings &gt; Developer settings &gt; Personal access tokens.</p>"},{"location":"data-sources/builtin-toolsets/github/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, set the following environment variable:</p> <pre><code>export GITHUB_TOKEN=\"&lt;your GitHub personal access token&gt;\"\n</code></pre> <p>Then add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n  git/github:\n    enabled: true\n</code></pre> <p>To test, run:</p> <pre><code>holmes ask \"Check the troubleshooting guide in our GitHub repository for common deployment issues\"\n</code></pre> <pre><code>holmes:\n  additionalEnvVars:\n    - name: GITHUB_TOKEN\n      value: \"&lt;your GitHub personal access token&gt;\"\n  toolsets:\n    git/github:\n      enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/github/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can configure additional options:</p> <pre><code>toolsets:\n  git/github:\n    enabled: true\n    config:\n      default_branch: \"main\"  # Default branch to use\n      max_file_size: 1048576  # Maximum file size to read (1MB)\n      timeout: 30  # Request timeout in seconds\n</code></pre>"},{"location":"data-sources/builtin-toolsets/github/#capabilities","title":"Capabilities","text":"Tool Name Description github_read_file Read a file from a GitHub repository github_list_files List files in a GitHub repository directory github_search_files Search for files in a repository by name or content github_create_pr Create a new pull request github_update_pr Update an existing pull request github_list_branches List branches in a repository github_create_branch Create a new branch github_get_commit Get details about a specific commit"},{"location":"data-sources/builtin-toolsets/grafanaloki/","title":"Grafana Loki","text":"<p>By enabling this toolset, HolmesGPT will fetch pod logs from Loki. Loki can be accessed directly or by proxying through a Grafana instance.</p> <p>You should enable this toolset to replace the default Kubernetes logs toolset if all your kubernetes/pod logs are consolidated inside Loki. It will make it easier for HolmesGPT to fetch incident logs, including the ability to precisely consult past logs.</p> <p>Logging Toolsets</p> <p>Only one logging toolset should be enabled at a time. If you enable this toolset, disable the default <code>kubernetes/logs</code> toolset.</p> <p>HolmesGPT provides several out-of-the-box alternatives for log access. You can select from these options:</p> <ul> <li>kubernetes/logs: Access logs directly through Kubernetes. This is the default toolset.</li> <li>coralogix/logs: Access logs through Coralogix.</li> <li>grafana/loki: Access Loki logs by proxying through a Grafana instance.</li> <li>opensearch/logs: Access logs through OpenSearch.</li> <li>datadog: Access logs through DataDog.</li> </ul>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#proxying-through-grafana","title":"Proxying through Grafana","text":"<p>This is the recommended approach because we intend to add more capabilities to the toolset that are only available with Grafana.</p>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#prerequisites","title":"Prerequisites","text":"<p>A Grafana service account token with the following permissions:</p> <ul> <li>Basic role -&gt; Viewer</li> <li>Data sources -&gt; Reader</li> </ul> <p>Check out this video on creating a Grafana service account token.</p> <p>Getting Grafana URL</p> <p>You can find the Grafana URL required for Loki in your Grafana cloud account settings.</p> <p>Obtaining the datasource UID</p> <p>You may have multiple Loki data sources setup in Grafana. HolmesGPT uses a single Loki datasource to fetch the logs and it needs to know the UID of this datasource.</p> <p>A simple way to get the datasource UID is to access the Grafana API by running the following request:</p> <pre><code># port forward if you are using Robusta's grafana from your kubernetes cluster\nkubectl port-forward svc/robusta-grafana 3000:80\n\n# List the loki data sources\ncurl -s -u &lt;username&gt;:&lt;password&gt; http://localhost:3000/api/datasources | jq '.[] | select(.type == \"loki\")'\n</code></pre> <p>This will return something like:</p> <pre><code>{\n    \"id\": 2,\n    \"uid\": \"klja8hsa-8a9c-4b35-1230-7baab22b02ee\",\n    \"orgId\": 1,\n    \"name\": \"Loki-kubernetes\",\n    \"type\": \"loki\",\n    \"typeName\": \"Loki\",\n    \"typeLogoUrl\": \"/public/app/plugins/datasource/loki/img/loki_icon.svg\",\n    \"access\": \"proxy\",\n    \"url\": \"http://loki.loki:3100\",\n    \"user\": \"\",\n    \"database\": \"\",\n    \"basicAuth\": false,\n    \"isDefault\": false,\n    \"jsonData\": {\n        \"httpHeaderName1\": \"admin\",\n        \"httpHeaderName2\": \"X-Scope-OrgID\",\n        \"tlsSkipVerify\": true\n    },\n    \"readOnly\": false\n}\n</code></pre> <p>In this case, the Loki datasource UID is <code>klja8hsa-8a9c-4b35-1230-7baab22b02ee</code>.</p>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#configuration-grafana-proxy","title":"Configuration (Grafana Proxy)","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n  grafana/loki:\n    enabled: true\n    config:\n      api_key: &lt;your grafana API key&gt;\n      url: https://xxxxxxx.grafana.net # Your Grafana cloud account URL\n      grafana_datasource_uid: &lt;the UID of the loki data source in Grafana&gt;\n\n  kubernetes/logs:\n    enabled: false # HolmesGPT's default logging mechanism MUST be disabled\n</code></pre> <pre><code>holmes:\n  toolsets:\n    grafana/loki:\n      enabled: true\n      config:\n        api_key: &lt;your grafana API key&gt;\n        url: https://xxxxxxx.grafana.net # Your Grafana cloud account URL\n        grafana_datasource_uid: &lt;the UID of the loki data source in Grafana&gt;\n\n    kubernetes/logs:\n      enabled: false # HolmesGPT's default logging mechanism MUST be disabled\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#direct-connection","title":"Direct Connection","text":"<p>The toolset can directly connect to a Loki instance without proxying through a Grafana instance. This is done by not setting the <code>grafana_datasource_uid</code> field. Not setting this field makes HolmesGPT assume that it is directly connecting to Loki.</p>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#configuration-direct-connection","title":"Configuration (Direct Connection)","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n  grafana/loki:\n    enabled: true\n    config:\n      url: http://loki.logging\n      headers:\n        X-Scope-OrgID: \"&lt;tenant id&gt;\" # Set the X-Scope-OrgID if loki multitenancy is enabled\n\n  kubernetes/logs:\n    enabled: false # HolmesGPT's default logging mechanism MUST be disabled\n</code></pre> <pre><code>holmes:\n  toolsets:\n    grafana/loki:\n      enabled: true\n      config:\n        url: http://loki.logging\n        headers:\n          X-Scope-OrgID: \"&lt;tenant id&gt;\" # Set the X-Scope-OrgID if loki multitenancy is enabled\n\n    kubernetes/logs:\n      enabled: false # HolmesGPT's default logging mechanism MUST be disabled\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"data-sources/builtin-toolsets/grafanaloki/#search-labels","title":"Search Labels","text":"<p>You can tweak the labels used by the toolset to identify kubernetes resources. This is only needed if your Loki logs settings for <code>pod</code>, and <code>namespace</code> differ from the defaults.</p> Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml:</p> <pre><code>toolsets:\n  grafana/loki:\n    enabled: true\n    config:\n      url: ...\n      labels:\n          pod: \"pod\"\n          namespace: \"namespace\"\n</code></pre> <pre><code>holmes:\n  toolsets:\n    grafana/loki:\n      enabled: true\n      config:\n        url: ...\n        labels:\n            pod: \"pod\"\n            namespace: \"namespace\"\n</code></pre> <p>Use the following commands to list Loki's labels and determine which ones to use:</p> <pre><code># Make Loki accessible locally\nkubectl port-forward svc/loki 3100:3100\n\n# List all labels. You may have to add the -H 'X-Scope-OrgID:&lt;org id&gt;' option with a valid org id\ncurl http://localhost:3100/loki/api/v1/labels\n</code></pre>"},{"location":"data-sources/builtin-toolsets/grafanaloki/#capabilities","title":"Capabilities","text":"Tool Name Description fetch_pod_logs Fetches pod logs from Loki"},{"location":"data-sources/builtin-toolsets/grafanatempo/","title":"Grafana Tempo","text":"<p>By enabling this toolset, HolmesGPT will be able to fetch trace information from Grafana Tempo to debug performance related issues, like high latency in your application.</p>"},{"location":"data-sources/builtin-toolsets/grafanatempo/#proxying-through-grafana","title":"Proxying through Grafana","text":"<p>This is the recommended approach because we intend to add more capabilities to the toolset that are only available with Grafana.</p>"},{"location":"data-sources/builtin-toolsets/grafanatempo/#prerequisites","title":"Prerequisites","text":"<p>A Grafana service account token with the following permissions:</p> <ul> <li>Basic role -&gt; Viewer</li> <li>Data sources -&gt; Reader</li> </ul> <p>Check out this video on creating a Grafana service account token.</p> <p>Getting Grafana URL</p> <p>You can find the Grafana URL required for Tempo in your Grafana cloud account settings.</p> <p>Obtaining the datasource UID</p> <p>You may have multiple Tempo data sources setup in Grafana. HolmesGPT uses a single Tempo datasource to fetch the traces and it needs to know the UID of this datasource.</p> <p>A simple way to get the datasource UID is to access the Grafana API by running the following request:</p> <pre><code># port forward if you are using Robusta's grafana from your kubernetes cluster\nkubectl port-forward svc/robusta-grafana 3000:80\n# List the Tempo data sources\ncurl -s -u &lt;username&gt;:&lt;password&gt; http://localhost:3000/api/datasources | jq '.[] | select(.type == \"tempo\")'\n</code></pre> <p>This will return something like:</p> <pre><code>{\n    \"id\": 3,\n    \"uid\": \"klja8hsa-8a9c-4b35-1230-7baab22b02ee\",\n    \"orgId\": 1,\n    \"name\": \"Tempo\",\n    \"type\": \"tempo\",\n    \"typeName\": \"Tempo\",\n    \"typeLogoUrl\": \"/public/app/plugins/datasource/tempo/img/tempo_icon.svg\",\n    \"access\": \"proxy\",\n    \"url\": \"http://tempo-query-frontend.tempo:3100\",\n    \"user\": \"\",\n    \"database\": \"\",\n    \"basicAuth\": false,\n    \"isDefault\": false,\n    \"jsonData\": {\n        \"tlsSkipVerify\": true\n    },\n    \"readOnly\": false\n}\n</code></pre> <p>In this case, the Tempo datasource UID is <code>klja8hsa-8a9c-4b35-1230-7baab22b02ee</code>.</p>"},{"location":"data-sources/builtin-toolsets/grafanatempo/#configuration-grafana-proxy","title":"Configuration (Grafana Proxy)","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n  grafana/tempo:\n    enabled: true\n    config:\n      api_key: &lt;your grafana service account token&gt;\n      url: &lt;your grafana url&gt; # e.g. https://acme-corp.grafana.net\n      grafana_datasource_uid: &lt;the UID of the tempo data source in Grafana&gt;\n</code></pre> <p>To test, run:</p> <pre><code>holmes ask \"The payments DB is very slow, check tempo for any trace data\"\n</code></pre> <pre><code>holmes:\n  toolsets:\n    grafana/tempo:\n      enabled: true\n      config:\n        api_key: &lt;your grafana API key&gt;\n        url: &lt;your grafana url&gt; # e.g. https://acme-corp.grafana.net\n        grafana_datasource_uid: &lt;the UID of the tempo data source in Grafana&gt;\n        labels:\n          pod: \"k8s.pod.name\"\n          namespace: \"k8s.namespace.name\"\n          deployment: \"k8s.deployment.name\"\n          node: \"k8s.node.name\"\n          service: \"service.name\"\n</code></pre>"},{"location":"data-sources/builtin-toolsets/grafanatempo/#direct-connection","title":"Direct Connection","text":"<p>The toolset can directly connect to a Tempo instance without proxying through a Grafana instance. This is done by not setting the <code>grafana_datasource_uid</code> field. Not setting this field makes HolmesGPT assume that it is directly connecting to Tempo.</p>"},{"location":"data-sources/builtin-toolsets/grafanatempo/#configuration-direct-connection","title":"Configuration (Direct Connection)","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n  grafana/tempo:\n    enabled: true\n    config:\n      url: http://tempo.monitoring\n      headers:\n        X-Scope-OrgID: \"&lt;tenant id&gt;\" # Set the X-Scope-OrgID if tempo multitenancy is enabled\n</code></pre> <pre><code>holmes:\n  toolsets:\n    grafana/tempo:\n      enabled: true\n      config:\n        url: http://tempo.monitoring\n        headers:\n          X-Scope-OrgID: \"&lt;tenant id&gt;\" # Set the X-Scope-OrgID if tempo multitenancy is enabled\n</code></pre>"},{"location":"data-sources/builtin-toolsets/grafanatempo/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"data-sources/builtin-toolsets/grafanatempo/#search-labels","title":"Search Labels","text":"<p>You can tweak the labels used by the toolset to identify kubernetes resources. This is only needed if the trace labels differ from the defaults.</p> Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml:</p> <pre><code>toolsets:\n  grafana/tempo:\n    enabled: true\n    config:\n      url: ...\n      labels:\n        pod: \"k8s.pod.name\"\n        namespace: \"k8s.namespace.name\"\n        deployment: \"k8s.deployment.name\"\n        node: \"k8s.node.name\"\n        service: \"service.name\"\n</code></pre> <pre><code>holmes:\n  toolsets:\n    grafana/tempo:\n      enabled: true\n      config:\n        url: ...\n        labels:\n          pod: \"k8s.pod.name\"\n          namespace: \"k8s.namespace.name\"\n          deployment: \"k8s.deployment.name\"\n          node: \"k8s.node.name\"\n          service: \"service.name\"\n</code></pre>"},{"location":"data-sources/builtin-toolsets/grafanatempo/#capabilities","title":"Capabilities","text":"Tool Name Description fetch_tempo_tags List the tags available in Tempo fetch_tempo_traces Lists Tempo traces. At least one of <code>service_name</code>, <code>pod_name</code> or <code>deployment_name</code> argument is required. fetch_tempo_trace_by_id Retrieves detailed information about a Tempo trace using its trace ID. Use this to investigate a trace."},{"location":"data-sources/builtin-toolsets/helm/","title":"Helm \u2713","text":"<p>Enabled by Default</p> <p>This toolset is enabled by default. You do not need to configure it. You can disable it if you want to but doing so may negatively impact HolmesGPT's ability to investigate issues.</p> <p>By enabling this toolset, HolmesGPT will be able to read access to a cluster's Helm charts and releases.</p>"},{"location":"data-sources/builtin-toolsets/helm/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n    helm/core:\n        enabled: true\n</code></pre> <pre><code>holmes:\n    toolsets:\n        helm/core:\n            enabled: true\n    customClusterRoleRules:\n        - apiGroups: [\"\"]\n          resources: [\"secrets\", \"pods\", \"services\", \"configmaps\", \"persistentvolumeclaims\"]\n          verbs: [\"get\", \"list\", \"watch\"]\n        - apiGroups: [\"\"]\n          resources: [\"namespaces\"]\n          verbs: [\"get\"]\n        - apiGroups: [\"apps\"]\n          resources: [\"deployments\", \"statefulsets\", \"daemonsets\"]\n          verbs: [\"get\", \"list\", \"watch\"]\n        - apiGroups: [\"batch\"]\n          resources: [\"jobs\", \"cronjobs\"]\n          verbs: [\"get\", \"list\", \"watch\"]\n        - apiGroups: [\"networking.k8s.io\"]\n          resources: [\"ingresses\"]\n          verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/helm/#capabilities","title":"Capabilities","text":"<p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p> Tool Name Description helm_list Use to get all the current helm releases helm_values Use to gather Helm values or any released helm chart helm_status Check the status of a Helm release helm_history Get the revision history of a Helm release helm_manifest Fetch the generated Kubernetes manifest for a Helm release helm_hooks Get the hooks for a Helm release helm_chart Show the chart used to create a Helm release helm_notes Show the notes provided by the Helm chart <p>Need Custom Integration?</p> <p>By adding custom toolsets, users can extend HolmesGPT's investigation capabilities to address unique use cases, specific infrastructure setups, or organization-specific requirements. For example, custom toolsets might include specialized log analysis patterns or integration with external monitoring systems.</p> <p>Can't find the toolset you need? Create a custom toolset or request a new integration.</p>"},{"location":"data-sources/builtin-toolsets/internet/","title":"Internet \u2713","text":"<p>Enabled by Default</p> <p>This toolset is enabled by default and should typically remain enabled.</p> <p>By enabling this toolset, HolmesGPT will be able to fetch webpages. This tool is beneficial if you provide Holmes with publicly accessible web based runbooks.</p>"},{"location":"data-sources/builtin-toolsets/internet/#configuration","title":"Configuration","text":"<pre><code>holmes:\n    toolsets:\n        internet:\n            enabled: true\n            config: # optional\n              additional_headers:\n                Authorization: Bearer ...\n</code></pre>"},{"location":"data-sources/builtin-toolsets/internet/#capabilities","title":"Capabilities","text":"Tool Name Description fetch_webpage Fetch a webpage. Use this to fetch runbooks if they are present before starting your investigation (if no other tool like confluence is more appropriate)"},{"location":"data-sources/builtin-toolsets/kafka/","title":"Kafka","text":"<p>By enabling this toolset, HolmesGPT will be able to fetch metadata from Kafka. This provides Holmes the ability to introspect into Kafka by listing consumers and topics or finding lagging consumer groups.</p> <p>This toolset uses the AdminClient of the confluent-kafka python library. Kafka's Java API is also a good source of documentation.</p>"},{"location":"data-sources/builtin-toolsets/kafka/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n    kafka/admin:\n        enabled: true\n        config:\n            kafka_clusters:\n                - name: aks-prod-kafka\n                  kafka_broker: kafka-1.aks-prod-kafka-brokers.kafka.svc:9095\n                  kafka_username: kafka-plaintext-user\n                  kafka_password: ******\n                  kafka_sasl_mechanism: SCRAM-SHA-512\n                  kafka_security_protocol: SASL_PLAINTEXT\n                - name: gke-stg-kafka\n                  kafka_broker: gke-kafka.gke-stg-kafka-brokers.kafka.svc:9095\n                  kafka_username: kafka-plaintext-user\n                  kafka_password: ****\n                  kafka_sasl_mechanism: SCRAM-SHA-512\n                  kafka_security_protocol: SASL_PLAINTEXT\n</code></pre> <pre><code>holmes:\n    toolsets:\n        kafka/admin:\n            enabled: true\n            config:\n                kafka_clusters:\n                    - name: aks-prod-kafka\n                      kafka_broker: kafka-1.aks-prod-kafka-brokers.kafka.svc:9095\n                      kafka_username: kafka-plaintext-user\n                      kafka_password: ******\n                      kafka_sasl_mechanism: SCRAM-SHA-512\n                      kafka_security_protocol: SASL_PLAINTEXT\n                    - name: gke-stg-kafka\n                      kafka_broker: gke-kafka.gke-stg-kafka-brokers.kafka.svc:9095\n                      kafka_username: kafka-plaintext-user\n                      kafka_password: ****\n                      kafka_sasl_mechanism: SCRAM-SHA-512\n                      kafka_security_protocol: SASL_PLAINTEXT\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p> <p>Below is a description of the configuration field for each cluster:</p> Config key Description name Give a meaningful name to your cluster. Holmes will use it to decide what cluster to look into. Names must be unique across all clusters. kafka_broker List of host/port pairs to use for establishing the initial connection to the Kafka cluster kafka_username Username for SASL authentication kafka_password Password for SASL authentication kafka_sasl_mechanism SASL mechanism (e.g., SCRAM-SHA-512) kafka_security_protocol Security protocol (e.g., SASL_PLAINTEXT)"},{"location":"data-sources/builtin-toolsets/kafka/#capabilities","title":"Capabilities","text":"<p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p> Tool Name Description kafka_list_topics List all Kafka topics kafka_describe_topic Get detailed information about a specific topic kafka_list_consumers List all consumer groups kafka_describe_consumer Get detailed information about a consumer group kafka_consumer_lag Check consumer lag for a consumer group <p>Need Custom Integration?</p> <p>By adding custom toolsets, users can extend HolmesGPT's investigation capabilities to address unique use cases, specific infrastructure setups, or organization-specific requirements. For example, custom toolsets might include specialized log analysis patterns or integration with external monitoring systems.</p> <p>Can't find the toolset you need? Create a custom toolset or request a new integration.</p>"},{"location":"data-sources/builtin-toolsets/kubernetes/","title":"Kubernetes Toolsets","text":""},{"location":"data-sources/builtin-toolsets/kubernetes/#core","title":"Core \u2713","text":"<p>Enabled by Default</p> <p>This toolset is enabled by default and should typically remain enabled.</p> <p>By enabling this toolset, HolmesGPT will be able to describe and find Kubernetes resources like nodes, deployments, pods, etc.</p>"},{"location":"data-sources/builtin-toolsets/kubernetes/#configuration","title":"Configuration","text":"<pre><code>holmes:\n    toolsets:\n        kubernetes/core:\n            enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/kubernetes/#capabilities","title":"Capabilities","text":"Tool Name Description kubectl_describe Run kubectl describe command on a specific resource kubectl_get_by_name Get details of a specific resource with labels kubectl_get_by_kind_in_namespace List all resources of a given type in a namespace kubectl_get_by_kind_in_cluster List all resources of a given type across the cluster kubectl_find_resources Search for resources matching a keyword kubectl_get_yaml Get YAML definition of a resource kubectl_events Get events for a specific resource kubectl_memory_requests_all_namespaces Get memory requests for all pods across all namespaces in MiB kubectl_memory_requests_namespace Get memory requests for all pods in a specific namespace in MiB kubernetes_jq_query Query Kubernetes resources using jq filters"},{"location":"data-sources/builtin-toolsets/kubernetes/#logs","title":"Logs \u2713","text":"<p>Enabled by Default</p> <p>This toolset is enabled by default. You do not need to configure it.</p> <p>By enabling this toolset, HolmesGPT will be able to read kubernetes pod logs.</p> <p>HolmesGPT provides several out-of-the-box alternatives for log access. You can select from these options:</p> <ul> <li>kubernetes/logs: Access logs directly through Kubernetes. This is the default toolset.</li> <li>coralogix/logs: Access logs through Coralogix.</li> <li>grafana/loki: Access Loki logs by proxying through a Grafana instance.</li> <li>opensearch/logs: Access logs through OpenSearch.</li> <li>datadog: Access logs through DataDog.</li> </ul>"},{"location":"data-sources/builtin-toolsets/kubernetes/#configuration_1","title":"Configuration","text":"<pre><code>holmes:\n    toolsets:\n        kubernetes/logs:\n            enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/kubernetes/#capabilities_1","title":"Capabilities","text":"Tool Name Description kubectl_logs Fetch logs from a specific pod kubectl_logs_all_containers Fetch logs from all containers in a pod kubectl_previous_logs Fetch previous logs from a pod kubectl_previous_logs_all_containers Fetch previous logs from all containers in a pod kubectl_container_logs Fetch logs from a specific container in a pod kubectl_logs_grep Search for specific patterns in pod logs kubectl_logs_all_containers_grep Search for patterns in logs from all containers"},{"location":"data-sources/builtin-toolsets/kubernetes/#live-metrics","title":"Live Metrics","text":"<p>This toolset retrieves real-time CPU and memory usage for pods and nodes.</p>"},{"location":"data-sources/builtin-toolsets/kubernetes/#configuration_2","title":"Configuration","text":"<pre><code>holmes:\n    toolsets:\n        kubernetes/live_metrics:\n            enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/kubernetes/#capabilities_2","title":"Capabilities","text":"Tool Name Description kubectl_top_pods Get current CPU and memory usage for pods kubectl_top_nodes Get current CPU and memory usage for nodes"},{"location":"data-sources/builtin-toolsets/kubernetes/#prometheus-stack","title":"Prometheus Stack","text":"<p>This toolset fetches Prometheus target definitions. Requires specific cluster role rules.</p>"},{"location":"data-sources/builtin-toolsets/kubernetes/#configuration_3","title":"Configuration","text":"<pre><code>holmes:\n    toolsets:\n        kubernetes/prometheus_stack:\n            enabled: true\n    customClusterRoleRules:\n        - apiGroups: [\"monitoring.coreos.com\"]\n          resources: [\"servicemonitors\", \"podmonitors\", \"prometheusrules\"]\n          verbs: [\"get\", \"list\"]\n</code></pre>"},{"location":"data-sources/builtin-toolsets/kubernetes/#capabilities_3","title":"Capabilities","text":"Tool Name Description kubectl_get_prometheus_targets Get Prometheus monitoring targets kubectl_get_service_monitors Get ServiceMonitor resources kubectl_get_pod_monitors Get PodMonitor resources"},{"location":"data-sources/builtin-toolsets/kubernetes/#resource-lineage-extras","title":"Resource Lineage Extras","text":"<p>Two variations of resource lineage toolsets: one native and one using kubectl krew. Provides tools to fetch children/dependents and parents/dependencies of Kubernetes resources.</p>"},{"location":"data-sources/builtin-toolsets/kubernetes/#configuration_4","title":"Configuration","text":"<pre><code>holmes:\n    toolsets:\n        kubernetes/resource_lineage_extras:\n            enabled: true\n        # OR\n        kubernetes/resource_lineage_extras_krew:\n            enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/kubernetes/#capabilities_4","title":"Capabilities","text":"Tool Name Description kubectl_lineage_children Get child/dependent resources of a Kubernetes resource kubectl_lineage_parents Get parent/dependency resources of a Kubernetes resource"},{"location":"data-sources/builtin-toolsets/mcp/","title":"Model Context Protocol (MCP)","text":"<p>By enabling this toolset, HolmesGPT will be able to connect to external Model Context Protocol (MCP) servers, extending its capabilities with custom tools and data sources.</p> <p>MCP allows you to integrate external tools and services that implement the MCP specification, providing a standardized way to extend HolmesGPT's functionality.</p>"},{"location":"data-sources/builtin-toolsets/mcp/#prerequisites","title":"Prerequisites","text":"<ol> <li>An external MCP server running and accessible</li> <li>MCP server endpoint URL</li> <li>Any required authentication credentials for the MCP server</li> </ol>"},{"location":"data-sources/builtin-toolsets/mcp/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n  mcp/remote:\n    enabled: true\n    config:\n      servers:\n        - name: \"custom-tools\"\n          endpoint: \"http://your-mcp-server:8080\"\n          # Optional authentication\n          headers:\n            Authorization: \"Bearer &lt;your-token&gt;\"\n        - name: \"another-server\"\n          endpoint: \"https://mcp.example.com\"\n</code></pre> <pre><code>holmes:\n  toolsets:\n    mcp/remote:\n      enabled: true\n      config:\n        servers:\n          - name: \"custom-tools\"\n            endpoint: \"http://your-mcp-server:8080\"\n            # Optional authentication\n            headers:\n              Authorization: \"Bearer &lt;your-token&gt;\"\n          - name: \"another-server\"\n            endpoint: \"https://mcp.example.com\"\n</code></pre>"},{"location":"data-sources/builtin-toolsets/mcp/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can configure multiple MCP servers with different settings:</p> <pre><code>toolsets:\n  mcp/remote:\n    enabled: true\n    config:\n      timeout: 30  # Request timeout in seconds\n      max_retries: 3  # Maximum number of retries\n      servers:\n        - name: \"primary-tools\"\n          endpoint: \"http://primary-mcp-server:8080\"\n          timeout: 60  # Server-specific timeout\n          headers:\n            Authorization: \"Bearer &lt;token&gt;\"\n            X-API-Version: \"v1\"\n        - name: \"backup-tools\"\n          endpoint: \"https://backup-mcp-server.com\"\n          verify_ssl: false  # For development/testing\n</code></pre>"},{"location":"data-sources/builtin-toolsets/mcp/#mcp-server-examples","title":"MCP Server Examples","text":"<p>Popular MCP servers you can integrate:</p> <ul> <li>Database MCP: Direct database query capabilities</li> <li>Cloud Provider MCP: AWS/Azure/GCP specific tools</li> <li>Monitoring MCP: Custom monitoring system integration</li> <li>Internal Tools MCP: Company-specific tooling</li> </ul>"},{"location":"data-sources/builtin-toolsets/mcp/#capabilities","title":"Capabilities","text":"<p>The capabilities depend on the connected MCP servers. Common tool types include:</p> Tool Category Description Data Sources Access to external databases, APIs, or files Custom Commands Organization-specific operational commands Specialized Analysis Domain-specific analysis tools External Integrations Third-party service integrations <p>Dynamic Capabilities</p> <p>The exact tools available through MCP servers are discovered dynamically when the toolset connects to each server.</p>"},{"location":"data-sources/builtin-toolsets/mcp/#creating-your-own-mcp-server","title":"Creating Your Own MCP Server","text":"<p>To create a custom MCP server for your organization:</p> <ol> <li>Implement the MCP specification</li> <li>Expose your tools via the MCP protocol</li> <li>Configure HolmesGPT to connect to your server</li> <li>Your custom tools will be automatically available to Holmes</li> </ol> <p>See the MCP documentation for implementation details.</p>"},{"location":"data-sources/builtin-toolsets/newrelic/","title":"New Relic","text":"<p>By enabling this toolset, HolmesGPT will be able to fetch logs and traces from New Relic. This allows Holmes to access your application performance data and logs stored in New Relic for investigation purposes.</p> <p>Disable Default Logging Toolset</p> <p>The default HolmesGPT logging tool must be disabled if you use a different datasource for logs. HolmesGPT may still use kubectl to fetch logs and never call your datasource if <code>kubernetes/logs</code> is not disabled.</p> <p>HolmesGPT provides several out-of-the-box alternatives for log access. You can select from these options:</p> <ul> <li>kubernetes/logs: Access logs directly through Kubernetes. This is the default toolset.</li> <li>coralogix/logs: Access logs through Coralogix.</li> <li>grafana/loki: Access Loki logs by proxying through a Grafana instance.</li> <li>opensearch/logs: Access logs through OpenSearch.</li> <li>datadog: Access logs through DataDog.</li> </ul>"},{"location":"data-sources/builtin-toolsets/newrelic/#prerequisites","title":"Prerequisites","text":"<ol> <li>A New Relic User API Key</li> <li>Your New Relic Account ID</li> </ol> <p>You can find these in your New Relic account under Administration &gt; API keys and Account settings.</p>"},{"location":"data-sources/builtin-toolsets/newrelic/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, set the following environment variables:</p> <pre><code>export NEW_RELIC_USER_KEY=\"&lt;your New Relic User API key&gt;\"\nexport NEW_RELIC_ACCOUNT_ID=\"&lt;your New Relic account ID&gt;\"\n</code></pre> <p>Then add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n  newrelic/apm:\n    enabled: true\n    config:\n      region: \"US\"  # or \"EU\" for EU region\n\n  kubernetes/logs:\n    enabled: false  # Disable default Kubernetes logging if using New Relic for logs\n</code></pre> <pre><code>holmes:\n  additionalEnvVars:\n    - name: NEW_RELIC_USER_KEY\n      value: \"&lt;your New Relic User API key&gt;\"\n    - name: NEW_RELIC_ACCOUNT_ID\n      value: \"&lt;your New Relic account ID&gt;\"\n  toolsets:\n    newrelic/apm:\n      enabled: true\n      config:\n        region: \"US\"  # or \"EU\" for EU region\n\n    kubernetes/logs:\n      enabled: false  # Disable default Kubernetes logging if using New Relic for logs\n</code></pre>"},{"location":"data-sources/builtin-toolsets/newrelic/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can customize the New Relic region and other parameters:</p> <pre><code>toolsets:\n  newrelic/apm:\n    enabled: true\n    config:\n      region: \"US\"  # Options: \"US\" or \"EU\"\n      timeout: 30  # Request timeout in seconds\n      max_results: 1000  # Maximum number of results to fetch\n</code></pre>"},{"location":"data-sources/builtin-toolsets/newrelic/#capabilities","title":"Capabilities","text":"Tool Name Description newrelic_fetch_logs Fetch logs from New Relic for specified time ranges and filters newrelic_fetch_traces Fetch distributed traces from New Relic APM newrelic_query_nrql Execute NRQL queries against New Relic data newrelic_get_app_performance Get application performance metrics"},{"location":"data-sources/builtin-toolsets/notion/","title":"Notion","text":"<p>Notion Integration for HolmesGPT</p> <p>Enabling this toolset allows HolmesGPT to fetch pages from Notion, making it useful when providing Notion-based runbooks.</p>"},{"location":"data-sources/builtin-toolsets/notion/#setup-instructions","title":"Setup Instructions","text":"<ol> <li> <p>Create a Webhook Integration</p> <ul> <li>Go to the Notion Developer Portal.</li> <li>Create a new integration with read content capabilities.</li> </ul> </li> <li> <p>Grant Access to Pages</p> <ul> <li>Open the desired Notion page.</li> <li>Click the three dots in the top right.</li> <li>Select Connections and add your integration.</li> </ul> </li> <li> <p>Configure Authentication</p> <ul> <li>Retrieve the Internal Integration Secret from Notion.</li> <li>Create a Kubernetes secret in your cluster with this key.</li> <li>Configure the <code>NOTION_AUTH</code> environment variable.</li> </ul> </li> </ol>"},{"location":"data-sources/builtin-toolsets/notion/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, set the environment variable: <pre><code>export NOTION_AUTH=\"&lt;your Notion integration secret&gt;\"\n</code></pre></p> <p>Then add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist: <pre><code>toolsets:\n    notion:\n        enabled: true\n</code></pre></p> <pre><code>holmes:\n    additionalEnvVars:\n        - name: NOTION_AUTH\n          value: \"&lt;your Notion integration secret&gt;\"\n    toolsets:\n        notion:\n            enabled: true\n            config:\n                additional_headers:\n                    Authorization: Bearer {{ env.NOTION_AUTH }}\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/notion/#capabilities","title":"Capabilities","text":"<p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p> Tool Name Description fetch_notion_webpage Fetch a notion webpage. Use this to fetch notion runbooks if they are present before starting your investigation <p>Need Custom Integration?</p> <p>By adding custom toolsets, users can extend HolmesGPT's investigation capabilities to address unique use cases, specific infrastructure setups, or organization-specific requirements. For example, custom toolsets might include specialized log analysis patterns or integration with external monitoring systems.</p> <p>Can't find the toolset you need? Create a custom toolset or request a new integration.</p>"},{"location":"data-sources/builtin-toolsets/opensearch-logs/","title":"OpenSearch logs","text":"<p>By enabling this toolset, HolmesGPT will fetch pod logs from OpenSearch.</p> <p>You should enable this toolset to replace the default Kubernetes logs toolset if all your kubernetes pod logs are consolidated inside OpenSearch/Elastic. It will make it easier for HolmesGPT to fetch incident logs, including the ability to precisely consult past logs.</p> <p>Logging Toolsets</p> <p>Only one logging toolset should be enabled at a time. If you enable this toolset, disable the default <code>kubernetes/logs</code> toolset.</p> <p>HolmesGPT provides several out-of-the-box alternatives for log access. You can select from these options:</p> <ul> <li>kubernetes/logs: Access logs directly through Kubernetes. This is the default toolset.</li> <li>coralogix/logs: Access logs through Coralogix.</li> <li>grafana/loki: Access Loki logs by proxying through a Grafana instance.</li> <li>opensearch/logs: Access logs through OpenSearch.</li> <li>datadog: Access logs through DataDog.</li> </ul>"},{"location":"data-sources/builtin-toolsets/opensearch-logs/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n  opensearch/logs:\n    enabled: true\n    config:\n      opensearch_url: https://your-opensearch-cluster.com:443\n      index_pattern: fluentd-*\n      opensearch_auth_header: \"ApiKey your-api-key-here\"\n      labels:\n        pod: \"kubernetes.pod_name\"\n        namespace: \"kubernetes.namespace_name\"\n        timestamp: \"@timestamp\"\n        message: \"message\"\n\n  kubernetes/logs:\n    enabled: false # Disable default Kubernetes logging\n</code></pre> <pre><code>holmes:\n  toolsets:\n    opensearch/logs:\n      enabled: true\n      config:\n        opensearch_url: https://your-opensearch-cluster.com:443 # The URL to your opensearch cluster.\n        index_pattern: fluentd-* # The pattern matching the indexes containing the logs. Supports wildcards\n        opensearch_auth_header: \"ApiKey your-api-key-here\" # An optional header value set to the `Authorization` header for every request to opensearch.\n        labels: # set the labels according to how values are mapped in your opensearch cluster\n          pod: \"kubernetes.pod_name\"\n          namespace: \"kubernetes.namespace_name\"\n          timestamp: \"@timestamp\"\n          message: \"message\"\n\n    kubernetes/logs:\n      enabled: false # HolmesGPT's default logging mechanism MUST be disabled\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p>"},{"location":"data-sources/builtin-toolsets/opensearch-logs/#capabilities","title":"Capabilities","text":"Tool Name Description opensearch_fetch_logs Fetch logs from OpenSearch for specified pods and time ranges opensearch_search_logs Search logs in OpenSearch using query patterns"},{"location":"data-sources/builtin-toolsets/opensearch-status/","title":"OpenSearch status","text":"<p>By enabling this toolset, HolmesGPT will be able to access cluster metadata information like health, shards, and settings. This allows HolmesGPT to better troubleshoot problems with one or more opensearch clusters.</p>"},{"location":"data-sources/builtin-toolsets/opensearch-status/#configuration","title":"Configuration","text":"<p>The configuration for OpenSearch is passed through to the underlying opensearch-py library. Consult this library's user guide or reference documentation for configuring the connection to OpenSearch, including how to authenticate this toolset to an opensearch cluster.</p> Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist: <pre><code>toolsets:\n    opensearch/status:\n        enabled: true\n        config:\n            opensearch_clusters:\n                - hosts:\n                    - host1.com\n                    - host2.com\n                  headers:\n                    header1: \"value1\"\n                  use_ssl: &lt;boolean&gt;\n                  ssl_assert_hostname: &lt;boolean&gt;\n                  verify_certs: &lt;boolean&gt;\n                  ssl_show_warn: &lt;boolean&gt;\n                  http_auth:\n                    username: &lt;basic auth username&gt;\n                    password: &lt;basic auth password&gt;\n</code></pre></p> <p><pre><code>holmes:\n    toolsets:\n        opensearch/status:\n            enabled: true\n            config:\n                opensearch_clusters:\n                    - hosts:\n                        - host1.com\n                        - host2.com\n                      headers:\n                        header1: \"value1\"\n                      use_ssl: &lt;boolean&gt;\n                      ssl_assert_hostname: &lt;boolean&gt;\n                      verify_certs: &lt;boolean&gt;\n                      ssl_show_warn: &lt;boolean&gt;\n                      http_auth:\n                        username: &lt;basic auth username&gt;\n                        password: &lt;basic auth password&gt;\n</code></pre> Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p> <p>Here is an example of an insecure OpenSearch configuration for local development using a bearer token:</p> Holmes CLIRobusta Helm Chart <p>First, set the environment variables: <pre><code>export OPENSEARCH_URL=\"&lt;opensearch host URL&gt;\"\nexport OPENSEARCH_BEARER_TOKEN=\"&lt;secret bearer token&gt;\"\n</code></pre></p> <p>Then add the following to ~/.holmes/config.yaml: <pre><code>toolsets:\n    opensearch/status:\n        enabled: true\n        config:\n            opensearch_clusters:\n                - hosts:\n                    - host: \"{{ env.OPENSEARCH_URL }}\"\n                    port: 9200\n</code></pre></p> <pre><code>holmes:\n    additionalEnvVars:\n        - name: OPENSEARCH_URL\n          value: \"&lt;opensearch host URL&gt;\"\n        - name: OPENSEARCH_BEARER_TOKEN\n          value: \"&lt;secret bearer token&gt;\"\n    toolsets:\n        opensearch/status:\n            enabled: true\n            config:\n                opensearch_clusters:\n                    - hosts:\n                        - host: \"{{ env.OPENSEARCH_URL }}\"\n                        port: 9200\n</code></pre>"},{"location":"data-sources/builtin-toolsets/opensearch-status/#capabilities","title":"Capabilities","text":"<p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p> Tool Name Description opensearch_cluster_health Get cluster health information opensearch_cluster_stats Get cluster statistics opensearch_node_info Get information about cluster nodes opensearch_index_stats Get statistics for specific indices opensearch_shard_allocation Get shard allocation information <p>Need Custom Integration?</p> <p>By adding custom toolsets, users can extend HolmesGPT's investigation capabilities to address unique use cases, specific infrastructure setups, or organization-specific requirements. For example, custom toolsets might include specialized log analysis patterns or integration with external monitoring systems.</p> <p>Can't find the toolset you need? Create a custom toolset or request a new integration.</p>"},{"location":"data-sources/builtin-toolsets/prometheus/","title":"Prometheus","text":"<p>By enabling this toolset, HolmesGPT will be able to generate graphs from prometheus metrics as well as help you write and validate prometheus queries. HolmesGPT can also detect memory leak patterns, CPU throttling, lagging queues, and high latency issues.</p> <p>Prior to generating a PromQL query, HolmesGPT tends to list the available metrics. This is done to ensure the metrics used in PromQL are actually available.</p>"},{"location":"data-sources/builtin-toolsets/prometheus/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n    prometheus/metrics:\n        enabled: true\n        config:\n            # see below how to find prometheus_url\n            prometheus_url: http://&lt;prometheus host&gt;:9090 # e.g. http://robusta-kube-prometheus-st-prometheus.default.svc.cluster.local:9090\n\n            # optional\n            #headers:\n            #    Authorization: \"Basic &lt;base_64_encoded_string&gt;\"\n</code></pre> <pre><code>holmes:\n    toolsets:\n        prometheus/metrics:\n            enabled: true\n            config:\n                # see below how to find prometheus_url\n                prometheus_url: http://&lt;prometheus host&gt;:9090 # e.g. http://robusta-kube-prometheus-st-prometheus.default.svc.cluster.local:9090\n\n                # optional\n                #headers:\n                #    Authorization: \"Basic &lt;base_64_encoded_string&gt;\"\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p> <p>It is also possible to set the <code>PROMETHEUS_URL</code> environment variable instead of the above <code>prometheus_url</code> config key.</p>"},{"location":"data-sources/builtin-toolsets/prometheus/#advanced-configuration","title":"Advanced configuration","text":"<p>Below is the full list of options for this toolset:</p> <pre><code>prometheus/metrics:\n  enabled: true\n  config:\n    prometheus_url: http://localhost:9090\n    headers:\n      Authorization: \"Basic &lt;base_64_encoded_string&gt;\"\n    # Optional: timeout for requests in seconds\n    timeout: 30\n    # Optional: maximum number of metrics to fetch\n    max_metrics: 1000\n    # Optional: cache duration for metrics list\n    cache_duration: 300\n</code></pre>"},{"location":"data-sources/builtin-toolsets/prometheus/#finding-your-prometheus-url","title":"Finding your Prometheus URL","text":"<p>To find your Prometheus URL, you can:</p> <ol> <li> <p>For Kubernetes clusters with Prometheus Operator: <pre><code>kubectl get prometheus -A\nkubectl get svc -A | grep prometheus\n</code></pre></p> </li> <li> <p>Port forward to access Prometheus locally: <pre><code>kubectl port-forward svc/prometheus-server 9090:80\n# Then use: http://localhost:9090\n</code></pre></p> </li> <li> <p>Check your Helm releases: <pre><code>helm list -A | grep prometheus\n</code></pre></p> </li> </ol>"},{"location":"data-sources/builtin-toolsets/prometheus/#capabilities","title":"Capabilities","text":"Tool Name Description list_available_metrics List all available Prometheus metrics execute_prometheus_instant_query Execute an instant PromQL query execute_prometheus_range_query Execute a range PromQL query for time series data get_current_time Get current timestamp for time-based queries"},{"location":"data-sources/builtin-toolsets/rabbitmq/","title":"RabbitMQ","text":"<p>By enabling this toolset, HolmesGPT will be able to detect RabbitMQ partitions, memory alerts and disk alerts and suggest mitigations.</p> <p>This toolset follows a two step process to detect partition:</p> <ol> <li>The nodes and partitioning status is obtained by fetching information from the configured <code>management_url</code>.</li> <li>If some nodes are reported as not-running, the toolset will try to contact these nodes individually and deduct any partitioning state for any node that is actually running.</li> </ol>"},{"location":"data-sources/builtin-toolsets/rabbitmq/#configuration","title":"Configuration","text":"Robusta Helm ChartHolmes CLI <pre><code>holmes:\n  toolsets:\n    rabbitmq/core:\n      enabled: true\n      config:\n        clusters:\n          - id: rabbitmq # must be unique across all configured clusters\n            username: &lt;user&gt;\n            password: &lt;password&gt;\n            management_url: &lt;http://rabbitmq.rabbitmq:15672&gt;\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p> <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n  rabbitmq/core:\n    enabled: true\n    config:\n      clusters:\n        - id: rabbitmq # must be unique across all configured clusters\n          username: &lt;user&gt;\n          password: &lt;password&gt;\n          management_url: &lt;http://rabbitmq.rabbitmq:15672&gt;\n</code></pre>"},{"location":"data-sources/builtin-toolsets/rabbitmq/#advanced-configuration","title":"Advanced configuration","text":"<p>Below is the full list of options for this toolset:</p> <pre><code>rabbitmq/core:\n  enabled: true\n  config:\n    clusters:\n      - id: rabbitmq # must be unique across all configured clusters\n        username: &lt;user&gt;\n        password: &lt;password&gt;\n        management_url: &lt;http://rabbitmq.rabbitmq:15672&gt;\n        request_timeout_seconds: 30 # timeout for HTTP requests\n</code></pre>"},{"location":"data-sources/builtin-toolsets/rabbitmq/#capabilities","title":"Capabilities","text":"Tool Name Description get_rabbitmq_cluster_status Get cluster status and partition information get_rabbitmq_node_info Get detailed information about RabbitMQ nodes get_rabbitmq_queue_info Get information about queues get_rabbitmq_exchange_info Get information about exchanges get_rabbitmq_memory_usage Get memory usage statistics get_rabbitmq_disk_usage Get disk usage statistics"},{"location":"data-sources/builtin-toolsets/robusta/","title":"Robusta \u2713","text":"<p>Enabled by Default</p> <p>This toolset is enabled by default and should typically remain enabled.</p> <p>By enabling this toolset, HolmesGPT will be able to fetch alerts metadata. It allows HolmesGPT to fetch information about specific issues when chatting using \"Ask HolmesGPT\". This toolset is not necessary for Root Cause Analysis.</p>"},{"location":"data-sources/builtin-toolsets/robusta/#configuration","title":"Configuration","text":"<pre><code>holmes:\n    toolsets:\n        robusta:\n            enabled: true\n</code></pre>"},{"location":"data-sources/builtin-toolsets/robusta/#capabilities","title":"Capabilities","text":"Tool Name Description fetch_finding_by_id Fetches a robusta finding. Findings are events, like a Prometheus alert or a deployment update"},{"location":"data-sources/builtin-toolsets/servicenow/","title":"ServiceNow","text":"<p>By enabling this toolset, HolmesGPT will be able to interact with ServiceNow for ticket management, incident tracking, and accessing knowledge base articles during investigations.</p>"},{"location":"data-sources/builtin-toolsets/servicenow/#prerequisites","title":"Prerequisites","text":"<ol> <li>ServiceNow instance URL</li> <li>ServiceNow username and password or API token</li> <li>Appropriate ServiceNow roles and permissions</li> </ol>"},{"location":"data-sources/builtin-toolsets/servicenow/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, set the following environment variables:</p> <pre><code>export SERVICENOW_INSTANCE=\"&lt;your servicenow instance url&gt;\"\nexport SERVICENOW_USERNAME=\"&lt;your servicenow username&gt;\"\nexport SERVICENOW_PASSWORD=\"&lt;your servicenow password&gt;\"\n</code></pre> <p>Then add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n  servicenow/tickets:\n    enabled: true\n    config:\n      verify_ssl: true\n      timeout: 30\n</code></pre> <pre><code>holmes:\n  additionalEnvVars:\n    - name: SERVICENOW_INSTANCE\n      value: \"&lt;your servicenow instance url&gt;\"\n    - name: SERVICENOW_USERNAME\n      value: \"&lt;your servicenow username&gt;\"\n    - name: SERVICENOW_PASSWORD\n      value: \"&lt;your servicenow password&gt;\"\n  toolsets:\n    servicenow/tickets:\n      enabled: true\n      config:\n        verify_ssl: true\n        timeout: 30\n</code></pre>"},{"location":"data-sources/builtin-toolsets/servicenow/#advanced-configuration","title":"Advanced Configuration","text":"<p>You can customize ServiceNow integration settings:</p> <pre><code>toolsets:\n  servicenow/tickets:\n    enabled: true\n    config:\n      verify_ssl: true\n      timeout: 30  # Request timeout in seconds\n      max_results: 100  # Maximum number of tickets to fetch\n      default_table: \"incident\"  # Default ServiceNow table to query\n</code></pre>"},{"location":"data-sources/builtin-toolsets/servicenow/#capabilities","title":"Capabilities","text":"Tool Name Description servicenow_create_incident Create a new incident ticket in ServiceNow servicenow_get_incident Get details of a specific incident servicenow_search_incidents Search for incidents based on criteria servicenow_update_incident Update an existing incident servicenow_get_knowledge_base Search ServiceNow knowledge base articles servicenow_create_change_request Create a change request ticket"},{"location":"data-sources/builtin-toolsets/slab/","title":"Slab","text":"<p>By enabling this toolset, HolmesGPT will be able to consult runbooks from Slab pages.</p> <p>Retrieve your Slab API token prior to configuring this toolset. Do note that Slab API is only available for Slab premium users. See here.</p>"},{"location":"data-sources/builtin-toolsets/slab/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>First, set the environment variable: <pre><code>export SLAB_API_KEY=\"&lt;your slab API key&gt;\"\n</code></pre></p> <p>Then add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist: <pre><code>toolsets:\n    slab:\n        enabled: true\n</code></pre></p> <pre><code>holmes:\n    additionalEnvVars:\n        - name: SLAB_API_KEY\n          value: \"&lt;your slab API key&gt;\"\n    toolsets:\n        slab:\n            enabled: true\n</code></pre> <p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p> <p>To test, run:</p> <pre><code>holmes ask \"Why is my pod failing, if its a crashloopbackoff use the runbooks from slab\"\n</code></pre>"},{"location":"data-sources/builtin-toolsets/slab/#capabilities","title":"Capabilities","text":"<p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p> Tool Name Description fetch_slab_document Fetch a document from slab. Use this to fetch runbooks if they are present before starting your investigation. <p>Need Custom Integration?</p> <p>By adding custom toolsets, users can extend HolmesGPT's investigation capabilities to address unique use cases, specific infrastructure setups, or organization-specific requirements. For example, custom toolsets might include specialized log analysis patterns or integration with external monitoring systems.</p> <p>Can't find the toolset you need? Create a custom toolset or request a new integration.</p>"},{"location":"development/","title":"Development","text":"<p>Extend HolmesGPT with custom integrations and contribute to the project.</p>"},{"location":"development/#available-development-guides","title":"Available Development Guides","text":"<ul> <li>Contributing Guidelines - How to contribute to the HolmesGPT project</li> </ul>"},{"location":"development/#evaluations","title":"Evaluations","text":"<ul> <li>Evaluations Overview - Understanding HolmesGPT's evaluation framework</li> <li>Writing Evaluations - Create your own evaluation tests</li> <li>Reporting with Braintrust - Analyze evaluation results</li> </ul>"},{"location":"development/#development-setup","title":"Development Setup","text":"<p>To start developing with HolmesGPT:</p> <ol> <li>Set up development environment - Local Kubernetes cluster and tools</li> <li>Clone the repository - Get the latest source code</li> <li>Build and test - Create your custom integrations</li> <li>Deploy and validate - Test in a development environment</li> </ol>"},{"location":"development/#common-development-tasks","title":"Common Development Tasks","text":""},{"location":"development/#contributing","title":"Contributing","text":"<p>Help improve HolmesGPT: - Fix bugs and improve performance - Add new built-in toolsets - Enhance documentation - Share examples and best practices</p>"},{"location":"development/#development-resources","title":"Development Resources","text":"<ul> <li>Source Code: GitHub Repository</li> <li>Issue Tracker: GitHub Issues</li> <li>Community: Slack Channel</li> <li>Documentation: You're reading it!</li> </ul>"},{"location":"development/#getting-started","title":"Getting Started","text":"<p>New to HolmesGPT development? Start with:</p> <ol> <li>Contributing Guidelines - Understand the development process</li> <li>Evaluations Overview - Learn about the evaluation framework</li> </ol> <p>Ready to contribute? Begin with Contributing Guidelines.</p>"},{"location":"development/contributing/","title":"Contributing","text":""},{"location":"development/contributing/#before-you-get-started","title":"Before you get started","text":""},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please make sure to read and observe our Governance docs.</p>"},{"location":"development/contributing/#install-requirements","title":"Install requirements","text":"<ul> <li>Python <code>3.11</code></li> <li>poetry <code>1.8.4</code> &amp; up</li> <li>A LLM API key is required to use and test HolmesGPT</li> <li>OpenAI's <code>gpt4-o</code> is recommended.</li> <li>For details see Getting an API Key.</li> </ul>"},{"location":"development/contributing/#reporting-bugs","title":"Reporting bugs","text":"<p>We encourage those interested to contribute code and also appreciate when issues are reported.</p> <ul> <li>Create a new issue and label is as <code>bug</code></li> <li>Clearly state how to reproduce the bug:</li> <li>Which LLM you've used</li> <li>Which steps are required to reproduce<ul> <li>As LLMs answers may differ between runs - Does it always reproduce, or occasionally?</li> </ul> </li> </ul>"},{"location":"development/contributing/#contributing-code","title":"Contributing Code","text":"<ul> <li>Fork the repository and clone it locally.</li> <li>Create a new branch and make your changes</li> <li>Add or update tests to ensure your changes are covered.</li> <li>Run <code>pytest</code> to verify all tests pass.</li> <li>Keep pull requests small and focused. if you have multiple changes, open a PR for each.</li> <li>Create a pull request back to the upstream repository.</li> <li>Wait for a review and address any comments</li> <li>Follow the guidelines in our Governance docs regarding code contributions</li> </ul>"},{"location":"development/contributing/#installation-from-source","title":"Installation from Source","text":"<p>For development purposes, install HolmesGPT from source with Poetry:</p> <ol> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/robusta-dev/holmesgpt.git\ncd holmesgpt\n</code></pre></p> </li> <li> <p>Install dependencies with Poetry:    <pre><code>poetry install\n</code></pre></p> </li> <li> <p>Set up your LLM API key (see API Keys documentation)</p> </li> <li> <p>Run HolmesGPT:    <pre><code>poetry run python holmes.py ask \"your question here\"\n</code></pre></p> </li> </ol>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<p>For help with contributing: - Contact us on Slack - Ask DeepWiki AI your questions - View the installation documentation for more details</p> <p></p>"},{"location":"development/evals/","title":"Evaluations","text":"<p>HolmesGPT uses automated evaluations (evals) to ensure consistent performance across different LLM models and to catch regressions during development. These evaluations test the system's ability to correctly diagnose Kubernetes issues.</p> <ul> <li>Writing Evaluations - Learn how to create new test cases and evaluations</li> <li>Reporting with Braintrust - Analyze results and debug failures using Braintrust</li> </ul>"},{"location":"development/evals/#overview","title":"Overview","text":"<p>The eval system comprises two main test suites:</p> <ul> <li>Ask Holmes: Tests single-question interactions with HolmesGPT</li> <li>Investigate: Tests HolmesGPT's ability to investigate specific issues reported by AlertManager</li> </ul> <p>Evals use fixtures that simulate real Kubernetes environments and tool outputs, allowing comprehensive testing without requiring live clusters.</p> <p>While results are tracked and analyzed using Braintrust, Braintrust is not necessary to writing, running and debugging evals.</p>"},{"location":"development/evals/#example","title":"Example","text":"<p>Below is an example of a report added to pull requests to catch regressions:</p> Test suite Test case Status ask_holmes 01_how_many_pods ask_holmes 02_what_is_wrong_with_pod ask_holmes 02_what_is_wrong_with_pod_LOKI ask_holmes 03_what_is_the_command_to_port_forward ask_holmes 04_related_k8s_events ask_holmes 05_image_version ask_holmes 06_explain_issue ask_holmes 07_high_latency ask_holmes 07_high_latency_LOKI ask_holmes 08_sock_shop_frontend ask_holmes 09_crashpod ask_holmes 10_image_pull_backoff ask_holmes 11_init_containers ask_holmes 12_job_crashing ask_holmes 12_job_crashing_CORALOGIX ask_holmes 12_job_crashing_LOKI ask_holmes 13_pending_node_selector ask_holmes 14_pending_resources ask_holmes 15_failed_readiness_probe ask_holmes 16_failed_no_toolset_found ask_holmes 17_oom_kill ask_holmes 18_crash_looping_v2 ask_holmes 19_detect_missing_app_details ask_holmes 20_long_log_file_search_LOKI ask_holmes 21_job_fail_curl_no_svc_account ask_holmes 22_high_latency_dbi_down ask_holmes 23_app_error_in_current_logs ask_holmes 23_app_error_in_current_logs_LOKI ask_holmes 24_misconfigured_pvc ask_holmes 25_misconfigured_ingress_class ask_holmes 26_multi_container_logs ask_holmes 27_permissions_error_no_helm_tools ask_holmes 28_permissions_error_helm_tools_enabled ask_holmes 29_events_from_alert_manager ask_holmes 30_basic_promql_graph_cluster_memory ask_holmes 31_basic_promql_graph_pod_memory ask_holmes 32_basic_promql_graph_pod_cpu ask_holmes 33_http_latency_graph ask_holmes 34_memory_graph ask_holmes 35_tempo ask_holmes 36_argocd_find_resource ask_holmes 37_argocd_wrong_namespace ask_holmes 38_rabbitmq_split_head ask_holmes 39_failed_toolset ask_holmes 40_disabled_toolset ask_holmes 41_setup_argo investigate 01_oom_kill investigate 02_crashloop_backoff investigate 03_cpu_throttling investigate 04_image_pull_backoff investigate 05_crashpod investigate 05_crashpod_LOKI investigate 06_job_failure investigate 07_job_syntax_error investigate 08_memory_pressure investigate 09_high_latency investigate 10_KubeDeploymentReplicasMismatch investigate 11_KubePodCrashLooping investigate 12_KubePodNotReady investigate 13_Watchdog investigate 14_tempo <p>Legend</p> <ul> <li> the test was successful</li> <li> the test failed but is known to be flakky or known to fail</li> <li> the test failed and should be fixed before merging the PR</li> </ul>"},{"location":"development/evals/#why-evaluations-matter","title":"Why Evaluations Matter","text":"<p>Evaluations serve several critical purposes:</p> <ol> <li>Quality Assurance: Ensure HolmesGPT provides accurate diagnostics and recommendations</li> <li>Model Comparison: Compare performance across different LLM models (GPT-4, Claude, Gemini, etc.)</li> <li>Regression Testing: Catch performance degradations when updating code or dependencies</li> <li>Toolset Validation: Verify that new toolsets and integrations work correctly</li> <li>Continuous Improvement: Identify areas where HolmesGPT needs enhancement</li> </ol>"},{"location":"development/evals/#how-to-run-evaluations","title":"How to Run Evaluations","text":""},{"location":"development/evals/#basic-usage","title":"Basic Usage","text":"<p>Run all evaluations: <pre><code>pytest ./tests/llm/test_*.py\n</code></pre></p> <p>By default the tests load and present mock files to the LLM whenever it asks for them. If a mock file is not present for a tool call, the tool call is passed through to the live tool itself. In a lot of cases this can cause the eval to fail unless the live environment (k8s cluster) matches what the LLM expects.</p> <p>Run specific test suite: <pre><code>pytest ./tests/llm/test_ask_holmes.py\npytest ./tests/llm/test_investigate.py\n</code></pre></p> <p>Run a specific test case: <pre><code>pytest ./tests/llm/test_ask_holmes.py -k \"01_how_many_pods\"\n</code></pre></p> <p>It is possible to investigate and debug why an eval fails by the output provided in the console. The output includes the correctness score, the reasoning for the score, information about what tools were called, the expected answer, as well as the LLM's answer.</p>"},{"location":"development/evals/#environment-variables","title":"Environment Variables","text":"<p>Configure evaluations using these environment variables:</p> Variable Example Description <code>MODEL</code> <code>MODEL=anthropic/claude-3.5</code> Specify which LLM model to use <code>CLASSIFIER_MODEL</code> <code>CLASSIFIER_MODEL=gpt-4o</code> The LLM model to use for scoring the answer (LLM as judge). Defaults to <code>MODEL</code> <code>ITERATIONS</code> <code>ITERATIONS=3</code> Run each test multiple times for consistency checking <code>RUN_LIVE</code> <code>RUN_LIVE=true</code> Execute <code>before-test</code> and <code>after-test</code> commands, ignore mock files <code>BRAINTRUST_API_KEY</code> <code>BRAINTRUST_API_KEY=sk-1dh1...swdO02</code> API key for Braintrust integration <code>UPLOAD_DATASET</code> <code>UPLOAD_DATASET=true</code> Sync dataset to Braintrust (safe, separated by branch) <code>PUSH_EVALS_TO_BRAINTRUST</code> <code>PUSH_EVALS_TO_BRAINTRUST=true</code> Upload evaluation results to Braintrust <code>EXPERIMENT_ID</code> <code>EXPERIMENT_ID=my_baseline</code> Custom experiment name for result tracking"},{"location":"development/evals/#simple-example","title":"Simple Example","text":"<p>Run a comprehensive evaluation: <pre><code>export MODEL=gpt-4o\n\n# Run with parallel execution for speed\npytest -n 10 ./tests/llm/test_*.py\n</code></pre></p>"},{"location":"development/evals/#live-testing","title":"Live Testing","text":"<p>For tests that require actual Kubernetes resources: <pre><code>export RUN_LIVE=true\n\npytest ./tests/llm/test_ask_holmes.py -k \"specific_test\"\n</code></pre></p> <p>Live testing requires a Kubernetes cluster and will execute <code>before-test</code> and <code>after-test</code> commands to set up/tear down resources. Not all tests support live testing. Some tests require manual setup.</p>"},{"location":"development/evals/#model-comparison-workflow","title":"Model Comparison Workflow","text":"<ol> <li> <p>Create Baseline: Run evaluations with a reference model    <pre><code>EXPERIMENT_ID=baseline_gpt4o MODEL=gpt-4o pytest -n 10 ./tests/llm/test_*\n</code></pre></p> </li> <li> <p>Test New Model: Run evaluations with the model you want to compare    <pre><code>EXPERIMENT_ID=test_claude35 MODEL=anthropic/claude-3.5 pytest -n 10 ./tests/llm/test_*\n</code></pre></p> </li> <li> <p>Compare Results: Use Braintrust dashboard to analyze performance differences</p> </li> </ol>"},{"location":"development/evals/#troubleshooting","title":"Troubleshooting","text":""},{"location":"development/evals/#common-issues","title":"Common Issues","text":"<ol> <li>Missing BRAINTRUST_API_KEY: Some tests are skipped without this key</li> <li>Live test failures: Ensure Kubernetes cluster access and proper permissions</li> <li>Mock file mismatches: Regenerate mocks with <code>generate_mocks: true</code></li> <li>Timeout errors: Increase test timeout or check network connectivity</li> </ol>"},{"location":"development/evals/#debug-mode","title":"Debug Mode","text":"<p>Enable verbose output: <pre><code>pytest -v -s ./tests/llm/test_ask_holmes.py -k \"specific_test\"\n</code></pre></p> <p>This shows detailed output including: - Expected vs actual results - Tool calls made by the LLM - Evaluation scores and rationales - Debugging information</p>"},{"location":"development/evals/reporting/","title":"Reporting with Braintrust","text":"<p>This guide explains how to use Braintrust to analyze evaluation results, debug failures, and compare model performance.</p> <ul> <li>Evaluations Overview - Introduction to HolmesGPT's evaluation system</li> <li>Writing Evaluations - Learn how to create new test cases and evaluations</li> </ul>"},{"location":"development/evals/reporting/#overview","title":"Overview","text":"<p>Braintrust is a platform for tracking and analyzing LLM evaluations. HolmesGPT evals can be used without Braintrust but using Braintrust has a few advantages:</p> <ul> <li>We can track how Holmes perform over time</li> <li>It's easier to run and debug many evals with Braintrust over simpler pytests because Braintrust organises the different components of a HolmesGPT investigation like the input, tool calls, reasoning for scoring, etc.</li> </ul>"},{"location":"development/evals/reporting/#setting-up-braintrust","title":"Setting Up Braintrust","text":""},{"location":"development/evals/reporting/#1-create-account","title":"1. Create Account","text":"<ol> <li>Visit braintrust.dev</li> <li>Sign up for an account</li> <li>Create a new project (e.g., \"HolmesGPT\")</li> </ol>"},{"location":"development/evals/reporting/#2-get-api-key","title":"2. Get API Key","text":"<ol> <li>Click your profile icon (top right)</li> <li>Go to Settings \u2192 API Keys</li> <li>Generate a new API key</li> <li>Copy the key (starts with <code>sk-</code>)</li> </ol>"},{"location":"development/evals/reporting/#3-configure-environment","title":"3. Configure Environment","text":"<pre><code>export BRAINTRUST_API_KEY=sk-your-api-key-here\n</code></pre>"},{"location":"development/evals/reporting/#running-evaluations-with-braintrust","title":"Running Evaluations with Braintrust","text":""},{"location":"development/evals/reporting/#basic-evaluation-run","title":"Basic Evaluation Run","text":"<pre><code>export BRAINTRUST_API_KEY=sk-your-key\nexport UPLOAD_DATASET=true\nexport PUSH_EVALS_TO_BRAINTRUST=true\n\npytest ./tests/llm/test_ask_holmes.py\n</code></pre>"},{"location":"development/evals/reporting/#named-experiment","title":"Named Experiment","text":"<pre><code>export EXPERIMENT_ID=baseline_gpt4o\nexport MODEL=gpt-4o\npytest -n 10 ./tests/llm/test_*.py\n</code></pre>"},{"location":"development/evals/reporting/#key-environment-variables","title":"Key Environment Variables","text":"Variable Purpose <code>UPLOAD_DATASET</code> Sync test cases to Braintrust <code>PUSH_EVALS_TO_BRAINTRUST</code> Upload evaluation results <code>EXPERIMENT_ID</code> Name your experiment run. This makes it easier to find and track in Braintrust's UI <code>MODEL</code> The LLM model for Holmes to use <code>CLASSIFIER_MODEL</code> The LLM model to use for scoring the answer (LLM as judge)"},{"location":"development/evals/reporting/#analyzing-evaluation-results","title":"Analyzing Evaluation Results","text":""},{"location":"development/evals/reporting/#output","title":"Output","text":"<p>The main Span of an evaluation will present the input (either the AlertManager issue or the user's question for Ask Holmes) as well as HolmesGPT's answer.</p> <p></p>"},{"location":"development/evals/reporting/#score-types","title":"Score Types","text":"<p>Correctness Score: - Measures accuracy of LLM responses - Values: 0 or 1 - Shows how well output matches expectations</p> <p></p>"},{"location":"development/evals/reporting/#debugging-failed-evaluations","title":"Debugging Failed Evaluations","text":""},{"location":"development/evals/reporting/#1-identify-failing-tests","title":"1. Identify Failing Tests","text":"<p>In the experiment view: - Sort by score (ascending) to see worst performers - Filter by specific score types - Look for patterns in failures</p>"},{"location":"development/evals/reporting/#2-examine-tool-call-traces","title":"2. Examine Tool Call Traces","text":"<p>Click on a failing test to see: - Input: The original prompt/question - Tool Calls: Which tools the LLM invoked - Tool Results: What data each tool returned - Output: The LLM's final response - Expected: What the test expected</p> <p></p>"},{"location":"development/evals/reporting/#3-common-failure-patterns","title":"3. Common Failure Patterns","text":"<p>Low Correctness Score: - LLM missed key information in tool outputs - Response doesn't address the core question - Factual errors in the analysis</p> <p>Low Context Score: - LLM didn't reference important context items - May indicate prompt engineering issues - Could suggest irrelevant context was provided</p> <p>Missing Tool Calls: - LLM didn't invoke necessary tools - Check if tool descriptions are clear - Verify mock data is realistic</p>"},{"location":"development/evals/reporting/#4-debug-example","title":"4. Debug Example","text":"<p>Failing Test: \"02_what_is_wrong_with_pod\" - Score: Correctness 0.2, Context 0.1 - Issue: LLM said \"pod is healthy\" but expected \"CrashLoopBackOff detected\"</p> <p>Investigation: 1. Check <code>kubectl_describe.txt</code> mock - contains correct CrashLoopBackOff status 2. Verify <code>kubectl_logs.txt</code> shows crash errors 3. Review LLM's tool calls - did it call <code>kubectl_describe</code>? 4. Examine LLM output - did it misinterpret the kubectl output?</p> <p>Solution: Update mock files to be more explicit about the crash status</p>"},{"location":"development/evals/writing/","title":"Writing Evaluations","text":"<p>This guide explains how to create new evaluations for HolmesGPT. Evaluations test the system's ability to correctly diagnose issues and provide accurate recommendations.</p> <ul> <li>Evaluations Overview - Introduction to HolmesGPT's evaluation system</li> <li>Reporting with Braintrust - Analyze results and debug failures using Braintrust</li> </ul>"},{"location":"development/evals/writing/#overview","title":"Overview","text":"<p>HolmesGPT supports two types of evaluations:</p> <ol> <li>Ask Holmes Tests: Chat-like interactions (<code>tests/llm/test_ask_holmes.py</code>)</li> <li>Investigation Tests: Issue analysis for events triggered by AlertManager (<code>tests/llm/test_investigate.py</code>)</li> </ol> <p>Each test consists of: - A test case definition (<code>test_case.yaml</code>) - Mock tool outputs (e.g., <code>kubectl_describe.txt</code>) - Optional Kubernetes manifests for live testing - Optional custom toolset configurations</p>"},{"location":"development/evals/writing/#high-level-steps","title":"High-Level Steps","text":"<ol> <li>Choose test type: Ask Holmes vs Investigation. Choose Ask Holmes for most use cases. Choose Investigations for issues triggered by AlertManager</li> <li>Create a test folder: Use numbered naming convention</li> <li>Define your test case:</li> <li>Create <code>test_case.yaml</code> with prompt and expectations</li> <li>Define kubectl or helm setup and teardown manifests</li> <li>Generate mock data: Using a live system</li> <li>Set evaluation criteria: Define minimum scores for test success</li> <li>Test and iterate: Run the test and refine as needed</li> </ol>"},{"location":"development/evals/writing/#step-by-step-example-creating-an-ask-holmes-test","title":"Step-by-Step Example: Creating an Ask Holmes Test","text":"<p>Let's create a simple test that asks about pod health status.</p>"},{"location":"development/evals/writing/#step-1-create-test-folder","title":"Step 1: Create Test Folder","text":"<pre><code>mkdir tests/llm/fixtures/test_ask_holmes/99_pod_health_check\ncd tests/llm/fixtures/test_ask_holmes/99_pod_health_check\n</code></pre>"},{"location":"development/evals/writing/#step-2-create-test_caseyaml","title":"Step 2: Create test_case.yaml","text":"<pre><code>user_prompt: 'Is the nginx pod healthy?'\nexpected_output:\n  - nginx pod is healthy\nevaluation:\n  correctness: 1\nbefore_test: kubectl apply -f ./manifest.yaml\nafter_test: kubectl delete -f ./manifest.yaml\n</code></pre> <ul> <li><code>user_prompt</code>: This is the question that will trigger Holmes' investigation</li> <li><code>expected_output</code>: This is a list of expected elements that MUST be found in Holmes' answer. The combination of these elements lead to a <code>correctness</code> score based on HolmesGPT's output. This <code>expected_output</code> will be compared against HolmesGPT's answer and evaluated by a LLM ('LLM as judge'). The resulting score is called <code>correctness</code> and is a binary score with a value of either <code>0</code> or <code>1</code>. HolmesGPT's answer is score <code>0</code> is any of the expected element is not present in the answer, <code>1</code> if all expected elements are preent in the answer.</li> <li><code>evaluation.correctness</code>: This is the expected correctness score and is used for pytest to fail the test. This expected <code>correctness</code> score should be <code>0</code> unless you expect HolmesGPT to systematically succeed the evaluation. Because of this, it is important for <code>expected_output</code> to be reduced to the minimally accepted output from HolmesGPT.</li> <li><code>before_test</code> and <code>after_test</code>: These are setup and teardown steps to reproduce the test on a fresh environment. It is important for these to be present because as HolmesGPT's code, prompt and toolset evolve the mocks become insufficient or inaccurate. These scripts are run automatically when the env var <code>RUN_LIVE=true</code> is set</li> </ul>"},{"location":"development/evals/writing/#step-3-generate-mock-tool-outputs","title":"Step 3: Generate Mock Tool Outputs","text":"<p>Create mock files that simulate kubectl command outputs.</p> <p>The best way to do this is to:</p> <ol> <li>Deploy the test case you want to build an eval for in a kubernetes cluster (run the <code>before_test</code> script manually)</li> <li>Configure HolmesGPT to connect to the cluster (via kubectl and any other relevant toolsets)</li> <li>Enable the auto generation of mock files by setting <code>generate_mocks: True</code> in your <code>test_case.yaml</code></li> <li>Repeatedly run the eval with <code>ITERATIONS=100 pytest tests/llm/test_ask_holmes.py -k 99_pod_health_check</code></li> <li>Removing the prefix <code>.AUTOGENERATED</code> from all autogenerated files</li> </ol>"},{"location":"development/evals/writing/#step-4-run-the-test","title":"Step 4: Run the Test","text":"<pre><code>pytest ./tests/llm/test_ask_holmes.py -k \"99_pod_health_check\" -v\n</code></pre>"},{"location":"development/evals/writing/#test-case-configuration-reference","title":"Test Case Configuration Reference","text":""},{"location":"development/evals/writing/#common-fields","title":"Common Fields","text":"Field Type Required Description <code>user_prompt</code> string Yes The question or prompt for HolmesGPT <code>expected_output</code> string or list Yes Expected elements in the response <code>evaluation</code> dict No Minimum scores for test to pass"},{"location":"development/evals/writing/#ask-holmes-specific-fields","title":"Ask Holmes Specific Fields","text":"Field Type Description <code>before_test</code> string Command to run before test (requires <code>RUN_LIVE=true</code>) <code>after_test</code> string Command to run after test cleanup <code>generate_mocks</code> boolean Whether to generate new mock files"},{"location":"development/evals/writing/#investigation-specific-fields","title":"Investigation Specific Fields","text":"Field Type Description <code>expected_sections</code> dict Required/prohibited sections in output"},{"location":"development/evals/writing/#example-complex-investigation-test","title":"Example: Complex Investigation Test","text":"<pre><code>user_prompt: \"Investigate this CrashLoopBackOff issue\"\nexpected_output:\n  - Pod is experiencing CrashLoopBackOff\n  - Container exits with code 1 due to configuration error\n  - Missing environment variable DATABASE_URL\nexpected_sections:\n  \"Root Cause Analysis\":\n    - CrashLoopBackOff\n    - configuration error\n  \"Recommended Actions\": true\n  \"External Links\": false\nevaluation:\n  correctness: 0\nbefore_test: kubectl apply -f ./manifest.yaml\nafter_test: kubectl delete -f ./manifest.yaml\n</code></pre>"},{"location":"development/evals/writing/#mock-file-generation","title":"Mock File Generation","text":""},{"location":"development/evals/writing/#automatic-generation","title":"Automatic Generation","text":"<p>Set <code>generate_mocks: true</code> in <code>test_case.yaml</code> and run with a live cluster:</p> <pre><code>ITERATIONS=100 pytest ./tests/llm/test_ask_holmes.py -k \"your_test\"\n</code></pre> <p>This captures real tool outputs and saves them as mock files.</p>"},{"location":"development/evals/writing/#manual-creation","title":"Manual Creation","text":"<p>Create files matching the tool names used by HolmesGPT:</p> <ul> <li><code>kubectl_describe.txt</code> - Pod/resource descriptions</li> <li><code>kubectl_logs.txt</code> - Container logs</li> <li><code>kubectl_events.txt</code> - Kubernetes events</li> <li><code>prometheus_query.txt</code> - Metrics data</li> <li><code>fetch_loki_logs.txt</code> - Log aggregation results</li> </ul>"},{"location":"development/evals/writing/#naming-convention","title":"Naming Convention","text":"<p>Mock files follow the pattern: <code>{tool_name}_{additional_context}.txt</code></p> <p>Examples: - <code>kubectl_describe_pod_nginx_default.txt</code> - <code>kubectl_logs_all_containers_nginx.txt</code> - <code>execute_prometheus_range_query.txt</code></p>"},{"location":"development/evals/writing/#toolset-configuration","title":"Toolset Configuration","text":"<p>Some tests require specific toolsets. Create a <code>toolsets.yaml</code> file:</p> <pre><code>toolsets:\n  - name: kubernetes\n    enabled: true\n  - name: prometheus\n    enabled: true\n    config:\n      prometheus_url: http://localhost:9090 # requires port-forward\n  - name: grafana_loki\n    enabled: true\n    config:\n      base_url: http://localhost:3000 # requires port-forward\n      api_key: \"{{env.GRAFANA_API_KEY}}\"\n      grafana_datasource_uid: \"{{env.GRAFANA_DATASOURCE_UID}}\"\n</code></pre>"},{"location":"development/evals/writing/#live-testing-with-real-resources","title":"Live Testing with Real Resources","text":"<p>For tests that need actual Kubernetes resources:</p>"},{"location":"development/evals/writing/#step-1-create-manifest","title":"Step 1: Create Manifest","text":"<p>manifest.yaml: <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: test-nginx\n  template:\n    metadata:\n      labels:\n        app: test-nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.20\n        ports:\n        - containerPort: 80\n</code></pre></p>"},{"location":"development/evals/writing/#step-2-configure-setupteardown","title":"Step 2: Configure Setup/Teardown","text":"<pre><code>user_prompt: 'How is the test-nginx deployment performing?'\nbefore-test: kubectl apply -f manifest.yaml\nafter-test: kubectl delete -f manifest.yaml\n# ... rest of configuration\n</code></pre>"},{"location":"development/evals/writing/#step-3-run-live-test","title":"Step 3: Run Live Test","text":"<pre><code>RUN_LIVE=true pytest ./tests/llm/test_ask_holmes.py -k \"your_test\"\n</code></pre> <p><code>RUN_LIVE</code> is currently incompatible with <code>ITERATIONS</code> &gt; 1.</p>"},{"location":"development/evals/writing/#evaluation-scoring","title":"Evaluation Scoring","text":""},{"location":"development/evals/writing/#correctness-score","title":"Correctness Score","text":"<p>Measures how well the output matches expected elements: - 1: Match - 0: Mismatch</p>"},{"location":"development/evals/writing/#setting-minimum-scores","title":"Setting Minimum Scores","text":"<pre><code>evaluation:\n  correctness: 1\n</code></pre>"},{"location":"development/evals/writing/#best-practices","title":"Best Practices","text":""},{"location":"development/evals/writing/#test-design","title":"Test Design","text":"<ol> <li>Start simple: Begin with basic scenarios before complex edge cases</li> <li>Clear expectations: Write specific, measurable expected outputs</li> <li>Realistic scenarios: Base tests on actual user problems</li> <li>Incremental complexity: Build from simple to complex test cases</li> </ol>"},{"location":"development/evals/writing/#mock-data-quality","title":"Mock Data Quality","text":"<ol> <li>Representative data: Use realistic kubectl outputs and logs</li> <li>Error scenarios: Include failure modes and edge cases</li> <li>Consistent formatting: Match actual tool output formats</li> <li>Sufficient detail: Include enough information for proper diagnosis</li> <li>Run repeatedly: Run mock generation many times to ensure all investigative paths are covered by mock files</li> </ol>"},{"location":"development/evals/writing/#troubleshooting-test-creation","title":"Troubleshooting Test Creation","text":""},{"location":"development/evals/writing/#common-issues","title":"Common Issues","text":"<p>Test always fails with low correctness score: - Check if expected_output matches actual LLM capabilities - Verify mock data provides sufficient information - Consider lowering score threshold temporarily</p> <p>Missing tool outputs: - Ensure mock files are named correctly - Check that required toolsets are enabled - Verify mock file content is properly formatted</p> <p>Inconsistent results: - Run multiple iterations: <code>ITERATIONS=5</code> - Check for non-deterministic elements in prompts - Consider using temperature=0 for more consistent outputs</p>"},{"location":"development/evals/writing/#debugging-commands","title":"Debugging Commands","text":"<pre><code># Verbose output showing all details\npytest -v -s ./tests/llm/test_ask_holmes.py -k \"your_test\"\n\n# Generate fresh mocks from live system\n# set `generate_mocks: True` in test_case.yaml` and then:\npytest ./tests/llm/test_ask_holmes.py -k \"your_test\"\n</code></pre> <p>This completes the evaluation writing guide. The next step is setting up reporting and analysis using Braintrust.</p>"},{"location":"installation/","title":"Getting Started","text":"<p>Choose your installation method and get HolmesGPT running in minutes.</p>"},{"location":"installation/#most-popular-options","title":"Most Popular Options","text":"<ul> <li> <p> Install CLI</p> <p>Run HolmesGPT from your terminal</p> <p> Install</p> </li> <li> <p> Install UI/TUI</p> <p>Use through a web interface or K9s plugin</p> <p> Install</p> </li> </ul>"},{"location":"installation/#more-options","title":"More Options","text":"<p>Using HolmesGPT via an API or Python SDK</p> <ul> <li>Deploy as a service in your cluster using a Helm Chart</li> <li>Embed HolmesGPT in your applications with a Python SDK</li> </ul>"},{"location":"installation/#need-help","title":"Need Help?","text":"<p>If you encounter issues during setup:</p> <ul> <li>Check our troubleshooting guide</li> <li>Visit our FAQ</li> <li>Join our Slack community</li> </ul> <p>Ready to get started? Pick your installation method above! \ud83d\ude80</p>"},{"location":"installation/cli-installation/","title":"Install CLI","text":"<p>Run HolmesGPT from your terminal as a standalone CLI tool.</p>"},{"location":"installation/cli-installation/#installation-options","title":"Installation Options","text":"Homebrew (Mac/Linux)PipxFrom Source (Poetry)Docker Container <ol> <li> <p>Add our tap:    <pre><code>brew tap robusta-dev/homebrew-holmesgpt\n</code></pre></p> </li> <li> <p>Install HolmesGPT:    <pre><code>brew install holmesgpt\n</code></pre></p> </li> <li> <p>Verify installation:    <pre><code>holmes --help\n</code></pre></p> </li> </ol> <ol> <li> <p>Install pipx</p> </li> <li> <p>Install HolmesGPT:    <pre><code>pipx install \"https://github.com/robusta-dev/holmesgpt/archive/refs/heads/master.zip\"\n</code></pre></p> </li> <li> <p>Verify installation:    <pre><code>holmes version\n</code></pre></p> </li> </ol> <p>For development or custom builds:</p> <ol> <li> <p>Install Poetry</p> </li> <li> <p>Install HolmesGPT:    <pre><code>git clone https://github.com/robusta-dev/holmesgpt.git\ncd holmesgpt\npoetry install --no-root\n</code></pre></p> </li> <li> <p>Run HolmesGPT:    <pre><code>poetry run python3 holmes.py ask \"what pods are unhealthy and why?\"\n</code></pre></p> </li> </ol> <p>Run HolmesGPT using the prebuilt Docker container:</p> <pre><code>docker run -it --net=host \\\n  -v ~/.holmes:/root/.holmes \\\n  -v ~/.aws:/root/.aws \\\n  -v ~/.config/gcloud:/root/.config/gcloud \\\n  -v $HOME/.kube/config:/root/.kube/config \\\n  us-central1-docker.pkg.dev/genuine-flight-317411/devel/holmes ask \"what pods are unhealthy and why?\"\n</code></pre>"},{"location":"installation/cli-installation/#using-holmesgpt-in-a-shell-script","title":"Using HolmesGPT in a Shell Script","text":""},{"location":"installation/cli-installation/#using-holmesgpt-in-a-cicd-pipeline","title":"Using HolmesGPT in a CI/CD Pipeline","text":""},{"location":"installation/cli-installation/#quick-start","title":"Quick Start","text":"<p>After installation, set up your AI provider and run your first investigation:</p> <ol> <li> <p>Set up API key (View AI Provider docs for more options):    <pre><code>export OPENAI_API_KEY=\"your-api-key\"\nexport ANTHROPIC_API_KEY=\"your-api-key\"\nexport GOOGLE_API_KEY=\"your-api-key\"\n</code></pre></p> </li> <li> <p>Create a test pod to investigate:    <pre><code>kubectl apply -f https://raw.githubusercontent.com/robusta-dev/kubernetes-demos/main/pending_pods/pending_pod_node_selector.yaml\n</code></pre></p> </li> <li> <p>Ask your first question:    <pre><code>holmes ask \"what is wrong with the user-profile-import pod?\"\n</code></pre></p> </li> <li> <p>Try interactive mode:    <pre><code>holmes ask \"what pods are unhealthy and why?\" --interactive\n</code></pre></p> </li> </ol>"},{"location":"installation/cli-installation/#need-help","title":"Need Help?","text":"<ul> <li>Check our troubleshooting guide</li> <li>Join our Slack community</li> <li>Report issues on GitHub</li> </ul>"},{"location":"installation/first-investigation/","title":"First Investigation","text":"<p>This guide walks you through running your first HolmesGPT investigation, assuming you've installed the CLI.</p>"},{"location":"installation/first-investigation/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>\u2705 HolmesGPT CLI installed - See CLI Installation Guide</li> <li>\u2705 AI provider API key configured - See API Keys Setup</li> <li>\u2705 kubectl access to a Kubernetes cluster - Any cluster will work</li> </ul>"},{"location":"installation/first-investigation/#step-1-verify-your-setup","title":"Step 1: Verify Your Setup","text":"<p>First, let's make sure everything is working:</p> <pre><code># Check Holmes is installed\nholmes --help\n\n# Check kubectl access\nkubectl cluster-info\n\n# Verify your API key is set (choose one)\necho $OPENAI_API_KEY\necho $ANTHROPIC_API_KEY\necho $GOOGLE_API_KEY\n</code></pre>"},{"location":"installation/first-investigation/#step-2-create-a-test-scenario","title":"Step 2: Create a Test Scenario","text":"<p>Let's create a problematic pod that Holmes can investigate:</p> <pre><code># Create a pod with a common issue (wrong node selector)\nkubectl apply -f https://raw.githubusercontent.com/robusta-dev/kubernetes-demos/main/pending_pods/pending_pod_node_selector.yaml\n</code></pre> <p>This creates a pod that will remain in \"Pending\" state due to an invalid node selector.</p>"},{"location":"installation/first-investigation/#step-3-your-first-investigation","title":"Step 3: Your First Investigation","text":"<p>Now let's ask Holmes to investigate:</p> <pre><code>holmes ask \"what is wrong with the user-profile-import pod?\"\n</code></pre> <p>Expected output: <pre><code>\ud83d\udd0d Investigating: what is wrong with the user-profile-import pod?\n\n\ud83d\udccb Investigation Summary:\nThe user-profile-import pod is in Pending state due to an invalid node selector.\nThe pod specifies nodeSelector: gpu-node=true, but no nodes in the cluster\nhave this label.\n\n\ud83d\udd27 Recommended Actions:\n1. Remove the nodeSelector constraint, or\n2. Add the required label to a node: kubectl label node &lt;node-name&gt; gpu-node=true\n\n\ud83d\udcca Resource Details:\n- Pod Status: Pending\n- Namespace: default\n- Node Selector: gpu-node=true\n- Available Nodes: 3 (none matching selector)\n</code></pre></p>"},{"location":"installation/first-investigation/#step-4-try-interactive-mode","title":"Step 4: Try Interactive Mode","text":"<p>For follow-up questions, use interactive mode:</p> <pre><code>holmes ask \"what pods are failing?\" --interactive\n</code></pre> <p>This starts an interactive session where you can ask follow-up questions:</p> <pre><code>\ud83d\udd0d Investigating: what pods are failing?\n\n\ud83d\udccb Investigation found 1 failing pod:\n- user-profile-import (Pending): Invalid node selector\n\n\ud83d\udcac Ask a follow-up question (or 'quit' to exit):\n&gt; how do I fix the node selector issue?\n\n\ud83d\udd27 To fix the node selector issue, you have two options:\n\n1. Remove the nodeSelector (recommended):\n   kubectl patch pod user-profile-import -p '{\"spec\":{\"nodeSelector\":null}}'\n\n2. Label a node to match:\n   kubectl label node worker-1 gpu-node=true\n\n&gt; quit\n</code></pre>"},{"location":"installation/first-investigation/#step-5-investigate-different-scenarios","title":"Step 5: Investigate Different Scenarios","text":"<p>Let's create and investigate different types of issues:</p>"},{"location":"installation/first-investigation/#image-pull-error","title":"Image Pull Error","text":"<pre><code># Create pod with non-existent image\nkubectl run bad-image --image=nonexistent:latest\n\n# Investigate\nholmes ask \"why is the bad-image pod failing?\"\n</code></pre>"},{"location":"installation/first-investigation/#resource-limits","title":"Resource Limits","text":"<pre><code># Create pod requesting too much memory\nkubectl run memory-hog --image=nginx --requests='memory=100Gi'\n\n# Investigate\nholmes ask \"what's wrong with the memory-hog pod?\"\n</code></pre>"},{"location":"installation/first-investigation/#application-crash","title":"Application Crash","text":"<pre><code># Create a crashing pod\nkubectl run crash-pod --image=busybox --command -- sh -c 'exit 1'\n\n# Investigate\nholmes ask \"why does crash-pod keep restarting?\"\n</code></pre>"},{"location":"installation/first-investigation/#step-6-investigate-real-alerts","title":"Step 6: Investigate Real Alerts","text":"<p>If you have Prometheus AlertManager, investigate real alerts:</p>"},{"location":"installation/first-investigation/#from-alertmanager","title":"From AlertManager","text":"<pre><code># Port-forward to AlertManager (if running in cluster)\nkubectl port-forward -n monitoring svc/alertmanager 9093:9093 &amp;\n\n# Investigate current alerts\nholmes investigate alertmanager --alertmanager-url http://localhost:9093\n</code></pre>"},{"location":"installation/first-investigation/#from-pagerduty","title":"From PagerDuty","text":"<pre><code># Investigate PagerDuty incidents\nholmes investigate pagerduty --pagerduty-api-key YOUR_API_KEY\n\n# Update incident with analysis\nholmes investigate pagerduty --pagerduty-api-key YOUR_API_KEY --update\n</code></pre>"},{"location":"installation/first-investigation/#from-opsgenie","title":"From OpsGenie","text":"<pre><code># Investigate OpsGenie alerts\nholmes investigate opsgenie --opsgenie-api-key YOUR_API_KEY\n</code></pre>"},{"location":"installation/first-investigation/#step-7-advanced-questions","title":"Step 7: Advanced Questions","text":"<p>Try these more advanced investigation scenarios:</p>"},{"location":"installation/first-investigation/#cluster-wide-issues","title":"Cluster-wide Issues","text":"<pre><code>holmes ask \"what nodes are having problems?\"\nholmes ask \"show me all unhealthy workloads\"\nholmes ask \"what's consuming the most resources?\"\n</code></pre>"},{"location":"installation/first-investigation/#performance-issues","title":"Performance Issues","text":"<pre><code>holmes ask \"why is my application slow?\"\nholmes ask \"what pods are being throttled?\"\nholmes ask \"show me memory usage trends\"\n</code></pre>"},{"location":"installation/first-investigation/#networking-issues","title":"Networking Issues","text":"<pre><code>holmes ask \"are there any networking problems?\"\nholmes ask \"why can't pods reach the database?\"\nholmes ask \"show me service connectivity issues\"\n</code></pre>"},{"location":"installation/first-investigation/#step-8-using-context-files","title":"Step 8: Using Context Files","text":"<p>Provide additional context to Holmes:</p> <pre><code># Save pod logs to a file\nkubectl logs user-profile-import &gt; pod-logs.txt\n\n# Ask Holmes to analyze with context\nholmes ask \"analyze this pod failure\" --file pod-logs.txt\n\n# Multiple files\nholmes ask \"investigate this deployment\" \\\n  --file deployment.yaml \\\n  --file logs.txt \\\n  --file metrics.json\n</code></pre>"},{"location":"installation/first-investigation/#tips-for-better-investigations","title":"Tips for Better Investigations","text":""},{"location":"installation/first-investigation/#1-be-specific","title":"1. Be Specific","text":"<p>\u274c Vague: \"something is broken\" \u2705 Specific: \"why is the checkout service returning 500 errors?\"</p>"},{"location":"installation/first-investigation/#2-include-context","title":"2. Include Context","text":"<p>\u274c No context: \"fix this\" \u2705 With context: \"the payment pod is crashing since the last deployment\"</p>"},{"location":"installation/first-investigation/#3-use-interactive-mode","title":"3. Use Interactive Mode","text":"<p>For complex issues, start broad and narrow down:</p> <pre><code>holmes ask \"what's wrong with my cluster?\" --interactive\n&gt; which namespace has the most issues?\n&gt; what's wrong with the payment service?\n&gt; how do I fix the database connection errors?\n</code></pre>"},{"location":"installation/first-investigation/#4-provide-timeframes","title":"4. Provide Timeframes","text":"<pre><code>holmes ask \"what problems started in the last hour?\"\nholmes ask \"show me errors since yesterday\"\n</code></pre>"},{"location":"installation/first-investigation/#common-patterns","title":"Common Patterns","text":""},{"location":"installation/first-investigation/#deployment-issues","title":"Deployment Issues","text":"<pre><code># After a deployment\nholmes ask \"are there any issues with the latest deployment?\"\nholmes ask \"did the rollout succeed in the production namespace?\"\n</code></pre>"},{"location":"installation/first-investigation/#resource-problems","title":"Resource Problems","text":"<pre><code># Resource monitoring\nholmes ask \"what pods are using too much memory?\"\nholmes ask \"which nodes are overloaded?\"\n</code></pre>"},{"location":"installation/first-investigation/#application-errors","title":"Application Errors","text":"<pre><code># Application debugging\nholmes ask \"why are users getting timeout errors?\"\nholmes ask \"what's causing the high error rate?\"\n</code></pre>"},{"location":"installation/first-investigation/#clean-up","title":"Clean Up","text":"<p>Remove the test resources:</p> <pre><code>kubectl delete pod user-profile-import\nkubectl delete pod bad-image\nkubectl delete pod memory-hog\nkubectl delete pod crash-pod\n</code></pre>"},{"location":"installation/first-investigation/#next-steps","title":"Next Steps","text":"<p>Now that you've run your first investigations:</p> <ul> <li>Configure Custom Toolsets - Add your monitoring tools</li> <li>Set Up Runbooks - Create organization-specific guidance</li> <li>Integrate with CI/CD - Automate investigation in pipelines</li> <li>Deploy as a Service - Use HTTP API for integrations</li> </ul>"},{"location":"installation/first-investigation/#need-help","title":"Need Help?","text":"<ul> <li>Common issues: Check our troubleshooting guide</li> <li>Community support: Join our Slack community</li> <li>Bug reports: Create an issue on GitHub</li> </ul>"},{"location":"installation/first-investigation/#whats-next","title":"What's Next?","text":"<p>\ud83c\udf89 Congratulations! You've successfully run your first HolmesGPT investigations.</p> <p>Holmes gets more powerful as you: - Connect more data sources (Prometheus, Grafana, etc.) - Add custom runbooks for your specific infrastructure - Integrate with your existing workflows and tools</p> <p>Happy investigating! \ud83d\udd75\ufe0f</p>"},{"location":"installation/kubernetes-installation/","title":"Install Helm Chart","text":"<p>Deploy HolmesGPT as a service in your Kubernetes cluster with an HTTP API access.</p> <p>When to use the Helm chart?</p> <p>Most users should use the CLI or UI/TUI instead. Using the Helm chart is only recommended if you're building a custom integration over an HTTP API.</p>"},{"location":"installation/kubernetes-installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster</li> <li>Helm</li> <li>kubectl configured to access your cluster</li> <li>Supported AI Provider API key.</li> </ul>"},{"location":"installation/kubernetes-installation/#installation","title":"Installation","text":"Basic InstallationCustom Installation <p>Add the Helm Repository</p> <pre><code>helm repo add holmesgpt https://robusta-dev.github.io/holmesgpt\nhelm repo update\n</code></pre> <p>Install HolmesGPT</p> <pre><code>helm install holmesgpt holmesgpt/holmes\n</code></pre> <p>Create a <code>values.yaml</code> file</p> <pre><code># values.yaml\nconfig:\n  aiProvider: \"openai\"\n  apiKey: \"your-api-key\"  # Or use secret\n  model: \"gpt-4\"\n\n# Use existing secret for API key\nsecret:\n  create: false\n  name: \"holmes-secrets\"\n  key: \"api-key\"\n</code></pre> <p>Install with custom values</p> <pre><code>helm install holmesgpt holmesgpt/holmes -f values.yaml\n</code></pre>"},{"location":"installation/kubernetes-installation/#using-secrets-for-api-keys","title":"Using Secrets for API Keys","text":"<p>Create a secret for your AI provider API key:</p> <pre><code>kubectl create secret generic holmes-secrets \\\n  --from-literal=api-key=\"your-api-key\"\n</code></pre> <p>Reference it in your <code>values.yaml</code>:</p> <pre><code>secret:\n  create: false\n  name: \"holmes-secrets\"\n  key: \"api-key\"\n</code></pre>"},{"location":"installation/kubernetes-installation/#http-api-usage","title":"HTTP API Usage","text":""},{"location":"installation/kubernetes-installation/#access-the-service","title":"Access the Service","text":""},{"location":"installation/kubernetes-installation/#port-forward-development","title":"Port Forward (Development)","text":"<pre><code>kubectl port-forward svc/holmesgpt 8080:80\n</code></pre>"},{"location":"installation/kubernetes-installation/#api-endpoints","title":"API Endpoints","text":""},{"location":"installation/kubernetes-installation/#ask-questions","title":"Ask Questions","text":"<pre><code>curl -X POST http://localhost:8080/ask \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"question\": \"what pods are failing in the default namespace?\"\n  }'\n</code></pre>"},{"location":"installation/kubernetes-installation/#investigate-alerts","title":"Investigate Alerts","text":"<pre><code>curl -X POST http://localhost:8080/investigate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"alert\": {\n      \"name\": \"HighMemoryUsage\",\n      \"namespace\": \"production\",\n      \"pod\": \"web-app-123\"\n    }\n  }'\n</code></pre>"},{"location":"installation/kubernetes-installation/#health-check","title":"Health Check","text":"<pre><code>curl http://localhost:8080/health\n</code></pre>"},{"location":"installation/kubernetes-installation/#response-format","title":"Response Format","text":"<pre><code>{\n  \"status\": \"success\",\n  \"result\": \"Analysis of your Kubernetes cluster shows...\",\n  \"investigation_id\": \"inv_123456\",\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"toolsets_used\": [\"kubernetes\", \"prometheus\"],\n  \"recommendations\": [\n    \"Scale up the deployment\",\n    \"Check resource limits\"\n  ]\n}\n</code></pre>"},{"location":"installation/kubernetes-installation/#upgrading","title":"Upgrading","text":"<pre><code>helm repo update\nhelm upgrade holmesgpt holmesgpt/holmes -f values.yaml\n</code></pre>"},{"location":"installation/kubernetes-installation/#uninstalling","title":"Uninstalling","text":"<pre><code>helm uninstall holmesgpt\n</code></pre>"},{"location":"installation/kubernetes-installation/#need-help","title":"Need Help?","text":"<ul> <li>Check our Helm chart documentation</li> <li>Join our Slack community</li> <li>Report issues on GitHub</li> </ul>"},{"location":"installation/python-installation/","title":"Install Python SDK","text":"<p>Embed HolmesGPT in your own applications with the Python API for programmatic root cause analysis.</p>"},{"location":"installation/python-installation/#installation","title":"Installation","text":"pippipenvpoetry <pre><code>pip install holmesgpt\n</code></pre> <pre><code>pipenv install holmesgpt\n</code></pre> <pre><code>poetry add holmesgpt\n</code></pre>"},{"location":"installation/python-installation/#quick-start","title":"Quick Start","text":""},{"location":"installation/python-installation/#basic-usage","title":"Basic Usage","text":"<pre><code>from holmes import ask_holmes\n\n# Simple question\nresult = ask_holmes(\"what pods are failing in production?\")\nprint(result)\n\n# With specific context\nresult = ask_holmes(\n    \"analyze this pod failure\",\n    context_files=[\"pod.yaml\", \"logs.txt\"]\n)\nprint(result)\n</code></pre>"},{"location":"installation/python-installation/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>from holmes.core import HolmesGPT\nfrom holmes.config import Config\n\n# Configure with custom settings\nconfig = Config(\n    ai_provider=\"openai\",\n    api_key=\"your-api-key\",\n    model=\"gpt-4\",\n    max_tokens=2000\n)\n\n# Initialize Holmes\nholmes = HolmesGPT(config)\n\n# Run investigation\nresult = holmes.investigate(\"what's wrong with my deployment?\")\n</code></pre> <pre><code>from holmes.core import HolmesGPT\nfrom holmes.toolsets import CustomToolset\n\n# Define custom toolset\nclass MyCustomToolset(CustomToolset):\n    def get_custom_data(self):\n        # Your custom data collection logic\n        return {\"custom_metrics\": \"data\"}\n\n# Configure Holmes with custom toolset\nholmes = HolmesGPT()\nholmes.add_toolset(MyCustomToolset())\n\nresult = holmes.investigate(\"analyze using my custom data\")\n</code></pre>"},{"location":"installation/python-installation/#api-reference","title":"API Reference","text":""},{"location":"installation/python-installation/#core-functions","title":"Core Functions","text":""},{"location":"installation/python-installation/#ask_holmesquestion-kwargs","title":"<code>ask_holmes(question, **kwargs)</code>","text":"<p>Ask Holmes a question synchronously.</p> <p>Parameters: - <code>question</code> (str): The question to ask - <code>context_files</code> (list, optional): List of file paths for context - <code>toolsets</code> (list, optional): Custom toolsets to use - <code>config</code> (Config, optional): Custom configuration</p> <p>Returns: - <code>str</code>: Holmes' analysis and recommendations</p>"},{"location":"installation/python-installation/#ask_holmes_asyncquestion-kwargs","title":"<code>ask_holmes_async(question, **kwargs)</code>","text":"<p>Async version of <code>ask_holmes()</code>.</p>"},{"location":"installation/python-installation/#holmesgptconfignone","title":"<code>HolmesGPT(config=None)</code>","text":"<p>Main Holmes class for advanced usage.</p> <p>Methods: - <code>investigate(question)</code>: Run investigation - <code>add_toolset(toolset)</code>: Add custom toolset - <code>set_context(context)</code>: Set investigation context</p>"},{"location":"installation/python-installation/#configuration-options","title":"Configuration Options","text":"<pre><code>from holmes.config import Config\n\nconfig = Config(\n    # AI Provider settings\n    ai_provider=\"openai\",  # \"openai\", \"anthropic\", \"bedrock\", \"vertex\"\n    api_key=\"your-key\",\n    model=\"gpt-4\",\n\n    # Investigation settings\n    max_tokens=2000,\n    temperature=0.1,\n    enable_context=True,\n\n    # Kubernetes settings\n    kubeconfig_path=\"~/.kube/config\",\n    default_namespace=\"default\",\n\n    # Custom toolsets\n    custom_toolsets=[],\n    enabled_toolsets=[\"kubernetes\", \"prometheus\"]\n)\n</code></pre>"},{"location":"installation/python-installation/#environment-variables","title":"Environment Variables","text":"<p>Set these environment variables for configuration:</p> <pre><code># AI Provider (choose one)\nexport OPENAI_API_KEY=\"your-openai-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\nexport GOOGLE_API_KEY=\"your-google-key\"\n\n# Optional: Custom configuration\nexport HOLMES_CONFIG_PATH=\"/path/to/config.yaml\"\nexport HOLMES_LOG_LEVEL=\"INFO\"\n</code></pre>"},{"location":"installation/python-installation/#examples-repository","title":"Examples Repository","text":"<p>Check out our examples repository for:</p> <ul> <li>Flask integration - Web app with Holmes</li> <li>Slack bot - Custom Slack integration</li> <li>Jupyter notebooks - Data analysis workflows</li> <li>CI/CD pipelines - GitHub Actions examples</li> </ul>"},{"location":"installation/python-installation/#next-steps","title":"Next Steps","text":"<ul> <li>API Keys Setup - Configure your AI provider</li> <li>Run Your First Investigation - Complete walkthrough</li> <li>Helm Configuration - Advanced settings and custom toolsets</li> </ul>"},{"location":"installation/python-installation/#need-help","title":"Need Help?","text":"<ul> <li>Check our Python SDK documentation for detailed API reference</li> <li>Join our Slack community</li> <li>Report issues on GitHub</li> </ul>"},{"location":"installation/ui-installation/","title":"Install UI/TUI","text":"<p>Use HolmesGPT through graphical interfaces via third-party integrations.</p>"},{"location":"installation/ui-installation/#k9s-plugin","title":"K9s Plugin","text":"<p>TODO Video of installing and configuring K9s Plugin</p> <p>Integrate HolmesGPT into your K9s Kubernetes terminal.</p>"},{"location":"installation/ui-installation/#install","title":"Install","text":"<ol> <li>Create plugin directory: <pre><code>mkdir -p ~/.k9s/plugins\n</code></pre></li> <li>Download plugin config: <pre><code>curl -o ~/.k9s/plugins/holmes.yaml \\\n  https://raw.githubusercontent.com/robusta-dev/holmesgpt/master/k9s-plugin.yaml\n</code></pre></li> <li>Set your API key: <pre><code>export OPENAI_API_KEY=\"your-api-key\"\n# or\nexport ANTHROPIC_API_KEY=\"your-api-key\"\n</code></pre></li> <li>Install HolmesGPT CLI: <pre><code>brew install robusta-dev/homebrew-holmesgpt/holmesgpt\n</code></pre></li> </ol>"},{"location":"installation/ui-installation/#usage","title":"Usage","text":"<ul> <li>Open K9s, select a resource, and press <code>Ctrl+H</code> to invoke HolmesGPT.</li> <li>Holmes will analyze the selected resource and display results.</li> </ul>"},{"location":"installation/ui-installation/#web-ui-robusta","title":"Web UI (Robusta)","text":"<p>The fastest way to use HolmesGPT is via the managed Robusta SaaS platform.</p> <p></p>"},{"location":"installation/ui-installation/#get-started","title":"Get Started","text":"<ol> <li>Sign up: platform.robusta.dev</li> <li>Connect your cluster: Follow the in-app wizard to install the Robusta agent and configure data sources.</li> <li>Investigate: Use the \"Ask Holmes\" chat to analyze alerts or ask questions like:</li> <li>\u201cWhat pods are failing in production?\u201d</li> <li>\u201cWhy did this alert fire?\u201d</li> </ol>"},{"location":"installation/ui-installation/#slack-bot-robusta","title":"Slack Bot (Robusta)","text":"<p>Tag HolmesGPT in any Slack message for instant analysis</p> <p></p>"},{"location":"installation/ui-installation/#need-help","title":"Need Help?","text":"<ul> <li>Troubleshooting guide</li> <li>Slack community</li> <li>GitHub Issues</li> </ul>"},{"location":"introduction/","title":"Introduction to HolmesGPT","text":"<p>Welcome to HolmesGPT, the extensible AI agent that transforms how DevOps and SRE teams troubleshoot Cloud-Native issues, investigate alerts, and bridge knowledge gaps through intelligent automation.</p>"},{"location":"introduction/#what-youll-learn","title":"What You'll Learn","text":"<p>In this section, you'll discover:</p> <ul> <li>What is HolmesGPT? - Core concepts and problem it solves</li> <li>Key Features - Comprehensive overview of capabilities</li> <li>Architecture Overview - How HolmesGPT works under the hood</li> <li>Getting Help - Support resources and community</li> </ul>"},{"location":"introduction/#why-holmesgpt-exists","title":"Why HolmesGPT Exists","text":"<p>Modern DevOps teams face two critical challenges: alert fatigue and knowledge gaps. Infrastructure generates thousands of alerts daily, while troubleshooting often requires expertise scattered across multiple team members and systems. HolmesGPT addresses both challenges through its extensible architecture that can integrate any troubleshooting tool through custom toolsets, while empowering teams to encode their organizational knowledge through natural-language runbooks. This combination of tool-calling capabilities and knowledge integration transforms generic AI models into specialized DevOps assistants.</p> <p>Ready to get started? Begin with What is HolmesGPT? to understand the fundamentals.</p>"},{"location":"reference/","title":"Reference","text":"<p>Complete reference documentation for HolmesGPT configuration, commands, and troubleshooting.</p>"},{"location":"reference/#reference-documentation","title":"Reference Documentation","text":""},{"location":"reference/#configuration-reference","title":"Configuration Reference","text":"<ul> <li>Helm Configuration - Complete Helm configuration reference</li> </ul>"},{"location":"reference/#quick-reference","title":"Quick Reference","text":""},{"location":"reference/#essential-commands","title":"Essential Commands","text":"<pre><code># Install HolmesGPT\nhelm install holmesgpt robusta/robusta --set enableHolmesGPT=true\n\n# Check status\nkubectl get pods -n robusta\n\n# View logs\nkubectl logs -n robusta deployment/holmes\n</code></pre>"},{"location":"reference/#key-configuration-keys","title":"Key Configuration Keys","text":"<pre><code># Enable HolmesGPT\nenableHolmesGPT: true\n\n# AI Provider (OpenAI example)\nholmes:\n  additionalEnvVars:\n  - name: MODEL\n    value: gpt-4o\n  - name: OPENAI_API_KEY\n    valueFrom:\n      secretKeyRef:\n        name: holmes-secrets\n        key: openAiKey\n</code></pre>"},{"location":"reference/#common-troubleshooting","title":"Common Troubleshooting","text":"Issue Solution AI provider errors Check API keys and model availability No data sources Verify toolset configuration and credentials Slow responses Review model selection and resource limits Permission errors Check RBAC and secret access"},{"location":"reference/#getting-help","title":"Getting Help","text":"<p>Can't find what you're looking for?</p> <ol> <li>Search this documentation using the search bar</li> <li>Check our troubleshooting guide for common issues</li> <li>Visit our troubleshooting guide</li> <li>Ask in our Slack community</li> <li>Report issues on GitHub</li> </ol> <p>Start with the troubleshooting guide for answers to common questions.</p>"},{"location":"reference/helm-configuration/","title":"Helm Configuration","text":"<p>Complete reference for configuring HolmesGPT with Helm values.</p>"},{"location":"reference/helm-configuration/#basic-configuration","title":"Basic Configuration","text":"<pre><code># values.yaml\nconfig:\n  # AI Provider Settings\n  aiProvider: \"openai\"  # \"openai\", \"anthropic\", \"bedrock\", \"vertex\"\n  model: \"gpt-4\"\n  maxTokens: 2000\n  temperature: 0.1\n\n# API Key Configuration\nsecret:\n  create: true\n  name: \"holmes-secrets\"\n  key: \"api-key\"\n  value: \"your-api-key\"\n\n# Service Configuration\nservice:\n  type: ClusterIP\n  port: 80\n  targetPort: 8080\n\n# Resource Limits\nresources:\n  limits:\n    cpu: 500m\n    memory: 512Mi\n  requests:\n    cpu: 250m\n    memory: 256Mi\n\n# RBAC\nrbac:\n  create: true\n  serviceAccountName: \"holmesgpt\"\n\n# Ingress\ningress:\n  enabled: false\n  className: \"nginx\"\n  annotations: {}\n  hosts:\n    - host: holmes.example.com\n      paths:\n        - path: /\n          pathType: Prefix\n  tls: []\n</code></pre>"},{"location":"reference/helm-configuration/#ai-provider-configuration","title":"AI Provider Configuration","text":""},{"location":"reference/helm-configuration/#openai","title":"OpenAI","text":"<pre><code>config:\n  aiProvider: \"openai\"\n  model: \"gpt-4\"\n  apiEndpoint: \"https://api.openai.com/v1\"\n\nsecret:\n  create: true\n  value: \"sk-...\"\n</code></pre>"},{"location":"reference/helm-configuration/#anthropic","title":"Anthropic","text":"<pre><code>config:\n  aiProvider: \"anthropic\"\n  model: \"claude-3-sonnet-20240229\"\n  apiEndpoint: \"https://api.anthropic.com\"\n\nsecret:\n  create: true\n  value: \"sk-ant-...\"\n</code></pre>"},{"location":"reference/helm-configuration/#aws-bedrock","title":"AWS Bedrock","text":"<pre><code>config:\n  aiProvider: \"bedrock\"\n  model: \"anthropic.claude-3-sonnet-20240229-v1:0\"\n  region: \"us-east-1\"\n\n# Use IAM roles or provide credentials\naws:\n  accessKeyId: \"AKIA...\"\n  secretAccessKey: \"...\"\n  sessionToken: \"\"  # Optional\n</code></pre>"},{"location":"reference/helm-configuration/#google-vertex-ai","title":"Google Vertex AI","text":"<pre><code>config:\n  aiProvider: \"vertex\"\n  model: \"gemini-pro\"\n  project: \"your-project-id\"\n  location: \"us-central1\"\n\n# Provide service account key\ngcp:\n  serviceAccountKey: |\n    {\n      \"type\": \"service_account\",\n      ...\n    }\n</code></pre>"},{"location":"reference/helm-configuration/#toolset-configuration","title":"Toolset Configuration","text":"<pre><code>toolsets:\n  - name: \"kubernetes\"\n    enabled: true\n    config: {}\n\n  - name: \"prometheus\"\n    enabled: true\n    config:\n      url: \"http://prometheus:9090\"\n      timeout: \"30s\"\n\n  - name: \"grafana\"\n    enabled: true\n    config:\n      url: \"http://grafana:3000\"\n      username: \"admin\"\n      password: \"admin\"\n\n  - name: \"loki\"\n    enabled: true\n    config:\n      url: \"http://loki:3100\"\n\n  - name: \"tempo\"\n    enabled: true\n    config:\n      url: \"http://tempo:3200\"\n</code></pre>"},{"location":"reference/helm-configuration/#security-configuration","title":"Security Configuration","text":""},{"location":"reference/helm-configuration/#rbac-permissions","title":"RBAC Permissions","text":"<pre><code>rbac:\n  create: true\n\n# Custom ClusterRole rules\nrbacRules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"services\", \"events\", \"nodes\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"apps\"]\n    resources: [\"deployments\", \"replicasets\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n</code></pre>"},{"location":"reference/helm-configuration/#network-policies","title":"Network Policies","text":"<pre><code>networkPolicy:\n  enabled: true\n  ingress:\n    - from:\n      - namespaceSelector:\n          matchLabels:\n            name: monitoring\n      ports:\n      - protocol: TCP\n        port: 8080\n  egress:\n    - to: []\n      ports:\n      - protocol: TCP\n        port: 443  # HTTPS to AI providers\n</code></pre>"},{"location":"reference/helm-configuration/#pod-security-context","title":"Pod Security Context","text":"<pre><code>securityContext:\n  runAsNonRoot: true\n  runAsUser: 1001\n  runAsGroup: 1001\n  fsGroup: 1001\n\ncontainerSecurityContext:\n  allowPrivilegeEscalation: false\n  readOnlyRootFilesystem: true\n  capabilities:\n    drop:\n    - ALL\n</code></pre>"},{"location":"reference/helm-configuration/#monitoring-configuration","title":"Monitoring Configuration","text":"<pre><code>monitoring:\n  enabled: true\n\n# Prometheus metrics\nmetrics:\n  enabled: true\n  port: 9090\n  path: /metrics\n\n# Service Monitor for Prometheus Operator\nserviceMonitor:\n  enabled: true\n  namespace: monitoring\n  labels:\n    release: prometheus\n\n# Grafana Dashboard\ngrafanaDashboard:\n  enabled: true\n  namespace: monitoring\n</code></pre>"},{"location":"reference/helm-configuration/#scaling-configuration","title":"Scaling Configuration","text":""},{"location":"reference/helm-configuration/#horizontal-pod-autoscaler","title":"Horizontal Pod Autoscaler","text":"<pre><code>autoscaling:\n  enabled: true\n  minReplicas: 2\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 70\n  targetMemoryUtilizationPercentage: 80\n</code></pre>"},{"location":"reference/helm-configuration/#vertical-pod-autoscaler","title":"Vertical Pod Autoscaler","text":"<pre><code>verticalPodAutoscaler:\n  enabled: true\n  updateMode: \"Auto\"\n  resourcePolicy:\n    containerPolicies:\n    - containerName: holmesgpt\n      maxAllowed:\n        cpu: 1\n        memory: 1Gi\n</code></pre>"},{"location":"reference/helm-configuration/#storage-configuration","title":"Storage Configuration","text":"<pre><code>persistence:\n  enabled: true\n  storageClass: \"standard\"\n  size: \"10Gi\"\n  accessMode: \"ReadWriteOnce\"\n\n# Volume mounts\nvolumes:\n  - name: config\n    configMap:\n      name: holmes-config\n  - name: cache\n    emptyDir: {}\n\nvolumeMounts:\n  - name: config\n    mountPath: /app/config\n    readOnly: true\n  - name: cache\n    mountPath: /app/cache\n</code></pre>"},{"location":"reference/helm-configuration/#environment-variables","title":"Environment Variables","text":"<pre><code>env:\n  - name: HOLMES_LOG_LEVEL\n    value: \"INFO\"\n  - name: HOLMES_MAX_INVESTIGATIONS\n    value: \"10\"\n  - name: HOLMES_CACHE_TTL\n    value: \"3600\"\n  - name: OPENAI_API_KEY\n    valueFrom:\n      secretKeyRef:\n        name: holmes-secrets\n        key: api-key\n</code></pre>"},{"location":"reference/helm-configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"reference/helm-configuration/#multi-tenant-setup","title":"Multi-Tenant Setup","text":"<pre><code># Namespace isolation\nnamespaceSelector:\n  matchLabels:\n    holmes-enabled: \"true\"\n\n# Per-tenant configuration\ntenants:\n  - name: \"team-a\"\n    namespace: \"team-a\"\n    aiProvider: \"openai\"\n    apiKey: \"sk-team-a-key\"\n  - name: \"team-b\"\n    namespace: \"team-b\"\n    aiProvider: \"anthropic\"\n    apiKey: \"sk-team-b-key\"\n</code></pre>"},{"location":"reference/helm-configuration/#custom-toolsets","title":"Custom Toolsets","text":"<pre><code>customToolsets:\n  - name: \"custom-monitoring\"\n    image: \"myregistry/custom-toolset:latest\"\n    config:\n      endpoint: \"https://my-monitoring.com\"\n      apiKey: \"secret\"\n</code></pre>"},{"location":"reference/helm-configuration/#production-recommendations","title":"Production Recommendations","text":"<pre><code># Production values.yaml\nreplicaCount: 3\n\nresources:\n  limits:\n    cpu: 1\n    memory: 1Gi\n  requests:\n    cpu: 500m\n    memory: 512Mi\n\nautoscaling:\n  enabled: true\n  minReplicas: 3\n  maxReplicas: 20\n\ningress:\n  enabled: true\n  className: \"nginx\"\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n    nginx.ingress.kubernetes.io/rate-limit: \"100\"\n  tls:\n    - secretName: holmes-tls\n      hosts:\n        - holmes.company.com\n\nmonitoring:\n  enabled: true\n\nnetworkPolicy:\n  enabled: true\n\nsecurityContext:\n  runAsNonRoot: true\n  runAsUser: 1001\n</code></pre>"},{"location":"reference/helm-configuration/#configuration-validation","title":"Configuration Validation","text":"<p>Validate your configuration before deployment:</p> <pre><code># Dry run\nhelm install holmesgpt holmesgpt/holmes -f values.yaml --dry-run\n\n# Template output\nhelm template holmesgpt holmesgpt/holmes -f values.yaml\n\n# Validate with kubeval\nhelm template holmesgpt holmesgpt/holmes -f values.yaml | kubeval\n</code></pre>"},{"location":"reference/helm-configuration/#troubleshooting-configuration","title":"Troubleshooting Configuration","text":"<p>Common configuration issues:</p> <ol> <li> <p>Invalid YAML syntax <pre><code>yamllint values.yaml\n</code></pre></p> </li> <li> <p>Missing required values <pre><code>helm lint -f values.yaml\n</code></pre></p> </li> <li> <p>RBAC permission issues <pre><code>kubectl auth can-i get pods --as=system:serviceaccount:default:holmesgpt\n</code></pre></p> </li> </ol> <p>For more configuration examples, see the examples directory in our repository.</p>"},{"location":"reference/troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and solutions for HolmesGPT deployment and operation.</p>"},{"location":"reference/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"reference/troubleshooting/#helm-installation-failed","title":"Helm Installation Failed","text":"<p>Problem: Helm install command fails with validation errors.</p> <p>Solution: <pre><code># Check Helm version (requires 3.x)\nhelm version\n\n# Verify repository is added\nhelm repo list | grep holmesgpt\n\n# Update repository\nhelm repo update\n\n# Check available chart versions\nhelm search repo holmesgpt\n</code></pre></p>"},{"location":"reference/troubleshooting/#pod-not-starting","title":"Pod Not Starting","text":"<p>Problem: HolmesGPT pod stuck in <code>Pending</code> or <code>CrashLoopBackOff</code> state.</p> <p>Diagnosis: <pre><code># Check pod status\nkubectl get pods -l app=holmesgpt\n\n# View pod events\nkubectl describe pod &lt;pod-name&gt;\n\n# Check logs\nkubectl logs &lt;pod-name&gt;\n</code></pre></p> <p>Common causes: - Insufficient resources - Missing API key - RBAC permission issues - Image pull failures</p>"},{"location":"reference/troubleshooting/#api-key-issues","title":"API Key Issues","text":""},{"location":"reference/troubleshooting/#invalid-api-key-error","title":"Invalid API Key Error","text":"<p>Problem: <code>Invalid API key</code> errors in logs.</p> <p>Solution: <pre><code># Verify secret exists\nkubectl get secret holmes-secrets\n\n# Check secret content (base64 encoded)\nkubectl get secret holmes-secrets -o yaml\n\n# Update API key\nkubectl patch secret holmes-secrets -p '{\"data\":{\"api-key\":\"&lt;base64-encoded-key&gt;\"}}'\n\n# Restart deployment\nkubectl rollout restart deployment holmesgpt\n</code></pre></p>"},{"location":"reference/troubleshooting/#api-rate-limits","title":"API Rate Limits","text":"<p>Problem: <code>Rate limit exceeded</code> errors.</p> <p>Solution: <pre><code># Check current usage in logs\nkubectl logs -l app=holmesgpt | grep \"rate limit\"\n\n# Configure rate limiting in values.yaml\ncat &lt;&lt;EOF &gt; values.yaml\nconfig:\n  rateLimit:\n    requestsPerMinute: 60\n    burstLimit: 10\nEOF\n\nhelm upgrade holmesgpt holmesgpt/holmes -f values.yaml\n</code></pre></p>"},{"location":"reference/troubleshooting/#permission-issues","title":"Permission Issues","text":""},{"location":"reference/troubleshooting/#rbac-errors","title":"RBAC Errors","text":"<p>Problem: <code>Forbidden</code> errors when accessing Kubernetes resources.</p> <p>Diagnosis: <pre><code># Check service account\nkubectl get serviceaccount holmesgpt\n\n# Verify cluster role binding\nkubectl get clusterrolebinding | grep holmesgpt\n\n# Test permissions\nkubectl auth can-i get pods --as=system:serviceaccount:default:holmesgpt\n</code></pre></p> <p>Solution: <pre><code># values.yaml\nrbac:\n  create: true\n\n# Additional permissions if needed\nrbacRules:\n  - apiGroups: [\"\"]\n    resources: [\"pods\", \"services\", \"events\", \"nodes\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n  - apiGroups: [\"apps\"]\n    resources: [\"deployments\", \"replicasets\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n</code></pre></p>"},{"location":"reference/troubleshooting/#namespace-access-issues","title":"Namespace Access Issues","text":"<p>Problem: Cannot access resources in specific namespaces.</p> <p>Solution: <pre><code># Create role binding for specific namespace\nkubectl create rolebinding holmesgpt-reader \\\n  --clusterrole=holmesgpt-reader \\\n  --serviceaccount=default:holmesgpt \\\n  --namespace=production\n</code></pre></p>"},{"location":"reference/troubleshooting/#network-connectivity-issues","title":"Network Connectivity Issues","text":""},{"location":"reference/troubleshooting/#cannot-reach-ai-provider","title":"Cannot Reach AI Provider","text":"<p>Problem: <code>Connection timeout</code> or <code>DNS resolution failed</code> errors.</p> <p>Diagnosis: <pre><code># Test from pod\nkubectl exec -it &lt;pod-name&gt; -- curl -I https://api.openai.com\n\n# Check DNS resolution\nkubectl exec -it &lt;pod-name&gt; -- nslookup api.openai.com\n\n# Verify network policies\nkubectl get networkpolicies\n</code></pre></p> <p>Solution: <pre><code># Network policy allowing egress to AI providers\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: holmesgpt-egress\nspec:\n  podSelector:\n    matchLabels:\n      app: holmesgpt\n  policyTypes:\n  - Egress\n  egress:\n  - to: []\n    ports:\n    - protocol: TCP\n      port: 443\n</code></pre></p>"},{"location":"reference/troubleshooting/#internal-service-communication","title":"Internal Service Communication","text":"<p>Problem: Cannot reach internal services (Prometheus, Grafana, etc.).</p> <p>Solution: <pre><code># Verify service endpoints\nkubectl get endpoints prometheus\n\n# Test connectivity\nkubectl exec -it &lt;pod-name&gt; -- curl http://prometheus:9090/api/v1/query\n\n# Check service discovery\nkubectl get services -A | grep prometheus\n</code></pre></p>"},{"location":"reference/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"reference/troubleshooting/#slow-response-times","title":"Slow Response Times","text":"<p>Problem: Investigations taking too long to complete.</p> <p>Diagnosis: <pre><code># Check resource usage\nkubectl top pods -l app=holmesgpt\n\n# Review metrics\nkubectl port-forward svc/holmesgpt 9090:9090\ncurl http://localhost:9090/metrics | grep holmes_\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Increase resources: <pre><code>resources:\n  limits:\n    cpu: 1\n    memory: 1Gi\n  requests:\n    cpu: 500m\n    memory: 512Mi\n</code></pre></p> </li> <li> <p>Optimize AI provider settings: <pre><code>config:\n  maxTokens: 1000  # Reduce token limit\n  temperature: 0   # Faster, more deterministic responses\n  timeout: 30s     # Set reasonable timeout\n</code></pre></p> </li> <li> <p>Enable caching: <pre><code>config:\n  cache:\n    enabled: true\n    ttl: 3600\n</code></pre></p> </li> </ol>"},{"location":"reference/troubleshooting/#high-memory-usage","title":"High Memory Usage","text":"<p>Problem: Pod consuming excessive memory.</p> <p>Solution: <pre><code># Monitor memory usage\nkubectl top pods -l app=holmesgpt --containers\n\n# Check for memory leaks in logs\nkubectl logs -l app=holmesgpt | grep -i \"memory\\|oom\"\n\n# Set memory limits\nhelm upgrade holmesgpt holmesgpt/holmes --set resources.limits.memory=512Mi\n</code></pre></p>"},{"location":"reference/troubleshooting/#investigation-issues","title":"Investigation Issues","text":""},{"location":"reference/troubleshooting/#no-results-returned","title":"No Results Returned","text":"<p>Problem: HolmesGPT returns empty or minimal results.</p> <p>Diagnosis: <pre><code># Check toolset configuration\nkubectl logs -l app=holmesgpt | grep \"toolset\"\n\n# Verify data source connectivity\nkubectl exec -it &lt;pod-name&gt; -- curl http://prometheus:9090/api/v1/query?query=up\n</code></pre></p> <p>Solutions:</p> <ol> <li> <p>Verify toolsets are enabled: <pre><code>toolsets:\n  - name: \"kubernetes\"\n    enabled: true\n  - name: \"prometheus\"\n    enabled: true\n    config:\n      url: \"http://prometheus:9090\"\n</code></pre></p> </li> <li> <p>Check data source configuration: <pre><code># Test Prometheus connectivity\ncurl \"http://prometheus:9090/api/v1/query?query=up\"\n\n# Verify Kubernetes permissions\nkubectl auth can-i get pods\n</code></pre></p> </li> </ol>"},{"location":"reference/troubleshooting/#incorrect-analysis","title":"Incorrect Analysis","text":"<p>Problem: HolmesGPT provides inaccurate or irrelevant analysis.</p> <p>Solutions:</p> <ol> <li> <p>Improve question specificity: <pre><code># Instead of: \"what's wrong?\"\n# Try: \"why is the payment-service pod restarting in the production namespace?\"\n</code></pre></p> </li> <li> <p>Provide context: <pre><code># Include relevant time frame\nholmes ask \"what errors occurred in the last 30 minutes?\"\n\n# Reference specific resources\nholmes ask \"analyze the nginx-deployment in the web namespace\"\n</code></pre></p> </li> <li> <p>Check AI model configuration: <pre><code>config:\n  model: \"gpt-4\"  # Use more capable model\n  temperature: 0.1  # More focused responses\n</code></pre></p> </li> </ol>"},{"location":"reference/troubleshooting/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"reference/troubleshooting/#missing-metrics","title":"Missing Metrics","text":"<p>Problem: Prometheus metrics not appearing.</p> <p>Solution: <pre><code># Verify metrics endpoint\nkubectl port-forward svc/holmesgpt 9090:9090\ncurl http://localhost:9090/metrics\n\n# Check ServiceMonitor\nkubectl get servicemonitor holmesgpt-metrics\n\n# Verify Prometheus is scraping\nkubectl port-forward svc/prometheus 9090:9090\n# Open http://localhost:9090/targets\n</code></pre></p>"},{"location":"reference/troubleshooting/#log-issues","title":"Log Issues","text":"<p>Problem: Missing or incomplete logs.</p> <p>Solution: <pre><code># Increase log level\nhelm upgrade holmesgpt holmesgpt/holmes --set config.logLevel=DEBUG\n\n# Check log format\nkubectl logs -l app=holmesgpt --tail=100\n\n# Verify log rotation\nkubectl describe pod &lt;pod-name&gt; | grep -A5 \"Mounts\"\n</code></pre></p>"},{"location":"reference/troubleshooting/#resource-cleanup","title":"Resource Cleanup","text":""},{"location":"reference/troubleshooting/#removing-failed-installation","title":"Removing Failed Installation","text":"<pre><code># List Helm releases\nhelm list\n\n# Uninstall release\nhelm uninstall holmesgpt\n\n# Clean up remaining resources\nkubectl delete clusterrole holmesgpt-reader\nkubectl delete clusterrolebinding holmesgpt-reader\nkubectl delete secret holmes-secrets\n</code></pre>"},{"location":"reference/troubleshooting/#reset-configuration","title":"Reset Configuration","text":"<pre><code># Reset to default values\nhelm upgrade holmesgpt holmesgpt/holmes --reset-values\n\n# Or start fresh\nhelm uninstall holmesgpt\nhelm install holmesgpt holmesgpt/holmes\n</code></pre>"},{"location":"reference/troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"reference/troubleshooting/#collecting-diagnostic-information","title":"Collecting Diagnostic Information","text":"<pre><code># Create support bundle\nkubectl get all -l app=holmesgpt -o yaml &gt; holmesgpt-resources.yaml\nkubectl describe pods -l app=holmesgpt &gt; holmesgpt-describe.txt\nkubectl logs -l app=holmesgpt --previous &gt; holmesgpt-logs.txt\n\n# Helm information\nhelm status holmesgpt &gt; helm-status.txt\nhelm get values holmesgpt &gt; helm-values.yaml\n</code></pre>"},{"location":"reference/troubleshooting/#enable-debug-mode","title":"Enable Debug Mode","text":"<pre><code># values.yaml\nconfig:\n  logLevel: \"DEBUG\"\n  debug: true\n\nenv:\n  - name: HOLMES_DEBUG\n    value: \"true\"\n</code></pre>"},{"location":"reference/troubleshooting/#community-support","title":"Community Support","text":"<ol> <li>Slack Community: robustacommunity.slack.com</li> <li>GitHub Issues: github.com/robusta-dev/holmesgpt/issues</li> <li>Documentation: docs.robusta.dev</li> </ol> <p>When reporting issues, please include: - HolmesGPT version - Kubernetes version - Helm chart version - Error messages and logs - Configuration (with sensitive data removed)</p>"},{"location":"reference/troubleshooting/#faq","title":"FAQ","text":""},{"location":"reference/troubleshooting/#q-holmesgpt-is-not-finding-any-issues-despite-obvious-problems","title":"Q: HolmesGPT is not finding any issues despite obvious problems","text":"<p>A: Check that: 1. RBAC permissions allow access to relevant namespaces 2. Toolsets are properly configured and enabled 3. Data sources are accessible from the HolmesGPT pod 4. The question is specific enough for the AI to understand</p>"},{"location":"reference/troubleshooting/#q-api-costs-are-too-high","title":"Q: API costs are too high","text":"<p>A: Optimize by: 1. Reducing <code>maxTokens</code> in configuration 2. Using a more cost-effective model 3. Enabling caching to reduce duplicate requests 4. Setting rate limits to control usage</p>"},{"location":"reference/troubleshooting/#q-holmesgpt-keeps-timing-out","title":"Q: HolmesGPT keeps timing out","text":"<p>A: Increase timeouts: <pre><code>config:\n  timeout: 60s\n  aiProvider:\n    timeout: 45s\n</code></pre></p>"},{"location":"reference/troubleshooting/#q-cannot-install-in-restricted-environments","title":"Q: Cannot install in restricted environments","text":"<p>A: For air-gapped installations: 1. Mirror the container images to your private registry 2. Configure image pull secrets 3. Use custom CA certificates if needed 4. Disable internet-dependent toolsets</p>"},{"location":"snippets/capabilities_table_header/","title":"Capabilities table header","text":""},{"location":"snippets/capabilities_table_header/#capabilities","title":"Capabilities","text":"Tool Name Description"},{"location":"snippets/custom_toolset_appeal/","title":"Custom toolset appeal","text":"<p>Need Custom Integration?</p> <p>By adding custom toolsets, users can extend HolmesGPT's investigation capabilities to address unique use cases, specific infrastructure setups, or organization-specific requirements. For example, custom toolsets might include specialized log analysis patterns or integration with external monitoring systems.</p> <p>Can't find the toolset you need? Create a custom toolset or request a new integration.</p>"},{"location":"snippets/disable_default_logging/","title":"Disable default logging","text":"<p>To disable the default logging toolset, add the following to your holmes configuration:</p> Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml:</p> <pre><code>toolsets:\n    kubernetes/logs:\n        enabled: false\n</code></pre> <pre><code>holmes:\n    toolsets:\n        kubernetes/logs:\n            enabled: false # HolmesGPT's default logging mechanism MUST be disabled\n</code></pre>"},{"location":"snippets/disable_default_logging_toolset/","title":"Disable default logging toolset","text":"<p>Disable Default Logging Toolset</p> <p>The default HolmesGPT logging tool must be disabled if you use a different datasource for logs. HolmesGPT may still use kubectl to fetch logs and never call your datasource if <code>kubernetes/logs</code> is not disabled.</p>"},{"location":"snippets/enabled_by_default/","title":"Enabled by default","text":"<p>Enabled by Default</p> <p>This toolset is enabled by default. You do not need to configure it. You can disable it if you want to but doing so may negatively impact HolmesGPT's ability to investigate issues.</p>"},{"location":"snippets/helm_upgrade_command/","title":"Helm upgrade command","text":"<p>Update your Helm values and run a Helm upgrade: <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre></p>"},{"location":"snippets/toolset_capabilities_intro/","title":"Toolset capabilities intro","text":"<p>The table below describes the specific capabilities provided by this toolset. HolmesGPT can decide to invoke any of these capabilities when answering questions or investigating issues.</p>"},{"location":"snippets/toolset_config_template/","title":"Toolset config template","text":""},{"location":"snippets/toolset_config_template/#configuration","title":"Configuration","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n    TOOLSET_PATH:\n        enabled: true\n        config:\n            # Add your configuration here\n            CUSTOM_CONFIG\n</code></pre> <pre><code>holmes:\n    toolsets:\n        TOOLSET_PATH:\n            enabled: true\n            config:\n                # Add your configuration here\n                CUSTOM_CONFIG\n</code></pre>"},{"location":"snippets/toolset_configuration_cli/","title":"Toolset configuration cli","text":"Holmes CLI <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n    TOOLSET_NAME:\n        enabled: true\n        config:\n            CONFIGURATION_OPTIONS\n</code></pre>"},{"location":"snippets/toolset_configuration_helm/","title":"Toolset configuration helm","text":"Robusta Helm Chart <pre><code>holmes:\n    toolsets:\n        TOOLSET_NAME:\n            enabled: true\n            config:\n                CONFIGURATION_OPTIONS\n</code></pre>"},{"location":"snippets/toolset_configuration_tabs/","title":"Toolset configuration tabs","text":"Holmes CLIRobusta Helm Chart <p>Add the following to ~/.holmes/config.yaml, creating the file if it doesn't exist:</p> <pre><code>toolsets:\n    TOOLSET_NAME:\n        enabled: true\n        config:\n            CONFIGURATION_OPTIONS\n</code></pre> <pre><code>holmes:\n    toolsets:\n        TOOLSET_NAME:\n            enabled: true\n            config:\n                CONFIGURATION_OPTIONS\n</code></pre> <p>Update your Helm values and run a Helm upgrade:</p> <pre><code>helm upgrade robusta robusta/robusta --values=generated_values.yaml --set clusterName=&lt;YOUR_CLUSTER_NAME&gt;\n</code></pre>"},{"location":"snippets/toolsets_that_provide_logging/","title":"Toolsets that provide logging","text":"<p>HolmesGPT provides several out-of-the-box alternatives for log access. You can select from these options:</p> <ul> <li>kubernetes/logs: Access logs directly through Kubernetes. This is the default toolset.</li> <li>coralogix/logs: Access logs through Coralogix.</li> <li>grafana/loki: Access Loki logs by proxying through a Grafana instance.</li> <li>opensearch/logs: Access logs through OpenSearch.</li> <li>datadog: Access logs through DataDog.</li> </ul>"},{"location":"usage/","title":"Usage &amp; Workflows","text":"<p>Learn how to effectively use HolmesGPT for incident investigation and root cause analysis.</p>"},{"location":"usage/#how-to-use-holmesgpt","title":"How to Use HolmesGPT","text":"<p>HolmesGPT can be accessed through multiple interfaces:</p> <ul> <li>Slack Integration - Trigger investigations directly from alerts</li> <li>Web UI Guide - Use the Robusta UI for visual investigations</li> <li>CLI Usage - Command-line interface for automation</li> </ul>"},{"location":"usage/#investigation-process","title":"Investigation Process","text":"<p>Understand how HolmesGPT works:</p> <ul> <li>Investigation Process - How HolmesGPT analyzes your infrastructure</li> <li>Best Practices - Tips for getting the most accurate results</li> </ul>"},{"location":"usage/#common-workflows","title":"Common Workflows","text":""},{"location":"usage/#alert-driven-investigation","title":"Alert-Driven Investigation","text":"<ol> <li>Receive alert in Slack/PagerDuty</li> <li>Click \"Ask HolmesGPT\" button</li> <li>Review AI analysis and recommendations</li> <li>Act on findings</li> </ol>"},{"location":"usage/#proactive-analysis","title":"Proactive Analysis","text":"<ol> <li>Access Robusta UI</li> <li>Navigate to \"Root Cause\" tab</li> <li>Select resources or timeframe to investigate</li> <li>Review insights and patterns</li> </ol>"},{"location":"usage/#automated-integration","title":"Automated Integration","text":"<ol> <li>Configure webhooks or API calls</li> <li>Trigger investigations programmatically</li> <li>Integrate results into your incident response workflow</li> </ol>"},{"location":"usage/#getting-the-best-results","title":"Getting the Best Results","text":"<ul> <li>Configure relevant data sources - More data leads to better analysis</li> <li>Provide context - Include alert details and symptoms</li> <li>Use specific timeframes - Focus on relevant time windows</li> <li>Review multiple perspectives - Check logs, metrics, and events</li> </ul> <p>Start with the Investigation Process to understand how HolmesGPT analyzes your infrastructure.</p>"}]}